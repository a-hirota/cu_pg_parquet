"""
PostgreSQL â†’ GPU 16ä¸¦åˆ— ctidåˆ†å‰²ç‰ˆ
16ä¸¦åˆ—ctidåˆ†å‰² + UNIXãƒ‰ãƒ¡ã‚¤ãƒ³ã‚½ã‚±ãƒƒãƒˆ + GPU Direct Storage

æœ€é©åŒ–:
- ctidã«ã‚ˆã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«16åˆ†å‰²ä¸¦åˆ—èª­ã¿å–ã‚Š
- UNIXãƒ‰ãƒ¡ã‚¤ãƒ³ã‚½ã‚±ãƒƒãƒˆæ¥ç¶šï¼ˆTCP loopbackã‚ˆã‚Šé«˜é€Ÿï¼‰
- 16å€‹ã®å°‚ç”¨GPUãƒãƒƒãƒ•ã‚¡ + GPUä¸Šconcat
- psycopg3ã®8MBãƒãƒƒãƒ•ã‚¡æ´»ç”¨
- GPU Direct Storageçµ±åˆ
- libpq 8KiBå•é¡Œã®å®Œå…¨å›é¿

æœŸå¾…åŠ¹æœ: 125 MB/s â†’ 2GB/s (16å€é«˜é€ŸåŒ–)

ç’°å¢ƒå¤‰æ•°:
GPUPASER_PG_DSN  : PostgreSQLæ¥ç¶šæ–‡å­—åˆ—ï¼ˆUNIXã‚½ã‚±ãƒƒãƒˆæ¨å¥¨ï¼‰
"""

import os
import time
import asyncio
import math
import tempfile
import psycopg
import rmm
import numpy as np
import numba
from numba import cuda
import cupy as cp
import argparse
from concurrent.futures import ThreadPoolExecutor

from src.metadata import fetch_column_meta
from src.cuda_kernels.postgresql_binary_parser import detect_pg_header_size
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet

TABLE_NAME = "lineorder"
OUTPUT_PARQUET_PATH = "benchmark/lineorder_parallel_ctid.output.parquet"

# ä¸¦åˆ—è¨­å®š
DEFAULT_PARALLEL = 16
UNIX_SOCKET_DSN = "dbname=postgres user=postgres host=/var/run/postgresql"

def check_gpu_direct_support():
    """GPU Direct ã‚µãƒãƒ¼ãƒˆç¢ºèª"""
    print("\n=== GPU Direct ã‚µãƒãƒ¼ãƒˆç¢ºèª ===")
    
    try:
        if os.path.exists("/proc/driver/nvidia-fs/stats"):
            print("âœ… nvidia-fs ãƒ‰ãƒ©ã‚¤ãƒæ¤œå‡º")
        else:
            print("âŒ nvidia-fs ãƒ‰ãƒ©ã‚¤ãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
    except Exception:
        print("âŒ nvidia-fs ç¢ºèªã‚¨ãƒ©ãƒ¼")
        return False
    
    try:
        import kvikio
        print(f"âœ… kvikio ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {kvikio.__version__}")
        os.environ["KVIKIO_COMPAT_MODE"] = "OFF"
        print("âœ… KVIKIO_COMPAT_MODE=OFF è¨­å®šå®Œäº†")
        return True
    except ImportError:
        print("âŒ kvikio ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False

def get_table_blocks(dsn, table_name):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç·ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’å–å¾—"""
    conn = psycopg.connect(dsn)
    try:
        with conn.cursor() as cur:
            cur.execute(f"SELECT pg_relation_size('{table_name}') / 8192 AS blocks")
            blocks = cur.fetchone()[0]
            return int(blocks)
    finally:
        conn.close()

def make_ctid_ranges(total_blocks, parallel_count):
    """ctidç¯„å›²ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    chunk_size = math.ceil(total_blocks / parallel_count)
    ranges = []
    
    for i in range(parallel_count):
        start_block = i * chunk_size
        end_block = min((i + 1) * chunk_size, total_blocks)
        if start_block < total_blocks:
            ranges.append((start_block, end_block))
    
    return ranges

def make_copy_sql(table_name, start_block, end_block, limit_rows=None):
    """ctidç¯„å›²æŒ‡å®šCOPY SQLã‚’ç”Ÿæˆ"""
    base_sql = f"""
    COPY (
        SELECT * FROM {table_name}
        WHERE ctid >= '({start_block},0)' 
          AND ctid < '({end_block},0)'
    """
    
    if limit_rows:
        # ä¸¦åˆ—æ•°ã§åˆ†å‰²ã—ãŸè¡Œæ•°åˆ¶é™
        per_worker_limit = limit_rows // DEFAULT_PARALLEL
        base_sql += f" LIMIT {per_worker_limit}"
    
    base_sql += ") TO STDOUT (FORMAT binary)"
    return base_sql

class ParallelCopyWorker:
    """ä¸¦åˆ—COPYå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼"""
    
    def __init__(self, worker_id, dsn, gpu_buffer, gpu_offset):
        self.worker_id = worker_id
        self.dsn = dsn
        self.gpu_buffer = gpu_buffer
        self.gpu_offset = gpu_offset
        self.temp_files = []
        
    async def process_range(self, start_block, end_block, copy_sql):
        """ctidç¯„å›²ã®ä¸¦åˆ—å‡¦ç†"""
        print(f"  Worker {self.worker_id}: ctidç¯„å›² ({start_block},{end_block}) é–‹å§‹...")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        temp_file = os.path.join(
            tempfile.gettempdir(),
            f"parallel_ctid_worker_{self.worker_id}_{start_block}_{end_block}.bin"
        )
        self.temp_files.append(temp_file)
        
        start_time = time.time()
        data_size = 0
        read_count = 0
        
        try:
            # UNIXãƒ‰ãƒ¡ã‚¤ãƒ³ã‚½ã‚±ãƒƒãƒˆæ¥ç¶š
            conn = await psycopg.AsyncConnection.connect(self.dsn)
            
            try:
                async with conn.cursor() as cur:
                    async with cur.copy(copy_sql) as copy_obj:
                        with open(temp_file, 'wb') as f:
                            async for chunk in copy_obj:
                                if chunk:
                                    # memoryview â†’ byteså¤‰æ›
                                    if isinstance(chunk, memoryview):
                                        chunk_bytes = chunk.tobytes()
                                    else:
                                        chunk_bytes = bytes(chunk)
                                    
                                    f.write(chunk_bytes)
                                    data_size += len(chunk_bytes)
                                    read_count += 1
                
                duration = time.time() - start_time
                print(f"  Worker {self.worker_id}: å®Œäº† ({duration:.2f}ç§’, {data_size/(1024*1024):.1f}MB, {read_count:,}å›)")
                
                return {
                    'worker_id': self.worker_id,
                    'temp_file': temp_file,
                    'data_size': data_size,
                    'duration': duration,
                    'read_count': read_count
                }
                
            finally:
                await conn.aclose()
                
        except Exception as e:
            print(f"  Worker {self.worker_id}: ã‚¨ãƒ©ãƒ¼ - {e}")
            return None
    
    def transfer_to_gpu(self, temp_file, data_size):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ« â†’ GPUè»¢é€ (GPU Direct)"""
        try:
            import kvikio
            from kvikio import CuFile
            
            start_time = time.time()
            
            # GPU Directã§ãƒ•ã‚¡ã‚¤ãƒ« â†’ GPUè»¢é€
            with CuFile(temp_file, "r") as cufile:
                # GPU ãƒãƒƒãƒ•ã‚¡ã®è©²å½“é ˜åŸŸã«è»¢é€
                future = cufile.pread(self.gpu_buffer, file_offset=0, buffer_offset=self.gpu_offset)
                bytes_read = future.get()
            
            duration = time.time() - start_time
            speed = bytes_read / (1024*1024) / duration
            
            print(f"  Worker {self.worker_id}: GPU Directè»¢é€å®Œäº† ({duration:.2f}ç§’, {speed:.1f}MB/sec)")
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.remove(temp_file)
            
            return bytes_read
            
        except Exception as e:
            print(f"  Worker {self.worker_id}: GPUè»¢é€ã‚¨ãƒ©ãƒ¼ - {e}")
            return 0

def run_bandwidth_test():
    """PCIeå¸¯åŸŸãƒ†ã‚¹ãƒˆ"""
    print("\n=== PCIeå¸¯åŸŸå¹…ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        size_mb = 1024
        host_data = np.random.randint(0, 256, size_mb * 1024 * 1024, dtype=np.uint8)
        
        start_time = time.time()
        device_data = cuda.to_device(host_data)
        htod_time = time.time() - start_time
        htod_speed = size_mb / htod_time
        
        start_time = time.time()
        result_data = device_data.copy_to_host()
        dtoh_time = time.time() - start_time
        dtoh_speed = size_mb / dtoh_time
        
        print(f"Hostâ†’Device: {htod_speed:.2f} MB/s")
        print(f"Deviceâ†’Host: {dtoh_speed:.2f} MB/s")
        
    except Exception as e:
        print(f"å¸¯åŸŸæ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")

async def run_parallel_ctid_benchmark(limit_rows=1000000, parallel_count=DEFAULT_PARALLEL):
    """16ä¸¦åˆ—ctidåˆ†å‰²ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    # DSNè¨­å®šï¼ˆUNIXã‚½ã‚±ãƒƒãƒˆå„ªå…ˆï¼‰
    dsn = os.environ.get("GPUPASER_PG_DSN", UNIX_SOCKET_DSN)
    if "host=/var/run/postgresql" in dsn or "host=" not in dsn:
        print(f"âœ… UNIXãƒ‰ãƒ¡ã‚¤ãƒ³ã‚½ã‚±ãƒƒãƒˆæ¥ç¶šä½¿ç”¨")
    else:
        print(f"âš ï¸  TCPæ¥ç¶šä½¿ç”¨ï¼ˆUNIXã‚½ã‚±ãƒƒãƒˆæ¨å¥¨ï¼‰")
    
    print(f"=== PostgreSQL â†’ GPU 16ä¸¦åˆ—ctidåˆ†å‰²ç‰ˆ ===")
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME}")
    print(f"è¡Œæ•°åˆ¶é™: {limit_rows:,}")
    print(f"ä¸¦åˆ—æ•°: {parallel_count}")
    print(f"æœ€é©åŒ–è¨­å®š:")
    print(f"  ctidåˆ†å‰²: {parallel_count}ä¸¦åˆ—èª­ã¿å–ã‚Š")
    print(f"  UNIXã‚½ã‚±ãƒƒãƒˆ: TCP loopbackã‚ˆã‚Šé«˜é€Ÿ")
    print(f"  GPU Direct: kvikio/cuFileä½¿ç”¨")
    print(f"  GPUãƒãƒƒãƒ•ã‚¡: {parallel_count}å€‹ã®å°‚ç”¨é ˜åŸŸ")
    
    # GPU Direct ã‚µãƒãƒ¼ãƒˆç¢ºèª
    if not check_gpu_direct_support():
        print("âŒ GPU Direct ã‚µãƒãƒ¼ãƒˆãŒä¸å®Œå…¨ã§ã™ã€‚")
        return

    # PCIeå¸¯åŸŸãƒ†ã‚¹ãƒˆ
    run_bandwidth_test()
    
    # RMM 22GBåˆæœŸåŒ–
    try:
        if not rmm.is_initialized():
            rmm.reinitialize(
                pool_allocator=True, 
                initial_pool_size=22*1024**3
            )
            print("âœ… RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº† (22GB)")
    except Exception as e:
        print(f"âŒ RMMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    start_total_time = time.time()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    conn = psycopg.connect(dsn)
    try:
        print("\nãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        start_meta_time = time.time()
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        meta_time = time.time() - start_meta_time
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")
        ncols = len(columns)
    finally:
        conn.close()

    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ–ãƒ­ãƒƒã‚¯æ•°å–å¾—
    print("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’å–å¾—ä¸­...")
    total_blocks = get_table_blocks(dsn, TABLE_NAME)
    print(f"ç·ãƒ–ãƒ­ãƒƒã‚¯æ•°: {total_blocks:,}")
    
    # ctidç¯„å›²åˆ†å‰²
    ranges = make_ctid_ranges(total_blocks, parallel_count)
    print(f"ctidç¯„å›²åˆ†å‰²: {len(ranges)}å€‹ã®ç¯„å›²")
    for i, (start, end) in enumerate(ranges):
        print(f"  ç¯„å›² {i}: ãƒ–ãƒ­ãƒƒã‚¯ {start:,} - {end:,}")

    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¨å®š
    estimated_size_per_worker = (limit_rows // parallel_count) * 200  # æ¦‚ç®—200bytes/è¡Œ
    total_estimated_size = estimated_size_per_worker * parallel_count
    print(f"æ¨å®šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_estimated_size / (1024*1024):.1f} MB")

    # 16å€‹ã®GPUãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿
    print(f"\n{parallel_count}å€‹ã®GPUãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿ä¸­...")
    gpu_buffers = []
    worker_offsets = []
    
    for i in range(parallel_count):
        buffer_size = estimated_size_per_worker * 2  # ä½™è£•ã‚’æŒãŸã›ã‚‹
        gpu_buffer = rmm.DeviceBuffer(size=buffer_size)
        gpu_buffers.append(gpu_buffer)
        worker_offsets.append(0)  # å„ãƒãƒƒãƒ•ã‚¡ã¯ç‹¬ç«‹ãªã®ã§ã‚ªãƒ•ã‚»ãƒƒãƒˆ0
        print(f"  GPUãƒãƒƒãƒ•ã‚¡ {i}: {buffer_size / (1024*1024):.1f} MBç¢ºä¿")

    # ä¸¦åˆ—COPYãƒ¯ãƒ¼ã‚«ãƒ¼ä½œæˆ
    print(f"\n{parallel_count}ä¸¦åˆ—COPYå‡¦ç†é–‹å§‹...")
    workers = []
    tasks = []
    
    for i, (start_block, end_block) in enumerate(ranges):
        worker = ParallelCopyWorker(i, dsn, gpu_buffers[i], worker_offsets[i])
        workers.append(worker)
        
        copy_sql = make_copy_sql(TABLE_NAME, start_block, end_block, limit_rows)
        task = worker.process_range(start_block, end_block, copy_sql)
        tasks.append(task)
    
    # ä¸¦åˆ—å®Ÿè¡Œ
    start_copy_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    copy_time = time.time() - start_copy_time
    
    # çµæœé›†è¨ˆ
    successful_results = [r for r in results if r is not None and not isinstance(r, Exception)]
    total_data_size = sum(r['data_size'] for r in successful_results)
    total_read_count = sum(r['read_count'] for r in successful_results)
    
    print(f"âœ… {parallel_count}ä¸¦åˆ—COPYå®Œäº† ({copy_time:.4f}ç§’)")
    print(f"  æˆåŠŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {len(successful_results)}/{parallel_count}")
    print(f"  ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_data_size / (1024*1024):.2f} MB")
    print(f"  ç·èª­ã¿è¾¼ã¿å›æ•°: {total_read_count:,}")
    print(f"  ä¸¦åˆ—è»¢é€é€Ÿåº¦: {total_data_size / (1024*1024) / copy_time:.2f} MB/sec")
    print(f"  å˜ä¸€ãƒ¯ãƒ¼ã‚«ãƒ¼æ›ç®—: {(total_data_size / (1024*1024) / copy_time) / parallel_count:.2f} MB/sec")

    # GPU Directä¸¦åˆ—è»¢é€
    print(f"\n{parallel_count}ä¸¦åˆ—GPU Directè»¢é€é–‹å§‹...")
    start_gpu_time = time.time()
    
    gpu_transfer_tasks = []
    for i, result in enumerate(successful_results):
        if result:
            worker = workers[result['worker_id']]
            task = asyncio.get_event_loop().run_in_executor(
                None, worker.transfer_to_gpu, result['temp_file'], result['data_size']
            )
            gpu_transfer_tasks.append(task)
    
    gpu_results = await asyncio.gather(*gpu_transfer_tasks)
    gpu_time = time.time() - start_gpu_time
    
    total_gpu_transferred = sum(gpu_results)
    gpu_speed = total_gpu_transferred / (1024*1024) / gpu_time
    
    print(f"âœ… {parallel_count}ä¸¦åˆ—GPU Directè»¢é€å®Œäº† ({gpu_time:.4f}ç§’)")
    print(f"  GPUè»¢é€é€Ÿåº¦: {gpu_speed:.2f} MB/sec")

    # GPUä¸Šã§ãƒ‡ãƒ¼ã‚¿çµåˆ
    print("\nGPUä¸Šã§ãƒ‡ãƒ¼ã‚¿çµåˆä¸­...")
    start_concat_time = time.time()
    
    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã§GPUãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
    trimmed_buffers = []
    actual_sizes = [r['data_size'] for r in successful_results]
    
    for i, actual_size in enumerate(actual_sizes):
        if actual_size > 0:
            trimmed_buffer = rmm.DeviceBuffer(size=actual_size)
            cp.cuda.runtime.memcpy(
                trimmed_buffer.ptr, gpu_buffers[i].ptr, actual_size,
                cp.cuda.runtime.memcpyDeviceToDevice
            )
            trimmed_buffers.append(trimmed_buffer)
    
    # GPUä¸Šã§ãƒãƒƒãƒ•ã‚¡çµåˆ
    total_actual_size = sum(actual_sizes)
    final_gpu_buffer = rmm.DeviceBuffer(size=total_actual_size)
    
    current_offset = 0
    for buffer in trimmed_buffers:
        cp.cuda.runtime.memcpy(
            final_gpu_buffer.ptr + current_offset,
            buffer.ptr,
            buffer.size,
            cp.cuda.runtime.memcpyDeviceToDevice
        )
        current_offset += buffer.size
    
    concat_time = time.time() - start_concat_time
    print(f"GPUçµåˆå®Œäº† ({concat_time:.4f}ç§’), æœ€çµ‚ã‚µã‚¤ã‚º: {total_actual_size / (1024*1024):.2f} MB")

    # numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›
    print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›ä¸­...")
    raw_dev = cuda.as_cuda_array(final_gpu_buffer).view(dtype=np.uint8)
    print(f"GPU ã‚¢ãƒ¬ã‚¤å¤‰æ›å®Œäº†: {raw_dev.shape[0]:,} bytes")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
    header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
    header_size = detect_pg_header_size(header_sample)
    print(f"ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º: {header_size} ãƒã‚¤ãƒˆ")

    # GPUæœ€é©åŒ–å‡¦ç†
    print("GPUæœ€é©åŒ–å‡¦ç†ä¸­...")
    start_processing_time = time.time()
    
    try:
        cudf_df, detailed_timing = postgresql_to_cudf_parquet(
            raw_dev=raw_dev,
            columns=columns,
            ncols=ncols,
            header_size=header_size,
            output_path=OUTPUT_PARQUET_PATH,
            compression='snappy',
            use_rmm=True,
            optimize_gpu=True
        )
        
        processing_time = time.time() - start_processing_time
        rows = len(cudf_df)
        parse_time = detailed_timing.get('gpu_parsing', 0)
        decode_time = detailed_timing.get('cudf_creation', 0)
        write_time = detailed_timing.get('parquet_export', 0)
        
        print(f"GPUæœ€é©åŒ–å‡¦ç†å®Œäº† ({processing_time:.4f}ç§’), è¡Œæ•°: {rows}")
        
    except Exception as e:
        print(f"GPUæœ€é©åŒ–å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
        return

    total_time = time.time() - start_total_time
    decimal_cols = sum(1 for col in columns if col.arrow_id == 5)
    
    print(f"\n=== 16ä¸¦åˆ—ctidåˆ†å‰²ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ===")
    print(f"ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—       : {meta_time:.4f} ç§’")
    print(f"  {parallel_count}ä¸¦åˆ—COPY        : {copy_time:.4f} ç§’")
    print(f"  {parallel_count}ä¸¦åˆ—GPU Direct  : {gpu_time:.4f} ç§’")
    print(f"  GPUçµåˆ             : {concat_time:.4f} ç§’")
    print(f"  GPUãƒ‘ãƒ¼ã‚¹           : {parse_time:.4f} ç§’")
    print(f"  GPUãƒ‡ã‚³ãƒ¼ãƒ‰         : {decode_time:.4f} ç§’")
    print(f"  Parquetæ›¸ãè¾¼ã¿     : {write_time:.4f} ç§’")
    print("--- çµ±è¨ˆæƒ…å ± ---")
    print(f"  å‡¦ç†è¡Œæ•°      : {rows:,} è¡Œ")
    print(f"  å‡¦ç†åˆ—æ•°      : {len(columns)} åˆ—")
    print(f"  Decimalåˆ—æ•°   : {decimal_cols} åˆ—")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º  : {total_actual_size / (1024*1024):.2f} MB")
    print(f"  ä¸¦åˆ—æ•°        : {parallel_count}")
    print(f"  ctidç¯„å›²æ•°    : {len(ranges)}")
    
    total_cells = rows * len(columns)
    throughput = total_cells / decode_time if decode_time > 0 else 0
    parallel_copy_speed = total_data_size / (1024*1024) / copy_time
    single_equiv_speed = parallel_copy_speed / parallel_count
    
    print(f"  ã‚»ãƒ«å‡¦ç†é€Ÿåº¦        : {throughput:,.0f} cells/sec")
    print(f"  {parallel_count}ä¸¦åˆ—COPYé€Ÿåº¦     : {parallel_copy_speed:.2f} MB/sec")
    print(f"  å˜ä¸€æ›ç®—é€Ÿåº¦        : {single_equiv_speed:.2f} MB/sec")
    print(f"  GPU Directé€Ÿåº¦      : {gpu_speed:.2f} MB/sec")
    
    # æ€§èƒ½å‘ä¸Šè©•ä¾¡
    baseline_speed = 125  # å¾“æ¥ã®å˜ä¸€COPYé€Ÿåº¦
    improvement_ratio = parallel_copy_speed / baseline_speed
    
    print(f"  æ€§èƒ½å‘ä¸Šå€ç‡        : {improvement_ratio:.1f}å€ (å¯¾ {baseline_speed} MB/sec)")
    
    if parallel_copy_speed > 2000:
        performance_class = "ğŸ† è¶…é«˜é€Ÿ (2GB/s+)"
    elif parallel_copy_speed > 1000:
        performance_class = "ğŸ¥‡ é«˜é€Ÿ (1GB/s+)"
    elif parallel_copy_speed > 500:
        performance_class = "ğŸ¥ˆ ä¸­é€Ÿ (500MB/s+)"
    else:
        performance_class = "ğŸ¥‰ æ”¹å–„ä¸­"
    
    print(f"  æ€§èƒ½ã‚¯ãƒ©ã‚¹          : {performance_class}")
    
    print("--- 16ä¸¦åˆ—ctidåˆ†å‰²æœ€é©åŒ–åŠ¹æœ ---")
    print("  âœ… ctidåˆ†å‰²: ãƒ†ãƒ¼ãƒ–ãƒ«ä¸¦åˆ—èª­ã¿å–ã‚Š")
    print("  âœ… UNIXã‚½ã‚±ãƒƒãƒˆ: TCP loopbackã‚ˆã‚Šé«˜é€Ÿ")
    print("  âœ… psycopg3: 8MBãƒãƒƒãƒ•ã‚¡æ´»ç”¨")
    print("  âœ… GPU Direct: ä¸¦åˆ—GPUè»¢é€")
    print("  âœ… GPUçµåˆ: é«˜é€ŸGPUä¸Šconcat")
    print("  âœ… libpq 8KiBå•é¡Œ: å®Œå…¨å›é¿")
    print("=========================================")

    # æ¤œè¨¼ç”¨å‡ºåŠ›
    print(f"\ncuDFæ¤œè¨¼ç”¨å‡ºåŠ›:")
    try:
        print(f"å‡ºåŠ›Parquet: {OUTPUT_PARQUET_PATH}")
        print(f"èª­ã¿è¾¼ã¿ç¢ºèª: {len(cudf_df):,} è¡Œ Ã— {len(cudf_df.columns)} åˆ—")
        print("âœ… cuDFæ¤œè¨¼: æˆåŠŸ")
    except Exception as e:
        print(f"âŒ cuDFæ¤œè¨¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU 16ä¸¦åˆ—ctidåˆ†å‰²ç‰ˆ')
    parser.add_argument('--rows', type=int, default=1000000, help='å‡¦ç†è¡Œæ•°åˆ¶é™')
    parser.add_argument('--parallel', type=int, default=DEFAULT_PARALLEL, help='ä¸¦åˆ—æ•°')
    parser.add_argument('--bandwidth-test', action='store_true', help='å¸¯åŸŸãƒ†ã‚¹ãƒˆã®ã¿')
    parser.add_argument('--check-support', action='store_true', help='GPU Directã‚µãƒãƒ¼ãƒˆç¢ºèªã®ã¿')
    parser.add_argument('--check-blocks', action='store_true', help='ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ–ãƒ­ãƒƒã‚¯æ•°ç¢ºèªã®ã¿')
    
    args = parser.parse_args()
    
    try:
        cuda.current_context()
        print("âœ… CUDA context OK")
    except Exception as e:
        print(f"âŒ CUDAåˆæœŸåŒ–å¤±æ•—: {e}")
        exit(1)
    
    if args.check_support:
        check_gpu_direct_support()
        return
    
    if args.bandwidth_test:
        run_bandwidth_test()
        return
    
    if args.check_blocks:
        dsn = os.environ.get("GPUPASER_PG_DSN", UNIX_SOCKET_DSN)
        blocks = get_table_blocks(dsn, TABLE_NAME)
        print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {TABLE_NAME} ã®ç·ãƒ–ãƒ­ãƒƒã‚¯æ•°: {blocks:,}")
        ranges = make_ctid_ranges(blocks, args.parallel)
        print(f"{args.parallel}ä¸¦åˆ—ã§ã®ctidç¯„å›²åˆ†å‰²:")
        for i, (start, end) in enumerate(ranges):
            print(f"  ç¯„å›² {i}: ãƒ–ãƒ­ãƒƒã‚¯ {start:,} - {end:,}")
        return
    
    # ä¸¦åˆ—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    asyncio.run(run_parallel_ctid_benchmark(
        limit_rows=args.rows,
        parallel_count=args.parallel
    ))

if __name__ == "__main__":
    main()