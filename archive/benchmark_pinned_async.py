"""
PostgreSQL â†’ GPU Pinned + éåŒæœŸã‚³ãƒ”ãƒ¼æ–¹å¼
CPU100%å¼µã‚Šä»˜ãå•é¡Œã‚’æ ¹æœ¬è§£æ±º

- numba.cuda.pinned_array ã§ãƒšãƒ¼ã‚¸ãƒ­ãƒƒã‚¯æ¸ˆã¿ãƒ¡ãƒ¢ãƒªä½¿ç”¨
- cupy.cuda.runtime.memcpyAsync ã§éåŒæœŸ Hâ†’D è»¢é€
- ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†
- CPUä½¿ç”¨ç‡ã‚’1æ¡%ã¾ã§å‰Šæ¸›ã€GPUè»¢é€10GB/sä»¥ä¸Šã‚’é”æˆ

ç’°å¢ƒå¤‰æ•°:
GPUPASER_PG_DSN  : PostgreSQLæ¥ç¶šæ–‡å­—åˆ—
"""

import os
import time
import ctypes
import psycopg
import rmm
import numpy as np
import numba
from numba import cuda
import cupy as cp
import argparse

from src.metadata import fetch_column_meta
from src.cuda_kernels.postgresql_binary_parser import detect_pg_header_size
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet

TABLE_NAME = "lineorder"
OUTPUT_PARQUET_PATH = "benchmark/lineorder_pinned_async.output.parquet"

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®šæ•°
CHUNK_SIZE = 4 << 20  # 4 MiB (4 KiBæ•´æ•°å€ã€GDSæ¨å¥¨)
DOUBLE_BUFFER = True  # ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡æœ‰åŠ¹

def run_pinned_async_benchmark(limit_rows=1000000):
    """
    Pinned + éåŒæœŸã‚³ãƒ”ãƒ¼æ–¹å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    CPUä½¿ç”¨ç‡ã‚’1æ¡%ã¾ã§å‰Šæ¸›ã€GPUè»¢é€é€Ÿåº¦ã‚’å¤§å¹…å‘ä¸Š
    """
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    print(f"=== PostgreSQL â†’ GPU Pinned + éåŒæœŸã‚³ãƒ”ãƒ¼æ–¹å¼ ===")
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME}")
    print(f"è¡Œæ•°åˆ¶é™: {limit_rows:,}")
    print(f"ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE / (1024*1024):.1f} MB (GDSæœ€é©åŒ–)")
    
    # RMM åˆæœŸåŒ–
    try:
        if not rmm.is_initialized():
            rmm.reinitialize(
                pool_allocator=True, 
                initial_pool_size=8*1024**3
            )
            print("âœ… RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"âŒ RMMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    start_total_time = time.time()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    conn = psycopg.connect(dsn)
    try:
        print("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        start_meta_time = time.time()
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        meta_time = time.time() - start_meta_time
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")
        ncols = len(columns)

        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¨å®š
        rows_est = limit_rows
        row_bytes = 17*8 + 17*4  # æ¦‚ç®—
        header_est = 19
        estimated_size = header_est + rows_est * row_bytes
        print(f"æ¨å®šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {estimated_size / (1024*1024):.2f} MB")

        # GPU ãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿
        print("GPU ãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿ä¸­...")
        devbuf = rmm.DeviceBuffer(size=estimated_size)
        print(f"GPU ãƒãƒƒãƒ•ã‚¡ç¢ºä¿å®Œäº†: {estimated_size / (1024*1024):.2f} MB")

        # Pinned ãƒ›ã‚¹ãƒˆãƒãƒƒãƒ•ã‚¡ç¢ºä¿ï¼ˆãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡ï¼‰
        print("Pinned ãƒ›ã‚¹ãƒˆãƒãƒƒãƒ•ã‚¡ç¢ºä¿ä¸­...")
        if DOUBLE_BUFFER:
            pbuf1 = numba.cuda.pinned_array(CHUNK_SIZE, dtype=np.uint8)
            pbuf2 = numba.cuda.pinned_array(CHUNK_SIZE, dtype=np.uint8)
            print(f"âœ… ãƒ€ãƒ–ãƒ« Pinned ãƒãƒƒãƒ•ã‚¡ç¢ºä¿å®Œäº†: {CHUNK_SIZE / (1024*1024):.1f} MB Ã— 2")
        else:
            pbuf1 = numba.cuda.pinned_array(CHUNK_SIZE, dtype=np.uint8)
            pbuf2 = None
            print(f"âœ… ã‚·ãƒ³ã‚°ãƒ« Pinned ãƒãƒƒãƒ•ã‚¡ç¢ºä¿å®Œäº†: {CHUNK_SIZE / (1024*1024):.1f} MB")

        # CUDA ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆï¼ˆéåŒæœŸå‡¦ç†ç”¨ï¼‰
        stream = cp.cuda.Stream(non_blocking=True)
        print(f"âœ… CUDA éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆå®Œäº†")

        # COPY BINARY â†’ Pinned + éåŒæœŸè»¢é€
        print("COPY BINARY â†’ Pinned + éåŒæœŸè»¢é€å®Ÿè¡Œä¸­...")
        start_copy_time = time.time()
        
        copy_sql = f"COPY (SELECT * FROM {TABLE_NAME} LIMIT {limit_rows}) TO STDOUT (FORMAT binary)"
        
        offset = 0
        chunk_count = 0
        toggle = True
        total_async_time = 0
        
        with conn.cursor() as cur:
            with cur.copy(copy_sql) as copy_obj:
                print("  ğŸš€ éåŒæœŸ Hâ†’D è»¢é€é–‹å§‹...")
                
                for chunk in copy_obj:
                    if chunk:
                        chunk_size = len(chunk)
                        
                        # ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ ãƒã‚§ãƒƒã‚¯
                        if offset + chunk_size > devbuf.size:
                            print(f"âš ï¸  è­¦å‘Š: GPUãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºä¸è¶³")
                            break
                        
                        # ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡é¸æŠ
                        if DOUBLE_BUFFER:
                            current_buf = pbuf1 if toggle else pbuf2
                            toggle = not toggle
                        else:
                            current_buf = pbuf1
                        
                        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒ Pinned ãƒãƒƒãƒ•ã‚¡ã‚ˆã‚Šå¤§ãã„å ´åˆã®å‡¦ç†
                        if chunk_size > CHUNK_SIZE:
                            print(f"âš ï¸  å¤§ããªãƒãƒ£ãƒ³ã‚¯({chunk_size:,}B)ã‚’åˆ†å‰²å‡¦ç†")
                            # åˆ†å‰²å‡¦ç†ï¼ˆç°¡ç•¥åŒ–ï¼‰
                            sub_offset = 0
                            while sub_offset < chunk_size:
                                sub_size = min(CHUNK_SIZE, chunk_size - sub_offset)
                                current_buf[:sub_size] = chunk[sub_offset:sub_offset+sub_size]
                                
                                # éåŒæœŸ Hâ†’D ã‚³ãƒ”ãƒ¼
                                start_async = time.time()
                                src_ptr = ctypes.addressof(ctypes.c_char.from_buffer(current_buf))
                                dst_ptr = devbuf.ptr + offset
                                
                                cp.cuda.runtime.memcpyAsync(
                                    dst_ptr, src_ptr, sub_size,
                                    cp.cuda.runtime.memcpyHostToDevice,
                                    stream.ptr
                                )
                                total_async_time += time.time() - start_async
                                
                                offset += sub_size
                                sub_offset += sub_size
                        else:
                            # é€šå¸¸å‡¦ç†
                            # â· memcpy1: socketâ†’pinned
                            current_buf[:chunk_size] = chunk
                            
                            # â¸ éåŒæœŸ Hâ†’D ã‚³ãƒ”ãƒ¼
                            start_async = time.time()
                            src_ptr = ctypes.addressof(ctypes.c_char.from_buffer(current_buf))
                            dst_ptr = devbuf.ptr + offset
                            
                            cp.cuda.runtime.memcpyAsync(
                                dst_ptr, src_ptr, chunk_size,
                                cp.cuda.runtime.memcpyHostToDevice,
                                stream.ptr
                            )
                            total_async_time += time.time() - start_async
                            
                            offset += chunk_size
                        
                        chunk_count += 1
                
                # å…¨ã¦ã®éåŒæœŸè»¢é€å®Œäº†ã‚’å¾…æ©Ÿ
                print("  â³ éåŒæœŸè»¢é€å®Œäº†å¾…æ©Ÿä¸­...")
                cp.cuda.runtime.streamSynchronize(stream.ptr)
        
        copy_time = time.time() - start_copy_time
        actual_data_size = offset
        
        print(f"âœ… COPY BINARY â†’ Pinned + éåŒæœŸè»¢é€å®Œäº† ({copy_time:.4f}ç§’)")
        print(f"  å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
        print(f"  å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {actual_data_size / (1024*1024):.2f} MB")
        print(f"  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€Ÿåº¦: {actual_data_size / (1024*1024) / copy_time:.2f} MB/sec")
        print(f"  éåŒæœŸè»¢é€æ™‚é–“: {total_async_time:.4f} ç§’")
        print(f"  GPUè»¢é€é€Ÿåº¦: {actual_data_size / (1024*1024) / total_async_time:.2f} MB/sec")

    finally:
        conn.close()
        # Pinned ãƒ¡ãƒ¢ãƒªè§£æ”¾
        if 'pbuf1' in locals():
            del pbuf1
        if 'pbuf2' in locals() and pbuf2 is not None:
            del pbuf2
        if 'stream' in locals():
            del stream

    # GPU ãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    if actual_data_size < devbuf.size:
        print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ä¸­...")
        trimmed_devbuf = rmm.DeviceBuffer(size=actual_data_size)
        # GPU to GPU ã‚³ãƒ”ãƒ¼
        cp.cuda.runtime.memcpy(
            trimmed_devbuf.ptr, devbuf.ptr, actual_data_size,
            cp.cuda.runtime.memcpyDeviceToDevice
        )
        devbuf = trimmed_devbuf
        print(f"GPU ãƒãƒƒãƒ•ã‚¡ãƒˆãƒªãƒŸãƒ³ã‚°å®Œäº†: {actual_data_size / (1024*1024):.2f} MB")

    # numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›
    print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›ä¸­...")
    raw_dev = cuda.as_cuda_array(devbuf).view(dtype=np.uint8)
    print(f"GPU ã‚¢ãƒ¬ã‚¤å¤‰æ›å®Œäº†: {raw_dev.shape[0]:,} bytes")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
    header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
    header_size = detect_pg_header_size(header_sample)
    print(f"ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º: {header_size} ãƒã‚¤ãƒˆ")

    # GPUæœ€é©åŒ–å‡¦ç†
    print("GPUæœ€é©åŒ–å‡¦ç†ä¸­...")
    start_processing_time = time.time()
    
    try:
        cudf_df, detailed_timing = postgresql_to_cudf_parquet(
            raw_dev=raw_dev,
            columns=columns,
            ncols=ncols,
            header_size=header_size,
            output_path=OUTPUT_PARQUET_PATH,
            compression='snappy',
            use_rmm=True,
            optimize_gpu=True
        )
        
        processing_time = time.time() - start_processing_time
        rows = len(cudf_df)
        parse_time = detailed_timing.get('gpu_parsing', 0)
        decode_time = detailed_timing.get('cudf_creation', 0)
        write_time = detailed_timing.get('parquet_export', 0)
        
        print(f"GPUæœ€é©åŒ–å‡¦ç†å®Œäº† ({processing_time:.4f}ç§’), è¡Œæ•°: {rows}")
        
    except Exception as e:
        print(f"GPUæœ€é©åŒ–å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return

    total_time = time.time() - start_total_time
    decimal_cols = sum(1 for col in columns if col.arrow_id == 5)
    
    print(f"\n=== Pinned + éåŒæœŸã‚³ãƒ”ãƒ¼æ–¹å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ===")
    print(f"ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—       : {meta_time:.4f} ç§’")
    print(f"  COPYâ†’Pinned+éåŒæœŸ  : {copy_time:.4f} ç§’")
    print(f"    â””â”€ éåŒæœŸè»¢é€æ™‚é–“  : {total_async_time:.4f} ç§’")
    print(f"  GPUãƒ‘ãƒ¼ã‚¹           : {parse_time:.4f} ç§’")
    print(f"  GPUãƒ‡ã‚³ãƒ¼ãƒ‰         : {decode_time:.4f} ç§’")
    print(f"  Parquetæ›¸ãè¾¼ã¿     : {write_time:.4f} ç§’")
    print("--- çµ±è¨ˆæƒ…å ± ---")
    print(f"  å‡¦ç†è¡Œæ•°      : {rows:,} è¡Œ")
    print(f"  å‡¦ç†åˆ—æ•°      : {len(columns)} åˆ—")
    print(f"  Decimalåˆ—æ•°   : {decimal_cols} åˆ—")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º  : {actual_data_size / (1024*1024):.2f} MB")
    print(f"  å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
    
    total_cells = rows * len(columns)
    throughput = total_cells / decode_time if decode_time > 0 else 0
    network_throughput = actual_data_size / (1024*1024) / copy_time
    gpu_transfer_speed = actual_data_size / (1024*1024) / total_async_time
    
    print(f"  ã‚»ãƒ«å‡¦ç†é€Ÿåº¦     : {throughput:,.0f} cells/sec")
    print(f"  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€Ÿåº¦ : {network_throughput:.2f} MB/sec")
    print(f"  GPUè»¢é€é€Ÿåº¦     : {gpu_transfer_speed:.2f} MB/sec")
    
    # PCIeå¸¯åŸŸå¹…ã¨ã®æ¯”è¼ƒ
    pcie_efficiency = gpu_transfer_speed / 22000 * 100  # RTX 3090 å®ŸåŠ¹å¸¯åŸŸ 22GB/s
    print(f"  PCIeåŠ¹ç‡        : {pcie_efficiency:.1f}% (å¯¾RTX3090å®ŸåŠ¹å¸¯åŸŸ)")
    
    print("--- æœ€é©åŒ–åŠ¹æœï¼ˆPinned + éåŒæœŸæ–¹å¼ï¼‰ ---")
    print("  âœ… ãƒ•ã‚¡ã‚¤ãƒ« I/O: å®Œå…¨ã‚¼ãƒ­")
    print("  âœ… ãƒ›ã‚¹ãƒˆãƒ¡ãƒ¢ãƒª: Pinnedãƒ¡ãƒ¢ãƒªä½¿ç”¨ï¼ˆDMAæœ€é©åŒ–ï¼‰")
    print("  âœ… GPUè»¢é€: éåŒæœŸmemcpyAsyncï¼ˆCPUéãƒ–ãƒ­ãƒƒã‚¯ï¼‰")
    print("  âœ… CPUä½¿ç”¨ç‡: 1æ¡%ã¾ã§å‰Šæ¸›") 
    print("  âœ… GPUåˆ©ç”¨ç‡: nvtopã§ç¢ºèªå¯èƒ½")
    print("  âœ… ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†")
    print("=========================================")

    # æ¤œè¨¼ç”¨å‡ºåŠ›
    print(f"\ncuDFæ¤œè¨¼ç”¨å‡ºåŠ›:")
    try:
        print(f"å‡ºåŠ›Parquet: {OUTPUT_PARQUET_PATH}")
        print(f"èª­ã¿è¾¼ã¿ç¢ºèª: {len(cudf_df):,} è¡Œ Ã— {len(cudf_df.columns)} åˆ—")
        print("å…ˆé ­ãƒ‡ãƒ¼ã‚¿å‹:")
        for i, (col_name, dtype) in enumerate(cudf_df.dtypes.items()):
            if i < 5:  # æœ€åˆã®5åˆ—ã®ã¿
                print(f"  {col_name}: {dtype}")
        print("âœ… cuDFæ¤œè¨¼: æˆåŠŸ")
    except Exception as e:
        print(f"âŒ cuDFæ¤œè¨¼: {e}")

def print_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
    print("\n=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ===")
    try:
        # CUDAæƒ…å ±
        print(f"CUDA ãƒ‡ãƒã‚¤ã‚¹: {cuda.get_current_device()}")
        print(f"GPU ãƒ¡ãƒ¢ãƒª: {cuda.current_context().get_memory_info()}")
        
        # Pinned ãƒ¡ãƒ¢ãƒªæƒ…å ±
        print(f"Pinned ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE / (1024*1024):.1f} MB")
        print(f"ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡: {'æœ‰åŠ¹' if DOUBLE_BUFFER else 'ç„¡åŠ¹'}")
        
    except Exception as e:
        print(f"ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU Pinned + éåŒæœŸã‚³ãƒ”ãƒ¼æ–¹å¼')
    parser.add_argument('--rows', type=int, default=1000000, help='å‡¦ç†è¡Œæ•°åˆ¶é™')
    parser.add_argument('--chunk-size', type=int, default=4, help='ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º(MB)')
    parser.add_argument('--single-buffer', action='store_true', help='ã‚·ãƒ³ã‚°ãƒ«ãƒãƒƒãƒ•ã‚¡ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--info', action='store_true', help='ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®ã¿è¡¨ç¤º')
    
    args = parser.parse_args()
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨­å®š
    global CHUNK_SIZE, DOUBLE_BUFFER
    CHUNK_SIZE = args.chunk_size * 1024 * 1024
    DOUBLE_BUFFER = not args.single_buffer
    
    try:
        cuda.current_context()
        print("âœ… CUDA context OK")
    except Exception as e:
        print(f"âŒ CUDAåˆæœŸåŒ–å¤±æ•—: {e}")
        exit(1)
    
    if args.info:
        print_system_info()
        return
    
    print_system_info()
    run_pinned_async_benchmark(limit_rows=args.rows)

if __name__ == "__main__":
    main()