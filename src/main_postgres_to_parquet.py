"""
cuDF ZeroCopyçµ±åˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼

ä»¥ä¸‹ã®æœ€é©åŒ–ã‚’çµ±åˆ:
1. ä¸¦åˆ—åŒ–ã•ã‚ŒãŸè¡Œæ¤œå‡ºãƒ»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŠ½å‡º
2. cuDFã«ã‚ˆã‚‹ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼Arrowå¤‰æ›
3. GPUç›´æ¥Parquetæ›¸ãå‡ºã—
4. ãƒ¡ãƒ¢ãƒªã‚³ã‚¢ãƒ¬ãƒƒã‚·ãƒ³ã‚°æœ€é©åŒ–
5. RMMçµ±åˆãƒ¡ãƒ¢ãƒªç®¡ç†
"""

from __future__ import annotations
from typing import List, Dict, Any, Optional, Tuple
import os
import time
import warnings
import numpy as np
import cupy as cp
import cudf
import cudf.core.dtypes
from numba import cuda
import pyarrow as pa
import rmm

from .types import (
    ColumnMeta, INT16, INT32, INT64, FLOAT32, FLOAT64, DECIMAL128, 
    UTF8, BINARY, DATE32, TS64_US, BOOL, UNKNOWN
)
from .memory_manager import GPUMemoryManager, BufferInfo
from .cuda_kernels.optimized_parsers import (
    parse_binary_chunk_gpu_enhanced,
    optimize_grid_size
)
from .cuda_kernels.column_processor import (
    pass1_column_wise_integrated
)
from .cuda_kernels.decimal_tables import (
    POW10_TABLE_LO_HOST, POW10_TABLE_HI_HOST
)
from .build_cudf_from_buf import CuDFZeroCopyProcessor
from .build_buf_from_postgres import detect_pg_header_size
from .write_parquet_from_cudf import write_cudf_to_parquet_with_options


class ZeroCopyProcessor:
    """ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
    
    def __init__(self, use_rmm: bool = True, optimize_gpu: bool = True):
        """
        åˆæœŸåŒ–
        
        Args:
            use_rmm: RMM (Rapids Memory Manager) ã‚’ä½¿ç”¨
            optimize_gpu: GPUæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
        """
        self.use_rmm = use_rmm
        self.optimize_gpu = optimize_gpu
        self.cudf_processor = CuDFZeroCopyProcessor(use_rmm=use_rmm)
        self.gmm = GPUMemoryManager()
        self.device_props = self._get_device_properties()
        
        # RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
        if use_rmm:
            try:
                rmm.reinitialize(
                    pool_allocator=True,
                    initial_pool_size=2**31,  # 2GB
                    maximum_pool_size=2**33   # 8GB
                )
                print("RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº† (æœ€å¤§8GB)")
            except Exception as e:
                warnings.warn(f"RMMåˆæœŸåŒ–è­¦å‘Š: {e}")
    
    def _get_device_properties(self) -> dict:
        """ç¾åœ¨ã®GPUãƒ‡ãƒã‚¤ã‚¹ç‰¹æ€§ã‚’å–å¾—"""
        try:
            device = cuda.get_current_device()
            return {
                'MAX_THREADS_PER_BLOCK': device.MAX_THREADS_PER_BLOCK,
                'MULTIPROCESSOR_COUNT': device.MULTIPROCESSOR_COUNT,
                'MAX_GRID_DIM_X': device.MAX_GRID_DIM_X,
                'GLOBAL_MEMORY': device.TOTAL_MEMORY,
                'SHARED_MEMORY_PER_BLOCK': device.MAX_SHARED_MEMORY_PER_BLOCK,
                'WARP_SIZE': device.WARP_SIZE
            }
        except Exception as e:
            warnings.warn(f"GPUç‰¹æ€§å–å¾—å¤±æ•—: {e}")
            return {
                'MAX_THREADS_PER_BLOCK': 1024,
                'MULTIPROCESSOR_COUNT': 16,
                'MAX_GRID_DIM_X': 65535,
                'GLOBAL_MEMORY': 8 * 1024**3,
                'SHARED_MEMORY_PER_BLOCK': 48 * 1024,
                'WARP_SIZE': 32
            }
    
    def create_string_buffers(
        self,
        columns: List[ColumnMeta],
        rows: int,
        raw_dev,
        field_offsets_dev,
        field_lengths_dev
    ) -> Dict[str, Any]:
        """
        æ–‡å­—åˆ—ãƒãƒƒãƒ•ã‚¡ä½œæˆ
        
        ãƒ¡ãƒ¢ãƒªã‚³ã‚¢ãƒ¬ãƒƒã‚·ãƒ³ã‚°ã¨ãƒ¯ãƒ¼ãƒ—åŠ¹ç‡ã‚’è€ƒæ…®
        """
        
        string_buffers = {}
        var_columns = [col for col in columns if col.is_variable and (col.arrow_id == UTF8 or col.arrow_id == BINARY)]
        
        if not var_columns:
            return string_buffers
        
        for col_idx, col in enumerate(var_columns):
            # å¯¾å¿œã™ã‚‹åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢
            actual_col_idx = None
            for i, c in enumerate(columns):
                if c.name == col.name:
                    actual_col_idx = i
                    break
            
            if actual_col_idx is None:
                continue
            
            try:
                # === 1. é•·ã•é…åˆ—ã®ä¸¦åˆ—æŠ½å‡º ===
                d_lengths = cuda.device_array(rows, dtype=np.int32)
                
                # ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºã®è¨ˆç®—
                blocks, threads = optimize_grid_size(0, rows, self.device_props)
                
                @cuda.jit
                def extract_lengths_coalesced(field_lengths, col_idx, lengths_out, num_rows):
                    """ãƒ¯ãƒ¼ãƒ—åŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸé•·ã•æŠ½å‡º"""
                    row = cuda.grid(1)
                    if row < num_rows:
                        lengths_out[row] = field_lengths[row, col_idx]
                
                extract_lengths_coalesced[blocks, threads](
                    field_lengths_dev, actual_col_idx, d_lengths, rows
                )
                cuda.synchronize()
                
                # === 2. GPUä¸Šã§ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—ï¼ˆCuPyä½¿ç”¨ç‰ˆï¼‰ ===
                # æ–‡å­—åˆ—é•·é…åˆ—ã‚’CuPyã«å¤‰æ›
                lengths_cupy = cp.asarray(d_lengths)
                
                # CuPyã®cumsumä½¿ç”¨ï¼ˆGPUå®Ÿè¡Œï¼‰
                offsets_cumsum = cp.cumsum(lengths_cupy, dtype=cp.int32)
                
                # ã‚ªãƒ•ã‚»ãƒƒãƒˆé…åˆ—ã‚’ä½œæˆï¼ˆ0ã‚’å…ˆé ­ã«è¿½åŠ ï¼‰
                d_offsets = cuda.device_array(rows + 1, dtype=np.int32)
                d_offsets[0] = 0  # æœ€åˆã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã¯0
                
                # CuPyã®çµæœã‚’Numbaé…åˆ—ã«ã‚³ãƒ”ãƒ¼
                @cuda.jit
                def copy_cumsum_to_offsets(cumsum_data, offsets_out, num_rows):
                    """CuPy cumsumçµæœã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆé…åˆ—ã«ã‚³ãƒ”ãƒ¼"""
                    idx = cuda.grid(1)
                    if idx < num_rows:
                        offsets_out[idx + 1] = cumsum_data[idx]
                
                copy_cumsum_to_offsets[blocks, threads](
                    cp.asarray(offsets_cumsum), d_offsets, rows
                )
                cuda.synchronize()
                
                # ãƒ‡ãƒãƒƒã‚°: GPUç´¯ç©å’Œã®æ¤œè¨¼
                try:
                    offsets_host = d_offsets.copy_to_host()
                    lengths_host = d_lengths.copy_to_host()
                    print(f"=== GPUç´¯ç©å’Œæ¤œè¨¼ ({col.name}) ===")
                    print(f"æ–‡å­—åˆ—é•·ã®æœ€åˆã®10è¦ç´ : {lengths_host[:10]}")
                    print(f"ã‚ªãƒ•ã‚»ãƒƒãƒˆã®æœ€åˆã®10è¦ç´ : {offsets_host[:10]}")
                    print(f"ã‚ªãƒ•ã‚»ãƒƒãƒˆã®æœ€å¾Œã®10è¦ç´ : {offsets_host[-10:]}")
                    print(f"æ–‡å­—åˆ—é•·çµ±è¨ˆ: min={lengths_host.min()}, max={lengths_host.max()}, avg={lengths_host.mean():.2f}")
                    print(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {offsets_host[-1]}")
                    print("=== æ¤œè¨¼çµ‚äº† ===")
                except Exception as e:
                    print(f"GPUç´¯ç©å’Œæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å–å¾—
                total_size_array = d_offsets[rows:rows+1].copy_to_host()
                total_size = int(total_size_array[0]) if len(total_size_array) > 0 else rows * 50
                
                if total_size == 0:
                    string_buffers[col.name] = {'data': None, 'offsets': None, 'actual_size': 0}
                    continue
                
                # === 3. ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡ã®ä¸¦åˆ—ã‚³ãƒ”ãƒ¼ ===
                d_data = cuda.device_array(total_size, dtype=np.uint8)
                
                @cuda.jit
                def copy_string_data_coalesced(
                    raw_data, field_offsets, field_lengths,
                    col_idx, data_out, offsets, num_rows
                ):
                    """ãƒ¡ãƒ¢ãƒªã‚³ã‚¢ãƒ¬ãƒƒã‚·ãƒ³ã‚°ã‚’è€ƒæ…®ã—ãŸæ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼"""
                    row = cuda.grid(1)
                    if row >= num_rows:
                        return
                    
                    field_offset = field_offsets[row, col_idx]
                    field_length = field_lengths[row, col_idx]
                    output_offset = offsets[row]
                    
                    # ãƒ¯ãƒ¼ãƒ—å†…ã§å”èª¿çš„ãªã‚³ãƒ”ãƒ¼
                    warp_id = row // 32
                    lane_id = row % 32
                    
                    # å„ã‚¹ãƒ¬ãƒƒãƒ‰ãŒè‡ªåˆ†ã®æ‹…å½“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
                    for i in range(field_length):
                        src_idx = field_offset + i
                        dst_idx = output_offset + i
                        
                        if (src_idx < raw_data.size and 
                            dst_idx < data_out.size):
                            data_out[dst_idx] = raw_data[src_idx]
                
                copy_string_data_coalesced[blocks, threads](
                    raw_dev, field_offsets_dev, field_lengths_dev,
                    actual_col_idx, d_data, d_offsets, rows
                )
                cuda.synchronize()
                
                string_buffers[col.name] = {
                    'data': d_data,
                    'offsets': d_offsets,
                    'actual_size': total_size
                }
                
            except Exception as e:
                warnings.warn(f"æ–‡å­—åˆ—ãƒãƒƒãƒ•ã‚¡ä½œæˆã‚¨ãƒ©ãƒ¼ ({col.name}): {e}")
                string_buffers[col.name] = {'data': None, 'offsets': None, 'actual_size': 0}
        
        return string_buffers
    
    def decode_and_export(
        self,
        raw_dev: cuda.cudadrv.devicearray.DeviceNDArray,
        field_offsets_dev,
        field_lengths_dev,
        columns: List[ColumnMeta],
        output_path: str,
        compression: str = 'snappy',
        **parquet_kwargs
    ) -> Tuple[cudf.DataFrame, Dict[str, float]]:
        """
        çµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ + ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå‡¦ç†
        
        Returns:
            (cudf_dataframe, timing_info)
        """
        
        timing_info = {}
        start_time = time.time()
        
        rows, ncols = field_lengths_dev.shape
        if rows == 0:
            raise ValueError("rows == 0")

        # === 1. å‰å‡¦ç†ã¨ãƒ¡ãƒ¢ãƒªæº–å‚™ ===
        prep_start = time.time()
        
        # Decimalå‡¦ç†ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«
        d_pow10_table_lo = cuda.to_device(POW10_TABLE_LO_HOST)
        d_pow10_table_hi = cuda.to_device(POW10_TABLE_HI_HOST)

        # æ–‡å­—åˆ—ãƒãƒƒãƒ•ã‚¡ä½œæˆ
        string_buffers = self.create_string_buffers(
            columns, rows, raw_dev, field_offsets_dev, field_lengths_dev
        )

        # çµ±åˆãƒãƒƒãƒ•ã‚¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        buffer_info = self.gmm.initialize_buffers(columns, rows)
        
        # NULLé…åˆ—
        d_nulls_all = cuda.device_array((rows, ncols), dtype=np.uint8)
        
        timing_info['preparation'] = time.time() - prep_start

        # === 2. çµ±åˆã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ ===
        kernel_start = time.time()
        
        # Grid/Blockã‚µã‚¤ã‚ºã®è¨ˆç®—
        blocks, threads = optimize_grid_size(0, rows, self.device_props)
        
        print(f"çµ±åˆã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ: {blocks} blocks Ã— {threads} threads")
        
        try:
            pass1_column_wise_integrated[blocks, threads](
                raw_dev,
                field_offsets_dev,
                field_lengths_dev,
                
                # åˆ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é…åˆ—
                buffer_info.column_types,
                buffer_info.column_is_variable,
                buffer_info.column_indices,
                
                # å›ºå®šé•·çµ±åˆãƒãƒƒãƒ•ã‚¡
                buffer_info.fixed_buffer,
                buffer_info.fixed_column_offsets,
                buffer_info.fixed_column_sizes,
                buffer_info.fixed_decimal_scales,
                buffer_info.row_stride,
                
                # å¯å¤‰é•·çµ±åˆãƒãƒƒãƒ•ã‚¡
                buffer_info.var_data_buffer,
                buffer_info.var_offset_arrays,
                buffer_info.var_column_mapping,
                
                # å…±é€šå‡ºåŠ›
                d_nulls_all,
                
                # Decimalå‡¦ç†ç”¨
                d_pow10_table_lo,
                d_pow10_table_hi
            )
            
            cuda.synchronize()
            
        except Exception as e:
            raise RuntimeError(f"çµ±åˆã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        timing_info['kernel_execution'] = time.time() - kernel_start

        # === 3. cuDF DataFrameä½œæˆï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰ ===
        cudf_start = time.time()
        
        cudf_df = self.cudf_processor.create_cudf_from_gpu_buffers_zero_copy(
            columns, rows, buffer_info, string_buffers
        )
        
        timing_info['cudf_creation'] = time.time() - cudf_start

        # === 4. Parquetæ›¸ãå‡ºã— ===
        export_start = time.time()
        
        parquet_timing = write_cudf_to_parquet_with_options(
            cudf_df,
            output_path,
            compression=compression,
            optimize_for_spark=True,
            **parquet_kwargs
        )
        
        timing_info['parquet_export'] = time.time() - export_start
        timing_info['parquet_details'] = parquet_timing
        timing_info['total'] = time.time() - start_time
        
        return cudf_df, timing_info
    
    def process_postgresql_to_parquet(
        self,
        raw_dev: cuda.cudadrv.devicearray.DeviceNDArray,
        columns: List[ColumnMeta],
        ncols: int,
        header_size: int,
        output_path: str,
        compression: str = 'snappy',
        **kwargs
    ) -> Tuple[cudf.DataFrame, Dict[str, float]]:
        """
        PostgreSQL â†’ cuDF â†’ GPU Parquet ã®çµ±åˆå‡¦ç†
        
        æœ€é©åŒ–ã‚’é©ç”¨ã—ãŸé«˜æ€§èƒ½ç‰ˆ
        """
        
        total_timing = {}
        overall_start = time.time()
        
        # === 1. GPUãƒ‘ãƒ¼ã‚¹ ===
        parse_start = time.time()
        
        print("=== GPUä¸¦åˆ—ãƒ‘ãƒ¼ã‚¹é–‹å§‹ ===")
        
        # Ultra Fastç‰ˆã®åˆ©ç”¨ã‚’è©¦è¡Œ
        use_ultra_fast = self.optimize_gpu and kwargs.get('use_ultra_fast', True)
        
        if use_ultra_fast:
            try:
                from .cuda_kernels.ultra_fast_parser import parse_binary_chunk_gpu_ultra_fast
                print("ğŸš€ Ultra Fast Parser ä½¿ç”¨")
                field_offsets_dev, field_lengths_dev = parse_binary_chunk_gpu_ultra_fast(
                    raw_dev, ncols, header_size=header_size
                )
                parser_used = "Ultra Fast"
            except Exception as e:
                print(f"âš ï¸ Ultra Fast Parser ã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ”„ å¾“æ¥ç‰ˆãƒ‘ãƒ¼ã‚µãƒ¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                use_ultra_fast = False
        
        if not use_ultra_fast:
            # å¾“æ¥ç‰ˆãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨
            from .build_buf_from_postgres import parse_binary_chunk_gpu
            field_offsets_dev, field_lengths_dev = parse_binary_chunk_gpu(
                raw_dev, ncols, threads_per_block=256, header_size=header_size
            )
            parser_used = "å¾“æ¥ç‰ˆ"
        
        rows = field_offsets_dev.shape[0]
        total_timing['gpu_parsing'] = time.time() - parse_start
        print(f"âœ… {parser_used}ãƒ‘ãƒ¼ã‚¹å®Œäº†: {rows} è¡Œ ({total_timing['gpu_parsing']:.4f}ç§’)")
        
        # === 2. çµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ + ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ===
        decode_start = time.time()
        
        print("=== çµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰é–‹å§‹ ===")
        cudf_df, decode_timing = self.decode_and_export(
            raw_dev, field_offsets_dev, field_lengths_dev,
            columns, output_path, compression, **kwargs
        )
        
        total_timing['decode_and_export'] = time.time() - decode_start
        total_timing.update(decode_timing)
        total_timing['overall_total'] = time.time() - overall_start
        
        # === 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ ===
        self._print_performance_stats(rows, len(columns), total_timing, len(raw_dev))
        
        return cudf_df, total_timing
    
    def _print_performance_stats(
        self, 
        rows: int, 
        cols: int, 
        timing: Dict[str, float], 
        data_size: int
    ):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®è¡¨ç¤º"""
        
        print(f"\n=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ ===")
        print(f"å‡¦ç†ãƒ‡ãƒ¼ã‚¿: {rows:,} è¡Œ Ã— {cols} åˆ—")
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data_size / (1024**2):.2f} MB")
        
        print("\n--- è©³ç´°ã‚¿ã‚¤ãƒŸãƒ³ã‚° ---")
        for key, value in timing.items():
            if isinstance(value, (int, float)):
                print(f"  {key:20}: {value:.4f} ç§’")
            elif isinstance(value, dict):
                print(f"  {key:20}: (è©³ç´°ã¯çœç•¥)")
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, (int, float)):
                        print(f"    {sub_key:18}: {sub_value:.4f}")
                    else:
                        print(f"    {sub_key:18}: {sub_value}")
            else:
                print(f"  {key:20}: {str(value)}")
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        total_cells = rows * cols
        overall_time = timing.get('overall_total', timing.get('total', 1.0))
        
        if overall_time > 0:
            cell_throughput = total_cells / overall_time
            data_throughput = (data_size / (1024**2)) / overall_time
            
            print(f"\n--- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ ---")
            print(f"  ã‚»ãƒ«å‡¦ç†é€Ÿåº¦: {cell_throughput:,.0f} cells/sec")
            print(f"  ãƒ‡ãƒ¼ã‚¿å‡¦ç†é€Ÿåº¦: {data_throughput:.2f} MB/sec")
            
            # GPUåŠ¹ç‡æŒ‡æ¨™
            if 'kernel_execution' in timing:
                kernel_efficiency = (timing['kernel_execution'] / overall_time) * 100
                print(f"  GPUä½¿ç”¨åŠ¹ç‡: {kernel_efficiency:.1f}%")
        
        print("=" * 30)


def ultimate_postgresql_to_cudf_parquet(
    raw_dev: cuda.cudadrv.devicearray.DeviceNDArray,
    columns: List[ColumnMeta],
    ncols: int,
    header_size: int,
    output_path: str,
    compression: str = 'snappy',
    use_rmm: bool = True,
    optimize_gpu: bool = True,
    **parquet_kwargs
) -> Tuple[cudf.DataFrame, Dict[str, float]]:
    """
    PostgreSQL â†’ cuDF â†’ GPU Parquet çµ±åˆå‡¦ç†é–¢æ•°
    
    æœ€é©åŒ–æŠ€è¡“ã‚’çµ±åˆã—ãŸé«˜æ€§èƒ½ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š
    - ä¸¦åˆ—åŒ–GPUè¡Œæ¤œå‡ºãƒ»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŠ½å‡º
    - ãƒ¡ãƒ¢ãƒªã‚³ã‚¢ãƒ¬ãƒƒã‚·ãƒ³ã‚°æœ€é©åŒ–
    - cuDFã‚¼ãƒ­ã‚³ãƒ”ãƒ¼Arrowå¤‰æ›
    - GPUç›´æ¥Parquetæ›¸ãå‡ºã—
    - RMMçµ±åˆãƒ¡ãƒ¢ãƒªç®¡ç†
    
    Args:
        raw_dev: GPUä¸Šã®PostgreSQLãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
        columns: åˆ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        ncols: åˆ—æ•°
        header_size: ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º
        output_path: Parquetå‡ºåŠ›ãƒ‘ã‚¹
        compression: åœ§ç¸®æ–¹å¼
        use_rmm: RMMä½¿ç”¨ãƒ•ãƒ©ã‚°
        optimize_gpu: GPUæœ€é©åŒ–ãƒ•ãƒ©ã‚°
        **parquet_kwargs: è¿½åŠ ã®Parquetã‚ªãƒ—ã‚·ãƒ§ãƒ³
    
    Returns:
        (cudf_dataframe, timing_information)
    """
    
    processor = ZeroCopyProcessor(
        use_rmm=use_rmm, 
        optimize_gpu=optimize_gpu
    )
    
    return processor.process_postgresql_to_parquet(
        raw_dev, columns, ncols, header_size, output_path, 
        compression, **parquet_kwargs
    )


__all__ = [
    "ZeroCopyProcessor",
    "ultimate_postgresql_to_cudf_parquet"
]