"""
PostgreSQL â†’ Rust â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆ
Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã§çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
"""

import os
import time
import subprocess
import json
import numpy as np
from numba import cuda
import rmm
import cudf
import cupy as cp
import kvikio
from pathlib import Path
from typing import List, Dict, Any, Optional
import psycopg
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys
import pandas as pd

from src.types import ColumnMeta, PG_OID_TO_ARROW, UNKNOWN
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet_direct
from src.cuda_kernels.postgres_binary_parser import detect_pg_header_size
from src.readPostgres.metadata import fetch_column_meta
import pyarrow.parquet as pq

TABLE_NAME = "lineorder"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå®Ÿè¡Œæ™‚ã«ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
OUTPUT_DIR = "/dev/shm"
RUST_BINARY = "/home/ubuntu/gpupgparser/rust_bench_optimized/target/release/pg_fast_copy_single_chunk"
MAX_QUEUE_SIZE = 3  # ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚º

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
chunk_stats = []
gpu_row_counts = {}  # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯IDã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸ï¼‰
shutdown_flag = threading.Event()


def signal_handler(sig, frame):
    """Ctrl+Cãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    print("\n\nâš ï¸  å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    shutdown_flag.set()
    sys.exit(1)


signal.signal(signal.SIGINT, signal_handler)


def setup_rmm_pool():
    """RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’é©åˆ‡ã«è¨­å®š"""
    try:
        if rmm.is_initialized():
            return
        
        # GPUãƒ¡ãƒ¢ãƒªã®90%ã‚’ä½¿ç”¨å¯èƒ½ã«è¨­å®š
        gpu_memory = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem']
        pool_size = int(gpu_memory * 0.9)
        
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=pool_size,
            maximum_pool_size=pool_size
        )
    except Exception as e:
        print(f"âš ï¸ RMMåˆæœŸåŒ–è­¦å‘Š: {e}")


def get_postgresql_metadata():
    """PostgreSQLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    conn = psycopg.connect(dsn)
    try:
        print(f"PostgreSQLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ (ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME})...")
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(columns)} åˆ—")
        
        
        return columns
    finally:
        conn.close()


def cleanup_files(total_chunks=8):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    files = [
        f"{OUTPUT_DIR}/{TABLE_NAME}_meta_0.json",
        f"{OUTPUT_DIR}/{TABLE_NAME}_data_0.ready"
    ] + [f"{OUTPUT_DIR}/chunk_{i}.bin" for i in range(total_chunks)]
    
    for f in files:
        if os.path.exists(f):
            os.remove(f)


def rust_producer(chunk_queue: queue.Queue, total_chunks: int, stats_queue: queue.Queue):
    """Rustè»¢é€ã‚’å®Ÿè¡Œã™ã‚‹Producerã‚¹ãƒ¬ãƒƒãƒ‰"""
    for chunk_id in range(total_chunks):
        if shutdown_flag.is_set():
            break
            
        try:
            print(f"\n[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}/{total_chunks} Rustè»¢é€é–‹å§‹...")
            
            env = os.environ.copy()
            env['CHUNK_ID'] = str(chunk_id)
            env['TOTAL_CHUNKS'] = str(total_chunks)
            
            rust_start = time.time()
            process = subprocess.run(
                [RUST_BINARY],
                capture_output=True,
                text=True,
                env=env
            )
            
            if process.returncode != 0:
                print(f"âŒ Rustã‚¨ãƒ©ãƒ¼: {process.stderr}")
                continue
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€Rustã®å‡ºåŠ›ã‚’è¡¨ç¤º
            if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
                for line in process.stdout.split('\n'):
                    if 'ãƒãƒ£ãƒ³ã‚¯' in line or 'ãƒšãƒ¼ã‚¸' in line or 'COPYç¯„å›²' in line:
                        print(f"[Rust Debug] {line}")
            
            # JSONçµæœã‚’æŠ½å‡º
            output = process.stdout
            json_start = output.find("===CHUNK_RESULT_JSON===")
            json_end = output.find("===END_CHUNK_RESULT_JSON===")
            
            if json_start != -1 and json_end != -1:
                json_str = output[json_start + len("===CHUNK_RESULT_JSON==="):json_end].strip()
                result = json.loads(json_str)
                rust_time = result['elapsed_seconds']
                file_size = result['total_bytes']
                chunk_file = result['chunk_file']
            else:
                rust_time = time.time() - rust_start
                chunk_file = f"{OUTPUT_DIR}/{TABLE_NAME}_chunk_{chunk_id}.bin"
                file_size = os.path.getsize(chunk_file)
            
            chunk_info = {
                'chunk_id': chunk_id,
                'chunk_file': chunk_file,
                'file_size': file_size,
                'rust_time': rust_time
            }
            
            print(f"[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} è»¢é€å®Œäº† ({rust_time:.1f}ç§’, {file_size / 1024**3:.1f}GB)")
            
            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_queue.put(chunk_info)
            stats_queue.put(('rust_time', rust_time))
            
        except Exception as e:
            print(f"[Producer] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
    chunk_queue.put(None)
    print("[Producer] å…¨ãƒãƒ£ãƒ³ã‚¯è»¢é€å®Œäº†")


def gpu_consumer(chunk_queue: queue.Queue, columns: List[ColumnMeta], consumer_id: int, stats_queue: queue.Queue, total_chunks: int):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹Consumerã‚¹ãƒ¬ãƒƒãƒ‰"""
    while not shutdown_flag.is_set():
        try:
            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_info = chunk_queue.get(timeout=1)
            
            if chunk_info is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                break
                
            chunk_id = chunk_info['chunk_id']
            chunk_file = chunk_info['chunk_file']
            file_size = chunk_info['file_size']
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†é–‹å§‹...")
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            gpu_start = time.time()
            
            # kvikio+RMMã§ç›´æ¥GPUè»¢é€
            transfer_start = time.time()
            
            # RMM DeviceBufferã‚’ä½¿ç”¨
            gpu_buffer = rmm.DeviceBuffer(size=file_size)
            
            # kvikioã§ç›´æ¥èª­ã¿è¾¼ã¿
            with kvikio.CuFile(chunk_file, "rb") as f:
                gpu_array = cp.asarray(gpu_buffer).view(dtype=cp.uint8)
                bytes_read = f.read(gpu_array)
            
            if bytes_read != file_size:
                raise RuntimeError(f"èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºä¸ä¸€è‡´: {bytes_read} != {file_size}")
            
            # numba cudaé…åˆ—ã«å¤‰æ›ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
            raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
            
            transfer_time = time.time() - transfer_start
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
            header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
            header_size = detect_pg_header_size(header_sample)
            
            # ç›´æ¥æŠ½å‡ºå‡¦ç†
            chunk_output = f"output/{TABLE_NAME}_chunk_{chunk_id}_queue.parquet"
            
            # ãƒãƒ£ãƒ³ã‚¯IDã¨æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
            os.environ['GPUPGPARSER_CURRENT_CHUNK'] = str(chunk_id)
            if chunk_id == total_chunks - 1:
                os.environ['GPUPGPARSER_LAST_CHUNK'] = '1'
            else:
                os.environ['GPUPGPARSER_LAST_CHUNK'] = '0'
            
            cudf_df, detailed_timing = postgresql_to_cudf_parquet_direct(
                raw_dev=raw_dev,
                columns=columns,
                ncols=len(columns),
                header_size=header_size,
                output_path=chunk_output,
                compression='snappy',
                use_rmm=True,
                optimize_gpu=True,
                verbose=False,
                test_mode=(os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1')
            )
            
            gpu_time = time.time() - gpu_start
            
            # å‡¦ç†çµ±è¨ˆ
            rows = len(cudf_df) if cudf_df is not None else 0
            
            # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜
            gpu_row_counts[chunk_id] = rows
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†å®Œäº† ({gpu_time:.1f}ç§’, {rows:,}è¡Œ)")
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
            if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1':
                print(f"[CHUNK DEBUG] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}: ")
                print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size / 1024**2:.1f} MB")
                print(f"  - æ¤œå‡ºè¡Œæ•°: {rows:,}è¡Œ")
                print(f"  - GPUãƒ‘ãƒ¼ã‚¹æ™‚é–“: {detailed_timing.get('gpu_parsing', 0):.2f}ç§’")
                print(f"  - è¡Œã‚ãŸã‚Š: {file_size/rows if rows > 0 else 0:.1f} bytes/row")
            
            # çµ±è¨ˆæƒ…å ±ã‚’é€ä¿¡
            stats_queue.put(('gpu_time', gpu_time))
            stats_queue.put(('transfer_time', transfer_time))
            stats_queue.put(('rows', rows))
            stats_queue.put(('size', file_size))
            
            # è©³ç´°çµ±è¨ˆã‚’ä¿å­˜
            chunk_stats.append({
                'chunk_id': chunk_id,
                'consumer_id': consumer_id,
                'rust_time': chunk_info['rust_time'],
                'gpu_time': gpu_time,
                'transfer_time': transfer_time,
                'parse_time': detailed_timing.get('gpu_parsing', 0),
                'string_time': detailed_timing.get('string_buffer_creation', 0),
                'write_time': detailed_timing.get('parquet_export', 0),
                'rows': rows,
                'size_gb': file_size / 1024**3
            })
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del raw_dev
            del gpu_buffer
            del gpu_array
            if cudf_df is not None:
                del cudf_df
            
            mempool.free_all_blocks()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if os.path.exists(chunk_file):
                os.remove(chunk_file)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"[Consumer-{consumer_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[Consumer-{consumer_id}] çµ‚äº†")


def validate_parquet_output(file_path: str, num_rows: int = 5, gpu_rows: int = None) -> bool:
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        num_rows: è¡¨ç¤ºã™ã‚‹è¡Œæ•°
        gpu_rows: GPUå‡¦ç†ã§æ¤œå‡ºã—ãŸè¡Œæ•°ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    
    Returns:
        æ¤œè¨¼æˆåŠŸã®å ´åˆTrue
    """
    try:
        # PyArrowã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        table = pq.read_table(file_path)
        
        print(f"\nğŸ“Š Parquetãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {os.path.basename(file_path)}")
        print(f"â”œâ”€ è¡Œæ•°: {table.num_rows:,}", end="")
        if gpu_rows is not None:
            if table.num_rows == gpu_rows:
                print(f" âœ… OK (GPUå‡¦ç†è¡Œæ•°ã¨ä¸€è‡´)")
            else:
                print(f" âŒ NG (GPUå‡¦ç†è¡Œæ•°: {gpu_rows:,})")
        else:
            print()
        print(f"â”œâ”€ åˆ—æ•°: {table.num_columns}")
        print(f"â””â”€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(file_path) / 1024**2:.2f} MB")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­{num_rows}è¡Œï¼‰:")
        print("â”€" * 80)
        df_sample = table.slice(0, num_rows).to_pandas()
        print(df_sample.to_string(index=False, max_colwidth=20))
        print("â”€" * 80)
        
        return True
        
    except Exception as e:
        print(f"âŒ Parquetæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_parallel_pipeline(columns: List[ColumnMeta], total_chunks: int):
    """çœŸã®ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ã‚­ãƒ¥ãƒ¼ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†
    chunk_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
    stats_queue = queue.Queue()
    
    start_time = time.time()
    
    # Producerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
    producer_thread = threading.Thread(
        target=rust_producer,
        args=(chunk_queue, total_chunks, stats_queue)
    )
    producer_thread.start()
    
    # Consumerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆ1ã¤ã®ã¿ - GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼‰
    consumer_thread = threading.Thread(
        target=gpu_consumer,
        args=(chunk_queue, columns, 1, stats_queue, total_chunks)
    )
    consumer_thread.start()
    
    # çµ±è¨ˆåé›†
    total_rust_time = 0
    total_gpu_time = 0
    total_transfer_time = 0
    total_rows = 0
    total_size = 0
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿã—ãªãŒã‚‰çµ±è¨ˆã‚’åé›†
    producer_thread.join()
    consumer_thread.join()
    
    # çµ±è¨ˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœã‚’åé›†
    while not stats_queue.empty():
        stat_type, value = stats_queue.get()
        if stat_type == 'rust_time':
            total_rust_time += value
        elif stat_type == 'gpu_time':
            total_gpu_time += value
        elif stat_type == 'transfer_time':
            total_transfer_time += value
        elif stat_type == 'rows':
            total_rows += value
        elif stat_type == 'size':
            total_size += value
    
    total_time = time.time() - start_time
    
    return {
        'total_time': total_time,
        'total_rust_time': total_rust_time,
        'total_gpu_time': total_gpu_time,
        'total_transfer_time': total_transfer_time,
        'total_rows': total_rows,
        'total_size': total_size,
        'processed_chunks': len(chunk_stats)
    }


def main(total_chunks=8, table_name=None, test_mode=False):
    global TABLE_NAME
    if table_name:
        TABLE_NAME = table_name
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€GPUç‰¹æ€§ã‚’è¡¨ç¤º
    if test_mode:
        from src.cuda_kernels.postgres_binary_parser import print_gpu_properties
        print_gpu_properties()
    
    # kvikioè¨­å®šç¢ºèª
    is_compat = os.environ.get("KVIKIO_COMPAT_MODE", "").lower() in ["on", "1", "true"]
    
    # RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š
    setup_rmm_pool()
    
    # CUDA contextç¢ºèª
    try:
        cuda.current_context()
    except Exception as e:
        print(f"âŒ CUDA context ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_files(total_chunks)
    
    # PostgreSQLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    columns = get_postgresql_metadata()
    
    try:
        # ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        print("\nä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        print("=" * 80)
        
        results = run_parallel_pipeline(columns, total_chunks)
        
        # æœ€çµ‚çµ±è¨ˆã‚’æ§‹é€ åŒ–è¡¨ç¤º
        total_gb = results['total_size'] / 1024**3
        
        print(f"\n{'='*80}")
        print(f" âœ… å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")
        
        # å…¨ä½“çµ±è¨ˆ
        print(f"\nã€å…¨ä½“çµ±è¨ˆã€‘")
        print(f"â”œâ”€ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_gb:.2f} GB")
        print(f"â”œâ”€ ç·è¡Œæ•°: {results['total_rows']:,} è¡Œ")
        print(f"â”œâ”€ ç·å®Ÿè¡Œæ™‚é–“: {results['total_time']:.2f}ç§’")
        print(f"â””â”€ å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_gb / results['total_time']:.2f} GB/ç§’")
        
        # å‡¦ç†æ™‚é–“å†…è¨³
        print(f"\nã€å‡¦ç†æ™‚é–“å†…è¨³ã€‘")
        print(f"â”œâ”€ Rustè»¢é€åˆè¨ˆ: {results['total_rust_time']:.2f}ç§’ ({total_gb / results['total_rust_time']:.2f} GB/ç§’)")
        print(f"â””â”€ GPUå‡¦ç†åˆè¨ˆ: {results['total_gpu_time']:.2f}ç§’ ({total_gb / results['total_gpu_time']:.2f} GB/ç§’)")
        print(f"   â””â”€ kvikioè»¢é€: {results['total_transfer_time']:.2f}ç§’")
        
        
        # ãƒãƒ£ãƒ³ã‚¯æ¯ã®è©³ç´°çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        if chunk_stats:
            print(f"\nã€ãƒãƒ£ãƒ³ã‚¯æ¯ã®å‡¦ç†æ™‚é–“ã€‘")
            print(f"â”Œ{'â”€'*7}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*12}â”")
            print(f"â”‚ãƒãƒ£ãƒ³ã‚¯â”‚ Rustè»¢é€ â”‚kvikioè»¢é€â”‚ GPUãƒ‘ãƒ¼ã‚¹â”‚ æ–‡å­—åˆ—å‡¦ç†â”‚ Parquet â”‚   å‡¦ç†è¡Œæ•°  â”‚")
            print(f"â”œ{'â”€'*7}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*12}â”¤")
            
            # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
            sorted_stats = sorted(chunk_stats, key=lambda x: x['chunk_id'])
            for stat in sorted_stats:
                print(f"â”‚  {stat['chunk_id']:^3}  â”‚ {stat['rust_time']:>6.2f}ç§’ â”‚ {stat['transfer_time']:>6.2f}ç§’ â”‚"
                      f" {stat['parse_time']:>6.2f}ç§’ â”‚ {stat['string_time']:>6.2f}ç§’ â”‚"
                      f" {stat['write_time']:>6.2f}ç§’ â”‚{stat['rows']:>10,}è¡Œâ”‚")
            
            print(f"â””{'â”€'*7}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*12}â”˜")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ€§èƒ½æ¸¬å®šå®Œäº†å¾Œã€ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã‚’å®Ÿè¡Œ
        sample_parquet = "output/chunk_0_queue.parquet"
        if os.path.exists(sample_parquet):
            # chunk_0ã®GPUå‡¦ç†è¡Œæ•°ã‚’å–å¾—
            gpu_rows_chunk0 = gpu_row_counts.get(0, None)
            validate_parquet_output(sample_parquet, num_rows=5, gpu_rows=gpu_rows_chunk0)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
        
        # å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®è¡Œæ•°ã‚’ç¢ºèª
        print("\nã€Parquetãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã€‘")
        actual_total_rows = 0
        parquet_files = sorted(Path("output").glob("chunk_*_queue.parquet"))
        for pf in parquet_files:
            try:
                table = pq.read_table(pf)
                actual_total_rows += table.num_rows
                print(f"â”œâ”€ {pf.name}: {table.num_rows:,} è¡Œ")
            except Exception as e:
                print(f"â”œâ”€ {pf.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
        print(f"â””â”€ å®Ÿéš›ã®ç·è¡Œæ•°: {actual_total_rows:,} è¡Œ")
        
        # --testãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã¨æ¯”è¼ƒ
        if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
            print("\nã€PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã¨ã®æ¯”è¼ƒã€‘")
            try:
                # PostgreSQLã®ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã‚’å–å¾—
                dsn = os.environ.get("GPUPASER_PG_DSN", "")
                with psycopg.connect(dsn) as conn:
                    with conn.cursor() as cur:
                        cur.execute(f"SELECT COUNT(*) FROM {table_name}")
                        pg_row_count = cur.fetchone()[0]
                        print(f"psql -c \"SELECT COUNT(*) FROM {table_name};\"ã®çµæœ: {pg_row_count:,} è¡Œ")
                        
                        # æ¯”è¼ƒçµæœã‚’è¡¨ç¤º
                        print(f"\nğŸ“Š Parquetå…¨ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: ")
                        if actual_total_rows == pg_row_count:
                            print(f"â””â”€ è¡Œæ•°: {actual_total_rows:,} OK (psqlã¨ä¸€è‡´)")
                        else:
                            print(f"â””â”€ è¡Œæ•°: {actual_total_rows:,} NG (psqlã¨ä¸ä¸€è‡´:{pg_row_count:,})")
                            
                        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚­ãƒ¼åˆ—ã‚’ç‰¹å®šï¼ˆé€šå¸¸ã¯æœ€åˆã®åˆ—ï¼‰
                        cur.execute(f"""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = '{table_name}' 
                            AND ordinal_position = 1
                        """)
                        key_column = cur.fetchone()
                        
                        if key_column:
                            key_column_name = key_column[0]
                            print(f"\nã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ¼åˆ—: {key_column_name}ï¼‰ã€‘")
                            
                            # å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
                            import cudf
                            all_dfs = []
                            duplicate_info = {}
                            
                            for i, pf in enumerate(parquet_files):
                                try:
                                    df = cudf.read_parquet(pf)
                                    if key_column_name in df.columns:
                                        # å„ãƒãƒ£ãƒ³ã‚¯å†…ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
                                        total_rows = len(df)
                                        unique_rows = df[key_column_name].nunique()
                                        duplicates_in_chunk = total_rows - unique_rows
                                        
                                        duplicate_info[pf.name] = {
                                            'total': total_rows,
                                            'unique': unique_rows,
                                            'duplicates': duplicates_in_chunk
                                        }
                                        
                                        if duplicates_in_chunk > 0:
                                            print(f"â”œâ”€ {pf.name}: {duplicates_in_chunk:,}å€‹ã®é‡è¤‡")
                                            # é‡è¤‡ã‚­ãƒ¼ã®ä¾‹ã‚’è¡¨ç¤º
                                            try:
                                                # Decimalåˆ—ã®å ´åˆã¯int64ã«å¤‰æ›
                                                if hasattr(df[key_column_name].dtype, 'precision'):
                                                    key_series = df[key_column_name].astype('int64')
                                                else:
                                                    key_series = df[key_column_name]
                                                key_counts = key_series.value_counts()
                                                dup_keys = key_counts[key_counts > 1].head(5)
                                            except Exception as e:
                                                print(f"â”‚   â””â”€ é‡è¤‡ã‚­ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                                                dup_keys = []
                                            
                                            # thread_idæƒ…å ±ãŒã‚ã‚Œã°è©³ç´°è¡¨ç¤º
                                            if '_thread_id' in df.columns and len(dup_keys) > 0:
                                                print(f"â”‚   â””â”€ ã‚¹ãƒ¬ãƒƒãƒ‰IDæƒ…å ±ä»˜ãé‡è¤‡åˆ†æ:")
                                                # dup_keysã‚’pandasã«å¤‰æ›ã—ã¦iterableã«ã™ã‚‹
                                                dup_keys_items = dup_keys.to_pandas().items()
                                                
                                                # æœ€åˆã®3ã¤ã®é‡è¤‡ã‚­ãƒ¼ã«ã¤ã„ã¦å…¨åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                                                sample_count = 0
                                                for key, count in dup_keys_items:
                                                    if sample_count >= 3:  # æœ€åˆã®3ã¤ã®ã¿
                                                        break
                                                    sample_count += 1
                                                    
                                                    # ã“ã®é‡è¤‡ã‚­ãƒ¼ã‚’æŒã¤è¡Œã®thread_idæƒ…å ±ã‚’å–å¾—
                                                    # Decimalåˆ—ã®å ´åˆã¯int64ã«å¤‰æ›
                                                    if hasattr(df[key_column_name].dtype, 'precision'):
                                                        dup_rows = df[df[key_column_name].astype('int64') == int(key)]
                                                    else:
                                                        dup_rows = df[df[key_column_name] == key]
                                                    thread_ids = dup_rows['_thread_id'].unique().to_pandas()
                                                    print(f"â”‚       â””â”€ {key_column_name}={key}: {count}å›å‡ºç¾")
                                                    print(f"â”‚           å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰: {sorted(thread_ids)}")
                                                    
                                                    # å…¨åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°åˆ—ã‚’é™¤ãï¼‰
                                                    print(f"â”‚           ")
                                                    print(f"â”‚           ã€å…¨åˆ—ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºã€‘")
                                                    # ãƒ‡ãƒãƒƒã‚°åˆ—ã‚’é™¤å¤–
                                                    debug_cols = ['_thread_id', '_row_position', '_thread_start_pos', '_thread_end_pos']
                                                    display_cols = [col for col in dup_rows.columns if col not in debug_cols]
                                                    
                                                    # å„è¡Œã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                                                    for row_idx in range(min(len(dup_rows), count)):  # å…¨ã¦ã®é‡è¤‡è¡Œã‚’è¡¨ç¤º
                                                        print(f"â”‚           ")
                                                        print(f"â”‚           è¡Œ{row_idx + 1}:")
                                                        
                                                        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º - ç°¡å˜ãªæ–¹æ³•ã§
                                                        for col in display_cols:
                                                            try:
                                                                # dup_rowsã‹ã‚‰ç›´æ¥å€¤ã‚’å–å¾—
                                                                value = dup_rows[col].iloc[row_idx]
                                                                
                                                                # æ–‡å­—åˆ—ã®å ´åˆã¯é•·ã•ã‚‚è¡¨ç¤º
                                                                if pd.api.types.is_string_dtype(dup_rows[col].dtype):
                                                                    # æœ«å°¾ã®ç©ºç™½ã‚’å¯è¦–åŒ–
                                                                    str_value = str(value)
                                                                    visible_value = str_value.replace(' ', 'Â·')
                                                                    print(f"â”‚             {col}: '{str_value}' (é•·ã•: {len(str_value)}, è¡¨ç¤º: '{visible_value}')")
                                                                else:
                                                                    print(f"â”‚             {col}: {value}")
                                                            except Exception as e:
                                                                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å˜ç´”ãªæ–‡å­—åˆ—è¡¨ç¾ã‚’è©¦ã™
                                                                try:
                                                                    simple_value = str(dup_rows[col].iloc[row_idx])
                                                                    print(f"â”‚             {col}: {simple_value}")
                                                                except:
                                                                    print(f"â”‚             {col}: (è¡¨ç¤ºã‚¨ãƒ©ãƒ¼)")
                                                        
                                                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
                                                        if '_thread_id' in dup_rows.columns:
                                                            print(f"â”‚             ---")
                                                            try:
                                                                print(f"â”‚             thread_id: {dup_rows['_thread_id'].iloc[row_idx]}")
                                                                if '_row_position' in dup_rows.columns:
                                                                    print(f"â”‚             row_position: {dup_rows['_row_position'].iloc[row_idx]:,}")
                                                                if '_thread_start_pos' in dup_rows.columns and '_thread_end_pos' in dup_rows.columns:
                                                                    start_pos = dup_rows['_thread_start_pos'].iloc[row_idx]
                                                                    end_pos = dup_rows['_thread_end_pos'].iloc[row_idx]
                                                                    print(f"â”‚             thread_range: [{start_pos:,} - {end_pos:,}]")
                                                            except:
                                                                pass
                                                    
                                                    print(f"â”‚           ")
                                                    
                                                    # å…ƒã®è©³ç´°è¡¨ç¤ºã‚‚ç¶­æŒï¼ˆç°¡ç•¥ç‰ˆï¼‰
                                                    for tid in sorted(thread_ids):
                                                        tid_rows = dup_rows[dup_rows['_thread_id'] == tid]
                                                        print(f"â”‚           â””â”€ thread_id={tid}: {len(tid_rows)}è¡Œ")
                                            else:
                                                for key, count in dup_keys.items():
                                                    print(f"â”‚   â””â”€ {key_column_name}={key}: {count}å›å‡ºç¾")
                                        
                                        all_dfs.append(df)
                                except Exception as e:
                                    print(f"â”œâ”€ {pf.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
                            
                            # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if all_dfs:
                                print(f"\nã€ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã€‘")
                                all_df = cudf.concat(all_dfs)
                                total_all = len(all_df)
                                unique_all = all_df[key_column_name].nunique()
                                total_duplicates = total_all - unique_all
                                
                                print(f"â”œâ”€ å…¨ãƒãƒ£ãƒ³ã‚¯åˆè¨ˆè¡Œæ•°: {total_all:,}")
                                print(f"â”œâ”€ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æ•°: {unique_all:,}")
                                print(f"â”œâ”€ ç·é‡è¤‡æ•°: {total_duplicates:,}")
                                
                                # ãƒãƒ£ãƒ³ã‚¯å†…é‡è¤‡ã®åˆè¨ˆ
                                chunk_duplicates = sum(info['duplicates'] for info in duplicate_info.values())
                                inter_chunk_duplicates = total_duplicates - chunk_duplicates
                                
                                if inter_chunk_duplicates > 0:
                                    print(f"â””â”€ ãƒãƒ£ãƒ³ã‚¯é–“é‡è¤‡: {inter_chunk_duplicates:,}å€‹")
                                    
                                    # ãƒãƒ£ãƒ³ã‚¯é–“ã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚­ãƒ¼ã®ç‰¹å®š
                                    key_appearances = {}
                                    for i, df in enumerate(all_dfs):
                                        for key in df[key_column_name].unique().to_pandas():
                                            if key not in key_appearances:
                                                key_appearances[key] = []
                                            key_appearances[key].append(i)
                                    
                                    # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã«å‡ºç¾ã™ã‚‹ã‚­ãƒ¼
                                    multi_chunk_keys = {k: v for k, v in key_appearances.items() if len(v) > 1}
                                    if multi_chunk_keys:
                                        print(f"\n    ãƒãƒ£ãƒ³ã‚¯é–“é‡è¤‡ã‚­ãƒ¼ã®ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:")
                                        
                                        # thread_idæƒ…å ±ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                        has_thread_id = any('_thread_id' in df.columns for df in all_dfs)
                                        
                                        for j, (key, chunks) in enumerate(list(multi_chunk_keys.items())[:5]):
                                            print(f"    â””â”€ {key_column_name}={key}: ãƒãƒ£ãƒ³ã‚¯{chunks}ã«å‡ºç¾")
                                            
                                            # thread_idæƒ…å ±ãŒã‚ã‚Œã°è©³ç´°è¡¨ç¤º
                                            if has_thread_id:
                                                for chunk_idx in chunks:
                                                    df = all_dfs[chunk_idx]
                                                    if '_thread_id' in df.columns:
                                                        # Decimalåˆ—ã®å ´åˆã¯int64ã«å¤‰æ›
                                                        if hasattr(df[key_column_name].dtype, 'precision'):
                                                            key_rows = df[df[key_column_name].astype('int64') == int(key)]
                                                        else:
                                                            key_rows = df[df[key_column_name] == key]
                                                        thread_ids = key_rows['_thread_id'].unique().to_pandas()
                                                        print(f"        â””â”€ ãƒãƒ£ãƒ³ã‚¯{chunk_idx}: thread_id={sorted(thread_ids)}")
                                
                                # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                                del all_df
                                del all_dfs
            except Exception as e:
                print(f"PostgreSQLè¡Œæ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        if actual_total_rows != results['total_rows']:
            print(f"\nâš ï¸  è¡Œæ•°ä¸ä¸€è‡´: GPUå ±å‘Šå€¤ {results['total_rows']:,} vs Parquetå®Ÿéš›å€¤ {actual_total_rows:,}")
        
        cleanup_files(total_chunks)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
    parser.add_argument('--table', type=str, default='lineorder', help='å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å')
    parser.add_argument('--parallel', type=int, default=16, help='ä¸¦åˆ—æ¥ç¶šæ•°')
    parser.add_argument('--chunks', type=int, default=8, help='ãƒãƒ£ãƒ³ã‚¯æ•°')
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆGPUç‰¹æ€§ãƒ»ã‚«ãƒ¼ãƒãƒ«æƒ…å ±è¡¨ç¤ºï¼‰')
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ['RUST_PARALLEL_CONNECTIONS'] = str(args.parallel)
    os.environ['TOTAL_CHUNKS'] = str(args.chunks)
    os.environ['TABLE_NAME'] = args.table  # Rustå´ã«ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä¼ãˆã‚‹
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    if args.test:
        os.environ['GPUPGPARSER_TEST_MODE'] = '1'
    
    main(total_chunks=args.chunks, table_name=args.table, test_mode=args.test)