"""
PostgreSQL â†’ Rust â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆ
Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã§çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
"""

import os
import time
import subprocess
import json
import numpy as np
from numba import cuda
import rmm
import cudf
import cupy as cp
import kvikio
from pathlib import Path
from typing import List, Dict, Any, Optional
import psycopg
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys

from src.types import ColumnMeta, PG_OID_TO_ARROW, UNKNOWN
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet_direct
from src.cuda_kernels.postgres_binary_parser import detect_pg_header_size
from src.readPostgres.metadata import fetch_column_meta
import pyarrow.parquet as pq

TABLE_NAME = "lineorder"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå®Ÿè¡Œæ™‚ã«ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
OUTPUT_DIR = "/dev/shm"
RUST_BINARY = "/home/ubuntu/gpupgparser/rust_bench_optimized/target/release/pg_fast_copy_single_chunk"
MAX_QUEUE_SIZE = 3  # ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚º

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
chunk_stats = []
gpu_row_counts = {}  # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯IDã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸ï¼‰
shutdown_flag = threading.Event()


def signal_handler(sig, frame):
    """Ctrl+Cãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    print("\n\nâš ï¸  å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    shutdown_flag.set()
    sys.exit(1)


signal.signal(signal.SIGINT, signal_handler)


def setup_rmm_pool():
    """RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’é©åˆ‡ã«è¨­å®š"""
    try:
        if rmm.is_initialized():
            return
        
        # GPUãƒ¡ãƒ¢ãƒªã®90%ã‚’ä½¿ç”¨å¯èƒ½ã«è¨­å®š
        gpu_memory = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem']
        pool_size = int(gpu_memory * 0.9)
        
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=pool_size,
            maximum_pool_size=pool_size
        )
    except Exception as e:
        print(f"âš ï¸ RMMåˆæœŸåŒ–è­¦å‘Š: {e}")


def get_postgresql_metadata():
    """PostgreSQLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    conn = psycopg.connect(dsn)
    try:
        print(f"PostgreSQLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ (ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME})...")
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(columns)} åˆ—")
        
        
        return columns
    finally:
        conn.close()


def cleanup_files(total_chunks=8):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    files = [
        f"{OUTPUT_DIR}/{TABLE_NAME}_meta_0.json",
        f"{OUTPUT_DIR}/{TABLE_NAME}_data_0.ready"
    ] + [f"{OUTPUT_DIR}/chunk_{i}.bin" for i in range(total_chunks)]
    
    for f in files:
        if os.path.exists(f):
            os.remove(f)


def rust_producer(chunk_queue: queue.Queue, total_chunks: int, stats_queue: queue.Queue):
    """Rustè»¢é€ã‚’å®Ÿè¡Œã™ã‚‹Producerã‚¹ãƒ¬ãƒƒãƒ‰"""
    for chunk_id in range(total_chunks):
        if shutdown_flag.is_set():
            break
            
        try:
            print(f"\n[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}/{total_chunks} Rustè»¢é€é–‹å§‹...")
            
            env = os.environ.copy()
            env['CHUNK_ID'] = str(chunk_id)
            env['TOTAL_CHUNKS'] = str(total_chunks)
            
            rust_start = time.time()
            process = subprocess.run(
                [RUST_BINARY],
                capture_output=True,
                text=True,
                env=env
            )
            
            if process.returncode != 0:
                print(f"âŒ Rustã‚¨ãƒ©ãƒ¼: {process.stderr}")
                continue
            
            # JSONçµæœã‚’æŠ½å‡º
            output = process.stdout
            json_start = output.find("===CHUNK_RESULT_JSON===")
            json_end = output.find("===END_CHUNK_RESULT_JSON===")
            
            if json_start != -1 and json_end != -1:
                json_str = output[json_start + len("===CHUNK_RESULT_JSON==="):json_end].strip()
                result = json.loads(json_str)
                rust_time = result['elapsed_seconds']
                file_size = result['total_bytes']
                chunk_file = result['chunk_file']
            else:
                rust_time = time.time() - rust_start
                chunk_file = f"{OUTPUT_DIR}/chunk_{chunk_id}.bin"
                file_size = os.path.getsize(chunk_file)
            
            chunk_info = {
                'chunk_id': chunk_id,
                'chunk_file': chunk_file,
                'file_size': file_size,
                'rust_time': rust_time
            }
            
            print(f"[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} è»¢é€å®Œäº† ({rust_time:.1f}ç§’, {file_size / 1024**3:.1f}GB)")
            
            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_queue.put(chunk_info)
            stats_queue.put(('rust_time', rust_time))
            
        except Exception as e:
            print(f"[Producer] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
    chunk_queue.put(None)
    print("[Producer] å…¨ãƒãƒ£ãƒ³ã‚¯è»¢é€å®Œäº†")


def gpu_consumer(chunk_queue: queue.Queue, columns: List[ColumnMeta], consumer_id: int, stats_queue: queue.Queue):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹Consumerã‚¹ãƒ¬ãƒƒãƒ‰"""
    while not shutdown_flag.is_set():
        try:
            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_info = chunk_queue.get(timeout=1)
            
            if chunk_info is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                break
                
            chunk_id = chunk_info['chunk_id']
            chunk_file = chunk_info['chunk_file']
            file_size = chunk_info['file_size']
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†é–‹å§‹...")
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            gpu_start = time.time()
            
            # kvikio+RMMã§ç›´æ¥GPUè»¢é€
            transfer_start = time.time()
            
            # RMM DeviceBufferã‚’ä½¿ç”¨
            gpu_buffer = rmm.DeviceBuffer(size=file_size)
            
            # kvikioã§ç›´æ¥èª­ã¿è¾¼ã¿
            with kvikio.CuFile(chunk_file, "rb") as f:
                gpu_array = cp.asarray(gpu_buffer).view(dtype=cp.uint8)
                bytes_read = f.read(gpu_array)
            
            if bytes_read != file_size:
                raise RuntimeError(f"èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºä¸ä¸€è‡´: {bytes_read} != {file_size}")
            
            # numba cudaé…åˆ—ã«å¤‰æ›ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
            raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
            
            transfer_time = time.time() - transfer_start
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
            header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
            header_size = detect_pg_header_size(header_sample)
            
            # ç›´æ¥æŠ½å‡ºå‡¦ç†
            chunk_output = f"output/chunk_{chunk_id}_queue.parquet"
            
            cudf_df, detailed_timing = postgresql_to_cudf_parquet_direct(
                raw_dev=raw_dev,
                columns=columns,
                ncols=len(columns),
                header_size=header_size,
                output_path=chunk_output,
                compression='snappy',
                use_rmm=True,
                optimize_gpu=True,
                verbose=False
            )
            
            gpu_time = time.time() - gpu_start
            
            # å‡¦ç†çµ±è¨ˆ
            rows = len(cudf_df) if cudf_df is not None else 0
            
            # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜
            gpu_row_counts[chunk_id] = rows
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†å®Œäº† ({gpu_time:.1f}ç§’, {rows:,}è¡Œ)")
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
            if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1':
                print(f"[CHUNK DEBUG] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}: ")
                print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size / 1024**2:.1f} MB")
                print(f"  - æ¤œå‡ºè¡Œæ•°: {rows:,}è¡Œ")
                print(f"  - GPUãƒ‘ãƒ¼ã‚¹æ™‚é–“: {detailed_timing.get('gpu_parsing', 0):.2f}ç§’")
                print(f"  - è¡Œã‚ãŸã‚Š: {file_size/rows if rows > 0 else 0:.1f} bytes/row")
            
            # çµ±è¨ˆæƒ…å ±ã‚’é€ä¿¡
            stats_queue.put(('gpu_time', gpu_time))
            stats_queue.put(('transfer_time', transfer_time))
            stats_queue.put(('rows', rows))
            stats_queue.put(('size', file_size))
            
            # è©³ç´°çµ±è¨ˆã‚’ä¿å­˜
            chunk_stats.append({
                'chunk_id': chunk_id,
                'consumer_id': consumer_id,
                'rust_time': chunk_info['rust_time'],
                'gpu_time': gpu_time,
                'transfer_time': transfer_time,
                'parse_time': detailed_timing.get('gpu_parsing', 0),
                'string_time': detailed_timing.get('string_buffer_creation', 0),
                'write_time': detailed_timing.get('parquet_export', 0),
                'rows': rows,
                'size_gb': file_size / 1024**3
            })
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del raw_dev
            del gpu_buffer
            del gpu_array
            if cudf_df is not None:
                del cudf_df
            
            mempool.free_all_blocks()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if os.path.exists(chunk_file):
                os.remove(chunk_file)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"[Consumer-{consumer_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[Consumer-{consumer_id}] çµ‚äº†")


def validate_parquet_output(file_path: str, num_rows: int = 5, gpu_rows: int = None) -> bool:
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        num_rows: è¡¨ç¤ºã™ã‚‹è¡Œæ•°
        gpu_rows: GPUå‡¦ç†ã§æ¤œå‡ºã—ãŸè¡Œæ•°ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    
    Returns:
        æ¤œè¨¼æˆåŠŸã®å ´åˆTrue
    """
    try:
        # PyArrowã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        table = pq.read_table(file_path)
        
        print(f"\nğŸ“Š Parquetãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {os.path.basename(file_path)}")
        print(f"â”œâ”€ è¡Œæ•°: {table.num_rows:,}", end="")
        if gpu_rows is not None:
            if table.num_rows == gpu_rows:
                print(f" âœ… OK (GPUå‡¦ç†è¡Œæ•°ã¨ä¸€è‡´)")
            else:
                print(f" âŒ NG (GPUå‡¦ç†è¡Œæ•°: {gpu_rows:,})")
        else:
            print()
        print(f"â”œâ”€ åˆ—æ•°: {table.num_columns}")
        print(f"â””â”€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(file_path) / 1024**2:.2f} MB")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­{num_rows}è¡Œï¼‰:")
        print("â”€" * 80)
        df_sample = table.slice(0, num_rows).to_pandas()
        print(df_sample.to_string(index=False, max_colwidth=20))
        print("â”€" * 80)
        
        return True
        
    except Exception as e:
        print(f"âŒ Parquetæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_parallel_pipeline(columns: List[ColumnMeta], total_chunks: int):
    """çœŸã®ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ã‚­ãƒ¥ãƒ¼ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†
    chunk_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
    stats_queue = queue.Queue()
    
    start_time = time.time()
    
    # Producerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
    producer_thread = threading.Thread(
        target=rust_producer,
        args=(chunk_queue, total_chunks, stats_queue)
    )
    producer_thread.start()
    
    # Consumerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆ1ã¤ã®ã¿ - GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼‰
    consumer_thread = threading.Thread(
        target=gpu_consumer,
        args=(chunk_queue, columns, 1, stats_queue)
    )
    consumer_thread.start()
    
    # çµ±è¨ˆåé›†
    total_rust_time = 0
    total_gpu_time = 0
    total_transfer_time = 0
    total_rows = 0
    total_size = 0
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿã—ãªãŒã‚‰çµ±è¨ˆã‚’åé›†
    producer_thread.join()
    consumer_thread.join()
    
    # çµ±è¨ˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœã‚’åé›†
    while not stats_queue.empty():
        stat_type, value = stats_queue.get()
        if stat_type == 'rust_time':
            total_rust_time += value
        elif stat_type == 'gpu_time':
            total_gpu_time += value
        elif stat_type == 'transfer_time':
            total_transfer_time += value
        elif stat_type == 'rows':
            total_rows += value
        elif stat_type == 'size':
            total_size += value
    
    total_time = time.time() - start_time
    
    return {
        'total_time': total_time,
        'total_rust_time': total_rust_time,
        'total_gpu_time': total_gpu_time,
        'total_transfer_time': total_transfer_time,
        'total_rows': total_rows,
        'total_size': total_size,
        'processed_chunks': len(chunk_stats)
    }


def main(total_chunks=8, table_name=None):
    global TABLE_NAME
    if table_name:
        TABLE_NAME = table_name
    # kvikioè¨­å®šç¢ºèª
    is_compat = os.environ.get("KVIKIO_COMPAT_MODE", "").lower() in ["on", "1", "true"]
    
    # RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š
    setup_rmm_pool()
    
    # CUDA contextç¢ºèª
    try:
        cuda.current_context()
    except Exception as e:
        print(f"âŒ CUDA context ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_files(total_chunks)
    
    # PostgreSQLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    columns = get_postgresql_metadata()
    
    try:
        # ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        print("\nä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        print("=" * 80)
        
        results = run_parallel_pipeline(columns, total_chunks)
        
        # æœ€çµ‚çµ±è¨ˆã‚’æ§‹é€ åŒ–è¡¨ç¤º
        total_gb = results['total_size'] / 1024**3
        
        print(f"\n{'='*80}")
        print(f" âœ… å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")
        
        # å…¨ä½“çµ±è¨ˆ
        print(f"\nã€å…¨ä½“çµ±è¨ˆã€‘")
        print(f"â”œâ”€ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_gb:.2f} GB")
        print(f"â”œâ”€ ç·è¡Œæ•°: {results['total_rows']:,} è¡Œ")
        print(f"â”œâ”€ ç·å®Ÿè¡Œæ™‚é–“: {results['total_time']:.2f}ç§’")
        print(f"â””â”€ å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_gb / results['total_time']:.2f} GB/ç§’")
        
        # å‡¦ç†æ™‚é–“å†…è¨³
        print(f"\nã€å‡¦ç†æ™‚é–“å†…è¨³ã€‘")
        print(f"â”œâ”€ Rustè»¢é€åˆè¨ˆ: {results['total_rust_time']:.2f}ç§’ ({total_gb / results['total_rust_time']:.2f} GB/ç§’)")
        print(f"â””â”€ GPUå‡¦ç†åˆè¨ˆ: {results['total_gpu_time']:.2f}ç§’ ({total_gb / results['total_gpu_time']:.2f} GB/ç§’)")
        print(f"   â””â”€ kvikioè»¢é€: {results['total_transfer_time']:.2f}ç§’")
        
        
        # ãƒãƒ£ãƒ³ã‚¯æ¯ã®è©³ç´°çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        if chunk_stats:
            print(f"\nã€ãƒãƒ£ãƒ³ã‚¯æ¯ã®å‡¦ç†æ™‚é–“ã€‘")
            print(f"â”Œ{'â”€'*7}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*12}â”")
            print(f"â”‚ãƒãƒ£ãƒ³ã‚¯â”‚ Rustè»¢é€ â”‚kvikioè»¢é€â”‚ GPUãƒ‘ãƒ¼ã‚¹â”‚ æ–‡å­—åˆ—å‡¦ç†â”‚ Parquet â”‚   å‡¦ç†è¡Œæ•°  â”‚")
            print(f"â”œ{'â”€'*7}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*12}â”¤")
            
            # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
            sorted_stats = sorted(chunk_stats, key=lambda x: x['chunk_id'])
            for stat in sorted_stats:
                print(f"â”‚  {stat['chunk_id']:^3}  â”‚ {stat['rust_time']:>6.2f}ç§’ â”‚ {stat['transfer_time']:>6.2f}ç§’ â”‚"
                      f" {stat['parse_time']:>6.2f}ç§’ â”‚ {stat['string_time']:>6.2f}ç§’ â”‚"
                      f" {stat['write_time']:>6.2f}ç§’ â”‚{stat['rows']:>10,}è¡Œâ”‚")
            
            print(f"â””{'â”€'*7}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*12}â”˜")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ€§èƒ½æ¸¬å®šå®Œäº†å¾Œã€ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã‚’å®Ÿè¡Œ
        sample_parquet = "output/chunk_0_queue.parquet"
        if os.path.exists(sample_parquet):
            # chunk_0ã®GPUå‡¦ç†è¡Œæ•°ã‚’å–å¾—
            gpu_rows_chunk0 = gpu_row_counts.get(0, None)
            validate_parquet_output(sample_parquet, num_rows=5, gpu_rows=gpu_rows_chunk0)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
        
        cleanup_files(total_chunks)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
    parser.add_argument('--table', type=str, default='lineorder', help='å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å')
    parser.add_argument('--parallel', type=int, default=16, help='ä¸¦åˆ—æ¥ç¶šæ•°')
    parser.add_argument('--chunks', type=int, default=8, help='ãƒãƒ£ãƒ³ã‚¯æ•°')
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ['RUST_PARALLEL_CONNECTIONS'] = str(args.parallel)
    os.environ['TOTAL_CHUNKS'] = str(args.chunks)
    os.environ['TABLE_NAME'] = args.table  # Rustå´ã«ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä¼ãˆã‚‹
    
    main(total_chunks=args.chunks, table_name=args.table)