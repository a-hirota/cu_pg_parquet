"""
PostgreSQL â†’ Rust â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆ
Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã§çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
"""

import os
import time
import subprocess
import json
import numpy as np
from numba import cuda
import rmm
import cudf
import cupy as cp
import kvikio
from pathlib import Path
from typing import List, Dict, Any, Optional
import psycopg
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys

from src.types import ColumnMeta, PG_OID_TO_ARROW, UNKNOWN
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet_direct
from src.cuda_kernels.postgres_binary_parser import detect_pg_header_size, estimate_row_size_from_columns
from src.readPostgres.metadata import fetch_column_meta
from src.utils.gpu_memory_manager import GPUMemoryManager, cleanup_gpu_memory
import pyarrow.parquet as pq

TABLE_NAME = "lineorder"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå®Ÿè¡Œæ™‚ã«ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
OUTPUT_DIR = "/dev/shm"
RUST_BINARY = "/home/ubuntu/gpupgparser/rust_bench_optimized/target/release/pg_fast_copy_single_chunk"
MAX_QUEUE_SIZE = 2  # ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚å‰Šæ¸›ï¼‰

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
chunk_stats = []
gpu_row_counts = {}  # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯IDã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸ï¼‰
shutdown_flag = threading.Event()


def signal_handler(sig, frame):
    """Ctrl+Cãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    print("\n\nâš ï¸  å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    shutdown_flag.set()
    sys.exit(1)


signal.signal(signal.SIGINT, signal_handler)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
gpu_memory_manager = None

def setup_memory_management(strategy='arena'):
    """æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªç®¡ç†ã®è¨­å®š"""
    global gpu_memory_manager
    try:
        gpu_memory_manager = GPUMemoryManager(strategy=strategy)
        gpu_memory_manager.setup_memory_resource()
        gpu_memory_manager.log_memory_status("åˆæœŸåŒ–å¾Œ")
    except Exception as e:
        print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–è­¦å‘Š: {e}, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        setup_rmm_pool_fallback()

def setup_rmm_pool_fallback():
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®RMMãƒ—ãƒ¼ãƒ«è¨­å®š"""
    try:
        if rmm.is_initialized():
            return
        gpu_memory = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem']
        pool_size = int(gpu_memory * 0.7)  # 70%ã«å‰Šæ¸›
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=pool_size,
            maximum_pool_size=int(gpu_memory * 0.85)  # æœ€å¤§85%
        )
    except Exception as e:
        print(f"âŒ RMMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")


def get_postgresql_metadata():
    """PostgreSQLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    conn = psycopg.connect(dsn)
    try:
        print(f"PostgreSQLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ (ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME})...")
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(columns)} åˆ—")
        
        
        return columns
    finally:
        conn.close()


def cleanup_files(total_chunks=8):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    files = [
        f"{OUTPUT_DIR}/{TABLE_NAME}_meta_0.json",
        f"{OUTPUT_DIR}/{TABLE_NAME}_data_0.ready"
    ] + [f"{OUTPUT_DIR}/chunk_{i}.bin" for i in range(total_chunks)]
    
    for f in files:
        if os.path.exists(f):
            os.remove(f)


def rust_producer(chunk_queue: queue.Queue, total_chunks: int, stats_queue: queue.Queue):
    """Rustè»¢é€ã‚’å®Ÿè¡Œã™ã‚‹Producerã‚¹ãƒ¬ãƒƒãƒ‰"""
    for chunk_id in range(total_chunks):
        if shutdown_flag.is_set():
            break
            
        try:
            print(f"\n[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}/{total_chunks} Rustè»¢é€é–‹å§‹...")
            
            env = os.environ.copy()
            env['CHUNK_ID'] = str(chunk_id)
            env['TOTAL_CHUNKS'] = str(total_chunks)
            
            rust_start = time.time()
            process = subprocess.run(
                [RUST_BINARY],
                capture_output=True,
                text=True,
                env=env
            )
            
            if process.returncode != 0:
                print(f"âŒ Rustã‚¨ãƒ©ãƒ¼: {process.stderr}")
                continue
            
            # JSONçµæœã‚’æŠ½å‡º
            output = process.stdout
            json_start = output.find("===CHUNK_RESULT_JSON===")
            json_end = output.find("===END_CHUNK_RESULT_JSON===")
            
            if json_start != -1 and json_end != -1:
                json_str = output[json_start + len("===CHUNK_RESULT_JSON==="):json_end].strip()
                result = json.loads(json_str)
                rust_time = result['elapsed_seconds']
                file_size = result['total_bytes']
                chunk_file = result['chunk_file']
            else:
                rust_time = time.time() - rust_start
                chunk_file = f"{OUTPUT_DIR}/chunk_{chunk_id}.bin"
                file_size = os.path.getsize(chunk_file)
            
            chunk_info = {
                'chunk_id': chunk_id,
                'chunk_file': chunk_file,
                'file_size': file_size,
                'rust_time': rust_time
            }
            
            print(f"[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} è»¢é€å®Œäº† ({rust_time:.1f}ç§’, {file_size / 1024**3:.1f}GB)")
            
            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_queue.put(chunk_info)
            stats_queue.put(('rust_time', rust_time))
            
        except Exception as e:
            print(f"[Producer] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
    chunk_queue.put(None)
    print("[Producer] å…¨ãƒãƒ£ãƒ³ã‚¯è»¢é€å®Œäº†")


def gpu_consumer(chunk_queue: queue.Queue, columns: List[ColumnMeta], consumer_id: int, stats_queue: queue.Queue, test_mode: bool = False):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹Consumerã‚¹ãƒ¬ãƒƒãƒ‰"""
    # æ¨å®šè¡Œã‚µã‚¤ã‚ºã‚’äº‹å‰è¨ˆç®—
    estimated_row_size = estimate_row_size_from_columns(columns)
    
    while not shutdown_flag.is_set():
        try:
            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_info = chunk_queue.get(timeout=1)
            
            if chunk_info is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                break
                
            chunk_id = chunk_info['chunk_id']
            chunk_file = chunk_info['chunk_file']
            file_size = chunk_info['file_size']
            
            # ãƒ‡ãƒãƒƒã‚°ï¼šæ¨å®šè¡Œæ•°ã‚’è¨ˆç®—
            estimated_rows = int(file_size / estimated_row_size)
            print(f"\n[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†é–‹å§‹...")
            if test_mode:
                print(f"  æ¨å®šè¡Œæ•°: {estimated_rows:,} (ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º {file_size:,} Ã· æ¨å®šè¡Œã‚µã‚¤ã‚º {estimated_row_size})")
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            gpu_start = time.time()
            
            # kvikio+RMMã§ç›´æ¥GPUè»¢é€
            transfer_start = time.time()
            
            # RMM DeviceBufferã‚’ä½¿ç”¨
            gpu_buffer = rmm.DeviceBuffer(size=file_size)
            
            # kvikioã§ç›´æ¥èª­ã¿è¾¼ã¿
            with kvikio.CuFile(chunk_file, "rb") as f:
                gpu_array = cp.asarray(gpu_buffer).view(dtype=cp.uint8)
                bytes_read = f.read(gpu_array)
            
            if bytes_read != file_size:
                raise RuntimeError(f"èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºä¸ä¸€è‡´: {bytes_read} != {file_size}")
            
            # numba cudaé…åˆ—ã«å¤‰æ›ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
            raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
            
            transfer_time = time.time() - transfer_start
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
            header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
            header_size = detect_pg_header_size(header_sample)
            
            # ç›´æ¥æŠ½å‡ºå‡¦ç†
            chunk_output = f"output/chunk_{chunk_id}_queue.parquet"
            
            # test_modeã§ã¯æˆ»ã‚Šå€¤ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
            result = postgresql_to_cudf_parquet_direct(
                raw_dev=raw_dev,
                columns=columns,
                ncols=len(columns),
                header_size=header_size,
                output_path=chunk_output,
                compression='snappy',
                use_rmm=True,
                optimize_gpu=True,
                verbose=False,
                test_mode=test_mode
            )
            
            # test_modeã®å ´åˆã€ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚‚è¿”ã•ã‚Œã‚‹
            if test_mode:
                if isinstance(result, tuple) and len(result) > 2:
                    cudf_df, detailed_timing, debug_info = result[:3]
                    
                    # æœ€å¾Œã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è§£æ
                    if debug_info is not None and os.path.exists('/home/ubuntu/gpupgparser/analyze_last_threads.py'):
                        print(f"\n[TEST MODE] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} - æœ€å¾Œã®10ã‚¹ãƒ¬ãƒƒãƒ‰ã®å‡¦ç†çŠ¶æ³:")
                        import sys
                        sys.path.append('/home/ubuntu/gpupgparser')
                        from analyze_last_threads import analyze_last_threads_debug
                        analyze_last_threads_debug(debug_info)
                else:
                    cudf_df, detailed_timing = result
            else:
                cudf_df, detailed_timing = result
            
            gpu_time = time.time() - gpu_start
            
            # å‡¦ç†çµ±è¨ˆ
            rows = len(cudf_df) if cudf_df is not None else 0
            
            # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜
            gpu_row_counts[chunk_id] = rows
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†å®Œäº† ({gpu_time:.1f}ç§’, {rows:,}è¡Œ)")
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
            if test_mode or os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1':
                discrepancy = abs(estimated_rows - rows) / estimated_rows * 100 if estimated_rows > 0 else 0
                print(f"\n[CHUNK ANALYSIS] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}:")
                print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024**3:.2f} GB)")
                print(f"  - æ¨å®šè¡Œã‚µã‚¤ã‚º: {estimated_row_size} bytes/è¡Œ")
                print(f"  - æ¨å®šè¡Œæ•°: {estimated_rows:,}")
                print(f"  - GPUæ¤œå‡ºè¡Œæ•°: {rows:,}")
                print(f"  - å·®ç•°: {estimated_rows - rows:,} è¡Œ ({discrepancy:.2f}%)")
                print(f"  - å®Ÿéš›ã®å¹³å‡è¡Œã‚µã‚¤ã‚º: {file_size/rows if rows > 0 else 0:.1f} bytes/è¡Œ")
                print(f"  - GPUãƒ‘ãƒ¼ã‚¹æ™‚é–“: {detailed_timing.get('gpu_parsing', 0):.2f}ç§’")
                
                # å¤§ããªå·®ç•°ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
                if discrepancy > 10:
                    print(f"  âš ï¸  è­¦å‘Š: 10%ä»¥ä¸Šã®è¡Œæ•°å·®ç•°ã‚’æ¤œå‡ºï¼")
            
            # çµ±è¨ˆæƒ…å ±ã‚’é€ä¿¡
            stats_queue.put(('gpu_time', gpu_time))
            stats_queue.put(('transfer_time', transfer_time))
            stats_queue.put(('rows', rows))
            stats_queue.put(('size', file_size))
            
            # è©³ç´°çµ±è¨ˆã‚’ä¿å­˜
            chunk_stats.append({
                'chunk_id': chunk_id,
                'consumer_id': consumer_id,
                'rust_time': chunk_info['rust_time'],
                'gpu_time': gpu_time,
                'transfer_time': transfer_time,
                'parse_time': detailed_timing.get('gpu_parsing', 0),
                'string_time': detailed_timing.get('string_buffer_creation', 0),
                'write_time': detailed_timing.get('parquet_export', 0),
                'rows': rows,
                'size_gb': file_size / 1024**3
            })
            
            # ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªè§£æ”¾
            # å‚ç…§ã‚’æ˜ç¤ºçš„ã«Noneã«è¨­å®š
            if 'raw_dev' in locals():
                raw_dev = None
            if 'gpu_buffer' in locals():
                gpu_buffer = None
            if 'gpu_array' in locals():
                gpu_array = None
            if 'cudf_df' in locals() and cudf_df is not None:
                cudf_df = None
            
            # GPUãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ç©æ¥µçš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if gpu_memory_manager:
                gpu_memory_manager.aggressive_cleanup()
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                mempool.free_all_blocks()
                pinned_mempool = cp.get_default_pinned_memory_pool()
                pinned_mempool.free_all_blocks()
                import gc
                gc.collect()
            
            # CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åŒæœŸ
            cp.cuda.Stream.null.synchronize()
            
            # ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’ãƒ­ã‚°ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰
            if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1' and gpu_memory_manager:
                gpu_memory_manager.log_memory_status(f"ãƒãƒ£ãƒ³ã‚¯{chunk_id + 1}å‡¦ç†å¾Œ")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if os.path.exists(chunk_file):
                os.remove(chunk_file)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"[Consumer-{consumer_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[Consumer-{consumer_id}] çµ‚äº†")


def print_detailed_chunk_statistics(columns):
    """å„ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º"""
    print("\n=== ãƒãƒ£ãƒ³ã‚¯åˆ¥è©³ç´°çµ±è¨ˆ ===")
    
    # æ¨å®šè¡Œã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆæ—¢å­˜ã®é–¢æ•°ã‚’ä½¿ç”¨ï¼‰
    estimated_row_size = estimate_row_size_from_columns(columns)
    print(f"æ¨å®šè¡Œã‚µã‚¤ã‚º: {estimated_row_size} ãƒã‚¤ãƒˆ/è¡Œï¼ˆestimate_row_size_from_columnsä½¿ç”¨ï¼‰")
    
    print(f"\n{'Chunk':<6} {'Size(GB)':<10} {'Est.Rows':<15} {'GPU Rows':<15} {'Diff':<15} {'Diff%':<8} {'Bytes/Row':<12} {'Status':<10}")
    print("-" * 110)
    
    total_est_rows = 0
    total_gpu_rows = 0
    total_size = 0
    
    # chunk_statsã‚’ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
    sorted_stats = sorted(chunk_stats, key=lambda x: x['chunk_id'])
    
    for stats in sorted_stats:
        chunk_id = stats['chunk_id']
        size_gb = stats['size_gb']
        size_bytes = size_gb * 1024**3
        gpu_rows = stats['rows']
        
        # æ¨å®šè¡Œæ•°ã‚’è¨ˆç®—
        est_rows = int(size_bytes / estimated_row_size)
        diff = est_rows - gpu_rows
        diff_pct = (diff / est_rows * 100) if est_rows > 0 else 0
        
        # å®Ÿéš›ã®ãƒã‚¤ãƒˆ/è¡Œ
        actual_bytes_per_row = size_bytes / gpu_rows if gpu_rows > 0 else 0
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        status = "OK" if abs(diff_pct) < 5 else "WARNING" if abs(diff_pct) < 10 else "ERROR"
        
        print(f"{chunk_id:<6} {size_gb:<10.2f} {est_rows:<15,} {gpu_rows:<15,} {diff:<15,} {diff_pct:<8.2f}% {actual_bytes_per_row:<12.1f} {status:<10}")
        
        total_est_rows += est_rows
        total_gpu_rows += gpu_rows
        total_size += size_bytes
    
    print("-" * 110)
    total_diff = total_est_rows - total_gpu_rows
    total_diff_pct = (total_diff / total_est_rows * 100) if total_est_rows > 0 else 0
    avg_bytes_per_row = total_size / total_gpu_rows if total_gpu_rows > 0 else 0
    
    print(f"{'Total':<6} {total_size/1024**3:<10.2f} {total_est_rows:<15,} {total_gpu_rows:<15,} {total_diff:<15,} {total_diff_pct:<8.2f}% {avg_bytes_per_row:<12.1f}")
    
    # ç•°å¸¸æ¤œå‡º
    print(f"\n=== ç•°å¸¸æ¤œå‡ºã‚µãƒãƒªãƒ¼ ===")
    error_chunks = [s for s in sorted_stats if abs((int(s['size_gb']*1024**3/estimated_row_size) - s['rows']) / int(s['size_gb']*1024**3/estimated_row_size) * 100) > 10]
    if error_chunks:
        print(f"âš ï¸  {len(error_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã§10%ä»¥ä¸Šã®è¡Œæ•°å·®ç•°ã‚’æ¤œå‡º:")
        for chunk in error_chunks:
            chunk_id = chunk['chunk_id']
            est = int(chunk['size_gb']*1024**3/estimated_row_size)
            act = chunk['rows']
            print(f"   - ãƒãƒ£ãƒ³ã‚¯{chunk_id}: æ¨å®š{est:,}è¡Œ â†’ å®Ÿéš›{act:,}è¡Œ (å·®ç•°: {(est-act)/est*100:.1f}%)")
    else:
        print("âœ… å…¨ãƒãƒ£ãƒ³ã‚¯ãŒæ­£å¸¸ç¯„å›²å†…ã§ã™")
    
    # ãƒãƒ£ãƒ³ã‚¯é–“ã®åˆ†æ
    print("\n=== ãƒãƒ£ãƒ³ã‚¯é–“åˆ†æ ===")
    if chunk_stats:
        # è¡Œæ•°ã®æ¨™æº–åå·®ã‚’è¨ˆç®—
        row_counts = [s['rows'] for s in chunk_stats]
        avg_rows = np.mean(row_counts)
        std_rows = np.std(row_counts)
        cv = std_rows / avg_rows * 100 if avg_rows > 0 else 0
        
        print(f"å¹³å‡è¡Œæ•°: {avg_rows:,.0f}")
        print(f"æ¨™æº–åå·®: {std_rows:,.0f}")
        print(f"å¤‰å‹•ä¿‚æ•°: {cv:.2f}%")
        
        if cv > 10:
            print("âš ï¸  è­¦å‘Š: ãƒãƒ£ãƒ³ã‚¯é–“ã§è¡Œæ•°ã®ã°ã‚‰ã¤ããŒå¤§ãã„")


def validate_parquet_output(file_path: str, num_rows: int = 5, gpu_rows: int = None) -> bool:
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        num_rows: è¡¨ç¤ºã™ã‚‹è¡Œæ•°
        gpu_rows: GPUå‡¦ç†ã§æ¤œå‡ºã—ãŸè¡Œæ•°ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    
    Returns:
        æ¤œè¨¼æˆåŠŸã®å ´åˆTrue
    """
    try:
        # PyArrowã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        table = pq.read_table(file_path)
        
        print(f"\nğŸ“Š Parquetãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {os.path.basename(file_path)}")
        print(f"â”œâ”€ è¡Œæ•°: {table.num_rows:,}", end="")
        if gpu_rows is not None:
            if table.num_rows == gpu_rows:
                print(f" âœ… OK (GPUå‡¦ç†è¡Œæ•°ã¨ä¸€è‡´)")
            else:
                print(f" âŒ NG (GPUå‡¦ç†è¡Œæ•°: {gpu_rows:,})")
        else:
            print()
        print(f"â”œâ”€ åˆ—æ•°: {table.num_columns}")
        print(f"â””â”€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(file_path) / 1024**2:.2f} MB")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­{num_rows}è¡Œï¼‰:")
        print("â”€" * 80)
        df_sample = table.slice(0, num_rows).to_pandas()
        print(df_sample.to_string(index=False, max_colwidth=20))
        print("â”€" * 80)
        
        return True
        
    except Exception as e:
        print(f"âŒ Parquetæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_parallel_pipeline(columns: List[ColumnMeta], total_chunks: int, test_mode: bool = False):
    """çœŸã®ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ã‚­ãƒ¥ãƒ¼ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†
    chunk_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
    stats_queue = queue.Queue()
    
    start_time = time.time()
    
    # Producerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
    producer_thread = threading.Thread(
        target=rust_producer,
        args=(chunk_queue, total_chunks, stats_queue)
    )
    producer_thread.start()
    
    # Consumerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆ1ã¤ã®ã¿ - GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼‰
    consumer_thread = threading.Thread(
        target=gpu_consumer,
        args=(chunk_queue, columns, 1, stats_queue, test_mode)
    )
    consumer_thread.start()
    
    # çµ±è¨ˆåé›†
    total_rust_time = 0
    total_gpu_time = 0
    total_transfer_time = 0
    total_rows = 0
    total_size = 0
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿã—ãªãŒã‚‰çµ±è¨ˆã‚’åé›†
    producer_thread.join()
    consumer_thread.join()
    
    # çµ±è¨ˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœã‚’åé›†
    while not stats_queue.empty():
        stat_type, value = stats_queue.get()
        if stat_type == 'rust_time':
            total_rust_time += value
        elif stat_type == 'gpu_time':
            total_gpu_time += value
        elif stat_type == 'transfer_time':
            total_transfer_time += value
        elif stat_type == 'rows':
            total_rows += value
        elif stat_type == 'size':
            total_size += value
    
    total_time = time.time() - start_time
    
    return {
        'total_time': total_time,
        'total_rust_time': total_rust_time,
        'total_gpu_time': total_gpu_time,
        'total_transfer_time': total_transfer_time,
        'total_rows': total_rows,
        'total_size': total_size,
        'processed_chunks': len(chunk_stats)
    }


def main(total_chunks=None, table_name=None, test_mode=False):
    global TABLE_NAME
    if table_name:
        TABLE_NAME = table_name
    # kvikioè¨­å®šç¢ºèª
    is_compat = os.environ.get("KVIKIO_COMPAT_MODE", "").lower() in ["on", "1", "true"]
    
    # æ”¹å–„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªç®¡ç†è¨­å®š
    # async: CUDA 11.2ä»¥ä¸Šã§ãƒ¡ãƒ¢ãƒªã‚’ã‚·ã‚¹ãƒ†ãƒ ã«è¿”å´å¯èƒ½ï¼ˆã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰GPUå‡¦ç†ã«æœ€é©ï¼‰
    # arena: ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–
    memory_strategy = os.environ.get('GPUPGPARSER_MEMORY_STRATEGY', 'async')
    setup_memory_management(strategy=memory_strategy)
    
    # CUDA contextç¢ºèª
    try:
        cuda.current_context()
    except Exception as e:
        print(f"âŒ CUDA context ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆè‡ªå‹•è¨ˆç®—å‰ãªã®ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    cleanup_files(32)  # æœ€å¤§32ãƒãƒ£ãƒ³ã‚¯ã¾ã§å¯¾å¿œ
    
    # PostgreSQLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    columns = get_postgresql_metadata()
    
    # ãƒãƒ£ãƒ³ã‚¯æ•°ã®è‡ªå‹•è¨ˆç®—ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    if total_chunks is None:
        # sys.pathã‚’ä¸€æ™‚çš„ã«èª¿æ•´ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent.parent.parent))
        from src.utils.chunk_calculator import get_chunk_recommendation
        
        total_chunks = get_chunk_recommendation(TABLE_NAME, columns)
        print(f"\nâœ… ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’è‡ªå‹•è¨ˆç®—ã—ã¾ã—ãŸ: {total_chunks}ãƒãƒ£ãƒ³ã‚¯")
    
    try:
        # ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        print("\nä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        print("=" * 80)
        
        results = run_parallel_pipeline(columns, total_chunks, test_mode)
        
        # æœ€çµ‚çµ±è¨ˆã‚’æ§‹é€ åŒ–è¡¨ç¤º
        total_gb = results['total_size'] / 1024**3
        
        print(f"\n{'='*80}")
        print(f" âœ… å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")
        
        # å…¨ä½“çµ±è¨ˆ
        print(f"\nã€å…¨ä½“çµ±è¨ˆã€‘")
        print(f"â”œâ”€ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_gb:.2f} GB")
        print(f"â”œâ”€ ç·è¡Œæ•°: {results['total_rows']:,} è¡Œ")
        print(f"â”œâ”€ ç·å®Ÿè¡Œæ™‚é–“: {results['total_time']:.2f}ç§’")
        print(f"â””â”€ å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_gb / results['total_time']:.2f} GB/ç§’")
        
        # å‡¦ç†æ™‚é–“å†…è¨³
        print(f"\nã€å‡¦ç†æ™‚é–“å†…è¨³ã€‘")
        print(f"â”œâ”€ Rustè»¢é€åˆè¨ˆ: {results['total_rust_time']:.2f}ç§’ ({total_gb / results['total_rust_time']:.2f} GB/ç§’)")
        print(f"â””â”€ GPUå‡¦ç†åˆè¨ˆ: {results['total_gpu_time']:.2f}ç§’ ({total_gb / results['total_gpu_time']:.2f} GB/ç§’)")
        print(f"   â””â”€ kvikioè»¢é€: {results['total_transfer_time']:.2f}ç§’")
        
        
        # ãƒãƒ£ãƒ³ã‚¯æ¯ã®è©³ç´°çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        if chunk_stats:
            print(f"\nã€ãƒãƒ£ãƒ³ã‚¯æ¯ã®å‡¦ç†æ™‚é–“ã€‘")
            print(f"â”Œ{'â”€'*7}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*12}â”")
            print(f"â”‚ãƒãƒ£ãƒ³ã‚¯â”‚ Rustè»¢é€ â”‚kvikioè»¢é€â”‚ GPUãƒ‘ãƒ¼ã‚¹â”‚ æ–‡å­—åˆ—å‡¦ç†â”‚ Parquet â”‚   å‡¦ç†è¡Œæ•°  â”‚")
            print(f"â”œ{'â”€'*7}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*12}â”¤")
            
            # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
            sorted_stats = sorted(chunk_stats, key=lambda x: x['chunk_id'])
            for stat in sorted_stats:
                print(f"â”‚  {stat['chunk_id']:^3}  â”‚ {stat['rust_time']:>6.2f}ç§’ â”‚ {stat['transfer_time']:>6.2f}ç§’ â”‚"
                      f" {stat['parse_time']:>6.2f}ç§’ â”‚ {stat['string_time']:>6.2f}ç§’ â”‚"
                      f" {stat['write_time']:>6.2f}ç§’ â”‚{stat['rows']:>10,}è¡Œâ”‚")
            
            print(f"â””{'â”€'*7}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*12}â”˜")
        
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è¡¨ç¤º
        if gpu_memory_manager:
            gpu_memory_manager.log_memory_status("å…¨å‡¦ç†å®Œäº†å¾Œ")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º
        if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1' and 'chunk_stats' in locals():
            print("\nã€ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰è©³ç´°çµ±è¨ˆã€‘")
            total_chunks = len(chunk_stats)
            if total_chunks > 0:
                # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®è¡Œæ•°ã‚’é›†è¨ˆ
                chunk_rows = {}
                for stat in chunk_stats:
                    chunk_id = stat['chunk_id']
                    rows = stat['rows']
                    chunk_rows[chunk_id] = rows
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                unique_chunks = len(set(chunk_rows.keys()))
                if unique_chunks < total_chunks:
                    print(f"âš ï¸ é‡è¤‡ãƒãƒ£ãƒ³ã‚¯æ¤œå‡º: {total_chunks - unique_chunks} å€‹")
                
                # è¡Œæ•°ã®åˆ†å¸ƒã‚’ç¢ºèª
                row_counts = list(chunk_rows.values())
                if row_counts:
                    avg_rows = sum(row_counts) / len(row_counts)
                    min_rows = min(row_counts)
                    max_rows = max(row_counts)
                    print(f"â”œâ”€ å¹³å‡è¡Œæ•°/ãƒãƒ£ãƒ³ã‚¯: {avg_rows:,.0f} è¡Œ")
                    print(f"â”œâ”€ æœ€å°è¡Œæ•°: {min_rows:,} è¡Œ")
                    print(f"â”œâ”€ æœ€å¤§è¡Œæ•°: {max_rows:,} è¡Œ")
                    print(f"â””â”€ å¤‰å‹•ä¿‚æ•°: {(max_rows - min_rows) / avg_rows * 100:.1f}%")
        # è©³ç´°ãªãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆã‚’è¡¨ç¤ºï¼ˆcolumnsãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        if 'columns' in locals() and columns and chunk_stats:
            print_detailed_chunk_statistics(columns)
        
        # æ€§èƒ½æ¸¬å®šå®Œäº†å¾Œã€ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã‚’å®Ÿè¡Œ
        sample_parquet = "output/chunk_0_queue.parquet"
        if os.path.exists(sample_parquet):
            # chunk_0ã®GPUå‡¦ç†è¡Œæ•°ã‚’å–å¾—
            gpu_rows_chunk0 = gpu_row_counts.get(0, None)
            validate_parquet_output(sample_parquet, num_rows=5, gpu_rows=gpu_rows_chunk0)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
        
        # æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if gpu_memory_manager:
            gpu_memory_manager.aggressive_cleanup()
        
        # å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®è¡Œæ•°ã‚’ç¢ºèª
        print("\nã€Parquetãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã€‘")
        actual_total_rows = 0
        try:
            from pathlib import Path as PathLib
            parquet_files = sorted(PathLib("output").glob("chunk_*_queue.parquet"))
            for pf in parquet_files:
                try:
                    table = pq.read_table(pf)
                    actual_total_rows += table.num_rows
                    print(f"â”œâ”€ {pf.name}: {table.num_rows:,} è¡Œ")
                except Exception as e:
                    print(f"â”œâ”€ {pf.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
            print(f"â””â”€ å®Ÿéš›ã®ç·è¡Œæ•°: {actual_total_rows:,} è¡Œ")
            
            # è¡Œæ•°ã®æ¤œè¨¼ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰
            gpu_total = 0
            if 'results' in locals() and results:
                gpu_total = results.get('total_rows', 0)
                if actual_total_rows != gpu_total:
                    print(f"\nâš ï¸  è¡Œæ•°ä¸ä¸€è‡´: GPUå ±å‘Šå€¤ {gpu_total:,} vs Parquetå®Ÿéš›å€¤ {actual_total_rows:,}")
                    
            # PostgreSQLã®å®Ÿéš›ã®è¡Œæ•°ã¨æ¯”è¼ƒï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰
            if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1':
                dsn = os.environ.get("GPUPASER_PG_DSN")
                if dsn:
                    try:
                        with psycopg.connect(dsn) as conn:
                            with conn.cursor() as cursor:
                                cursor.execute(f"SELECT COUNT(*) FROM {TABLE_NAME}")
                                pg_count = cursor.fetchone()[0]
                                print(f"\nğŸ“Š è¡Œæ•°æ¤œè¨¼çµæœ:")
                                print(f"â”œâ”€ PostgreSQLå®Ÿè¡Œæ•°: {pg_count:,} è¡Œ")
                                print(f"â”œâ”€ Parquetå®Ÿéš›å€¤: {actual_total_rows:,} è¡Œ")
                                print(f"â”œâ”€ å·®åˆ†: {actual_total_rows - pg_count:,} è¡Œ")
                                if actual_total_rows == pg_count:
                                    print(f"â””â”€ âœ… ä¸€è‡´")
                                else:
                                    print(f"â””â”€ âŒ ä¸ä¸€è‡´")
                                    print(f"\nè©³ç´°åˆ†æ:")
                                    print(f"â”œâ”€ èª¤å·®ç‡: {abs(actual_total_rows - pg_count) / pg_count * 100:.4f}%")
                                    if actual_total_rows > pg_count:
                                        print(f"â””â”€ é‡è¤‡è¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                                    else:
                                        print(f"â””â”€ æ¬ æè¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                    except Exception as e:
                        print(f"PostgreSQLè¡Œæ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            print(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
        
        cleanup_files(total_chunks)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
    parser.add_argument('--table', type=str, default='lineorder', help='å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å')
    parser.add_argument('--parallel', type=int, default=16, help='ä¸¦åˆ—æ¥ç¶šæ•°')
    parser.add_argument('--chunks', type=int, default=8, help='ãƒãƒ£ãƒ³ã‚¯æ•°')
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæœ€å¾Œã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤ºï¼‰')
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ['RUST_PARALLEL_CONNECTIONS'] = str(args.parallel)
    os.environ['TOTAL_CHUNKS'] = str(args.chunks)
    os.environ['TABLE_NAME'] = args.table  # Rustå´ã«ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä¼ãˆã‚‹
    
    main(total_chunks=args.chunks, table_name=args.table, test_mode=args.test)