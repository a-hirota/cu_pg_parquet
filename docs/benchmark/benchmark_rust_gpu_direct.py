"""
PostgreSQL â†’ Rust â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆ
Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã§çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
"""

import os
import time
import subprocess
import json
import numpy as np
from numba import cuda
import rmm
import cudf
import cupy as cp
import kvikio
from pathlib import Path
from typing import List, Dict, Any, Optional
import psycopg
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys
import pandas as pd

from src.types import ColumnMeta, PG_OID_TO_ARROW, UNKNOWN
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet_direct
from src.cuda_kernels.postgres_binary_parser import detect_pg_header_size
from src.readPostgres.metadata import fetch_column_meta
import pyarrow.parquet as pq

TABLE_NAME = "lineorder"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå®Ÿè¡Œæ™‚ã«ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
OUTPUT_DIR = "/dev/shm"
RUST_BINARY = "/home/ubuntu/gpupgparser/rust_bench_optimized/target/release/pg_fast_copy_single_chunk"
MAX_QUEUE_SIZE = 3  # ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚º

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
chunk_stats = []
gpu_row_counts = {}  # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯IDã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸ï¼‰
shutdown_flag = threading.Event()


def signal_handler(sig, frame):
    """Ctrl+Cãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    print("\n\nâš ï¸  å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    shutdown_flag.set()
    sys.exit(1)


signal.signal(signal.SIGINT, signal_handler)


def setup_rmm_pool():
    """RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’é©åˆ‡ã«è¨­å®š"""
    try:
        if rmm.is_initialized():
            return
        
        # GPUãƒ¡ãƒ¢ãƒªã®90%ã‚’ä½¿ç”¨å¯èƒ½ã«è¨­å®š
        gpu_memory = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem']
        pool_size = int(gpu_memory * 0.9)
        
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=pool_size,
            maximum_pool_size=pool_size
        )
    except Exception as e:
        print(f"âš ï¸ RMMåˆæœŸåŒ–è­¦å‘Š: {e}")


def get_postgresql_metadata(table_name):
    """PostgreSQLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    conn = psycopg.connect(dsn)
    try:
        print(f"PostgreSQLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ (ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name})...")
        columns = fetch_column_meta(conn, f"SELECT * FROM {table_name}")
        print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(columns)} åˆ—")
        
        
        return columns
    finally:
        conn.close()


def cleanup_files(total_chunks=8, table_name=None):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ä½¿ç”¨
    if table_name is None:
        table_name = TABLE_NAME
    
    # é€šå¸¸ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
    files = [
        f"{OUTPUT_DIR}/{table_name}_meta_0.json",
        f"{OUTPUT_DIR}/{table_name}_data_0.ready"
    ] + [f"{OUTPUT_DIR}/{table_name}_chunk_{i}.bin" for i in range(total_chunks)]
    
    for f in files:
        if os.path.exists(f):
            os.remove(f)
    
    # è¿½åŠ ã®å®‰å…¨å¯¾ç­–: OUTPUT_DIRå†…ã®å…¨ã¦ã®.binãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åãŒä¸€è‡´ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰
    try:
        output_path = Path(OUTPUT_DIR)
        if output_path.exists():
            # ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«é–¢é€£ã™ã‚‹å…¨ã¦ã®.binãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            for bin_file in output_path.glob(f"{table_name}_*.bin"):
                if bin_file.is_file():
                    bin_file.unlink()
    except Exception as e:
        print(f"âš ï¸ è¿½åŠ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã®è­¦å‘Š: {e}")


def rust_producer(chunk_queue: queue.Queue, total_chunks: int, stats_queue: queue.Queue, table_name: str):
    """Rustè»¢é€ã‚’å®Ÿè¡Œã™ã‚‹Producerã‚¹ãƒ¬ãƒƒãƒ‰"""
    for chunk_id in range(total_chunks):
        if shutdown_flag.is_set():
            break
            
        try:
            print(f"\n[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}/{total_chunks} Rustè»¢é€é–‹å§‹...")
            
            env = os.environ.copy()
            env['CHUNK_ID'] = str(chunk_id)
            env['TOTAL_CHUNKS'] = str(total_chunks)
            
            rust_start = time.time()
            process = subprocess.run(
                [RUST_BINARY],
                capture_output=True,
                text=True,
                env=env
            )
            
            if process.returncode != 0:
                print(f"âŒ Rustã‚¨ãƒ©ãƒ¼: {process.stderr}")
                continue
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€Rustã®å‡ºåŠ›ã‚’è¡¨ç¤º
            if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
                for line in process.stdout.split('\n'):
                    if 'ãƒãƒ£ãƒ³ã‚¯' in line or 'ãƒšãƒ¼ã‚¸' in line or 'COPYç¯„å›²' in line:
                        print(f"[Rust Debug] {line}")
            
            # JSONçµæœã‚’æŠ½å‡º
            output = process.stdout
            json_start = output.find("===CHUNK_RESULT_JSON===")
            json_end = output.find("===END_CHUNK_RESULT_JSON===")
            
            if json_start != -1 and json_end != -1:
                json_str = output[json_start + len("===CHUNK_RESULT_JSON==="):json_end].strip()
                result = json.loads(json_str)
                rust_time = result['elapsed_seconds']
                file_size = result['total_bytes']
                chunk_file = result['chunk_file']
            else:
                rust_time = time.time() - rust_start
                chunk_file = f"{OUTPUT_DIR}/{table_name}_chunk_{chunk_id}.bin"
                file_size = os.path.getsize(chunk_file)
            
            chunk_info = {
                'chunk_id': chunk_id,
                'chunk_file': chunk_file,
                'file_size': file_size,
                'rust_time': rust_time
            }
            
            print(f"[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} è»¢é€å®Œäº† ({rust_time:.1f}ç§’, {file_size / 1024**3:.1f}GB)")
            
            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_queue.put(chunk_info)
            stats_queue.put(('rust_time', rust_time))
            
        except Exception as e:
            print(f"[Producer] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
    chunk_queue.put(None)
    print("[Producer] å…¨ãƒãƒ£ãƒ³ã‚¯è»¢é€å®Œäº†")


def gpu_consumer(chunk_queue: queue.Queue, columns: List[ColumnMeta], consumer_id: int, stats_queue: queue.Queue, total_chunks: int, table_name: str, test_mode: bool = False):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹Consumerã‚¹ãƒ¬ãƒƒãƒ‰"""
    while not shutdown_flag.is_set():
        try:
            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_info = chunk_queue.get(timeout=1)
            
            if chunk_info is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                break
                
            chunk_id = chunk_info['chunk_id']
            chunk_file = chunk_info['chunk_file']
            file_size = chunk_info['file_size']
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†é–‹å§‹...")
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            gpu_start = time.time()
            
            # kvikio+RMMã§ç›´æ¥GPUè»¢é€
            transfer_start = time.time()
            
            
            # RMM DeviceBufferã‚’ä½¿ç”¨
            gpu_buffer = rmm.DeviceBuffer(size=file_size)
            
            # kvikioã§ç›´æ¥èª­ã¿è¾¼ã¿
            with kvikio.CuFile(chunk_file, "rb") as f:
                gpu_array = cp.asarray(gpu_buffer).view(dtype=cp.uint8)
                bytes_read = f.read(gpu_array)
            
            if bytes_read != file_size:
                raise RuntimeError(f"èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºä¸ä¸€è‡´: {bytes_read} != {file_size}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆkvikioèª­ã¿è¾¼ã¿å¾Œï¼‰ - ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®ã¿
            if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
                if os.path.exists(chunk_file):
                    print(f"[Consumer-{consumer_id}] kvikioèª­ã¿è¾¼ã¿å¾Œ: {chunk_file} ã¯ã¾ã å­˜åœ¨ã—ã¾ã™")
                else:
                    print(f"[Consumer-{consumer_id}] kvikioèª­ã¿è¾¼ã¿å¾Œ: {chunk_file} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
            
            # numba cudaé…åˆ—ã«å¤‰æ›ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
            raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
            
            transfer_time = time.time() - transfer_start
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
            header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
            header_size = detect_pg_header_size(header_sample)
            
            # ç›´æ¥æŠ½å‡ºå‡¦ç†
            chunk_output = f"output/{table_name}_chunk_{chunk_id}_queue.parquet"
            
            # ãƒãƒ£ãƒ³ã‚¯IDã¨æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
            os.environ['GPUPGPARSER_CURRENT_CHUNK'] = str(chunk_id)
            if chunk_id == total_chunks - 1:
                os.environ['GPUPGPARSER_LAST_CHUNK'] = '1'
            else:
                os.environ['GPUPGPARSER_LAST_CHUNK'] = '0'
            
            cudf_df, detailed_timing = postgresql_to_cudf_parquet_direct(
                raw_dev=raw_dev,
                columns=columns,
                ncols=len(columns),
                header_size=header_size,
                output_path=chunk_output,
                compression='snappy',
                use_rmm=True,
                optimize_gpu=True,
                verbose=False,
                test_mode=(os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1')
            )
            
            gpu_time = time.time() - gpu_start
            
            # å‡¦ç†çµ±è¨ˆ
            rows = len(cudf_df) if cudf_df is not None else 0
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆGPUå‡¦ç†å¾Œï¼‰
            if os.path.exists(chunk_file):
                print(f"[Consumer-{consumer_id}] GPUå‡¦ç†å¾Œ: {chunk_file} ã¯ã¾ã å­˜åœ¨ã—ã¾ã™")
            else:
                print(f"[Consumer-{consumer_id}] GPUå‡¦ç†å¾Œ: {chunk_file} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
            
            # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜
            gpu_row_counts[chunk_id] = rows
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†å®Œäº† ({gpu_time:.1f}ç§’, {rows:,}è¡Œ)")
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
            if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1':
                print(f"[CHUNK DEBUG] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}: ")
                print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size / 1024**2:.1f} MB")
                print(f"  - æ¤œå‡ºè¡Œæ•°: {rows:,}è¡Œ")
                print(f"  - GPUãƒ‘ãƒ¼ã‚¹æ™‚é–“: {detailed_timing.get('gpu_parsing', 0):.2f}ç§’")
                print(f"  - è¡Œã‚ãŸã‚Š: {file_size/rows if rows > 0 else 0:.1f} bytes/row")
            
            # çµ±è¨ˆæƒ…å ±ã‚’é€ä¿¡
            stats_queue.put(('gpu_time', gpu_time))
            stats_queue.put(('transfer_time', transfer_time))
            stats_queue.put(('rows', rows))
            stats_queue.put(('size', file_size))
            
            # è©³ç´°çµ±è¨ˆã‚’ä¿å­˜
            chunk_stats.append({
                'chunk_id': chunk_id,
                'consumer_id': consumer_id,
                'rust_time': chunk_info['rust_time'],
                'gpu_time': gpu_time,
                'transfer_time': transfer_time,
                'parse_time': detailed_timing.get('gpu_parsing', 0),
                'string_time': detailed_timing.get('string_buffer_creation', 0),
                'write_time': detailed_timing.get('parquet_export', 0),
                'rows': rows,
                'size_gb': file_size / 1024**3
            })
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del raw_dev
            del gpu_buffer
            del gpu_array
            if cudf_df is not None:
                del cudf_df
            
            mempool.free_all_blocks()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å‰Šé™¤å‰ã«ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            if test_mode:
                # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                if not hasattr(gpu_consumer, 'test_save_dir'):
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    gpu_consumer.test_save_dir = f"test_binaries/{timestamp}"
                    os.makedirs(gpu_consumer.test_save_dir, exist_ok=True)
                    print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å…ˆ - {gpu_consumer.test_save_dir}")
                
                # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                import shutil
                dst = f"{gpu_consumer.test_save_dir}/{table_name}_chunk_{chunk_id}.bin"
                shutil.copy2(chunk_file, dst)
                size = os.path.getsize(chunk_file) / (1024**3)
                print(f"  âœ“ {table_name}_chunk_{chunk_id}.bin ã‚’ä¿å­˜ ({size:.2f} GB)")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if os.path.exists(chunk_file):
                os.remove(chunk_file)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"[Consumer-{consumer_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[Consumer-{consumer_id}] çµ‚äº†")


def validate_parquet_output(file_path: str, num_rows: int = 5, gpu_rows: int = None) -> bool:
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        num_rows: è¡¨ç¤ºã™ã‚‹è¡Œæ•°
        gpu_rows: GPUå‡¦ç†ã§æ¤œå‡ºã—ãŸè¡Œæ•°ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    
    Returns:
        æ¤œè¨¼æˆåŠŸã®å ´åˆTrue
    """
    try:
        # PyArrowã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        table = pq.read_table(file_path)
        
        print(f"\nğŸ“Š Parquetãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {os.path.basename(file_path)}")
        print(f"â”œâ”€ è¡Œæ•°: {table.num_rows:,}", end="")
        if gpu_rows is not None:
            if table.num_rows == gpu_rows:
                print(f" âœ… OK (GPUå‡¦ç†è¡Œæ•°ã¨ä¸€è‡´)")
            else:
                print(f" âŒ NG (GPUå‡¦ç†è¡Œæ•°: {gpu_rows:,})")
        else:
            print()
        print(f"â”œâ”€ åˆ—æ•°: {table.num_columns}")
        print(f"â””â”€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(file_path) / 1024**2:.2f} MB")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­{num_rows}è¡Œï¼‰:")
        print("â”€" * 80)
        df_sample = table.slice(0, num_rows).to_pandas()
        print(df_sample.to_string(index=False, max_colwidth=20))
        print("â”€" * 80)
        
        return True
        
    except Exception as e:
        print(f"âŒ Parquetæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_parallel_pipeline(columns: List[ColumnMeta], total_chunks: int, table_name: str, test_mode: bool = False):
    """çœŸã®ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ã‚­ãƒ¥ãƒ¼ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†
    chunk_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
    stats_queue = queue.Queue()
    
    start_time = time.time()
    
    # Producerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
    producer_thread = threading.Thread(
        target=rust_producer,
        args=(chunk_queue, total_chunks, stats_queue, table_name)
    )
    producer_thread.start()
    
    # GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ï¼ˆProduceré–‹å§‹ã¨åŒæ™‚ã«å®Ÿè¡Œï¼‰
    warmup_thread = threading.Thread(
        target=gpu_warmup,
        args=(columns,)
    )
    warmup_thread.start()
    
    # Consumerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆ1ã¤ã®ã¿ - GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼‰
    consumer_thread = threading.Thread(
        target=gpu_consumer,
        args=(chunk_queue, columns, 1, stats_queue, total_chunks, table_name, test_mode)
    )
    consumer_thread.start()
    
    # çµ±è¨ˆåé›†
    total_rust_time = 0
    total_gpu_time = 0
    total_transfer_time = 0
    total_rows = 0
    total_size = 0
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿã—ãªãŒã‚‰çµ±è¨ˆã‚’åé›†
    warmup_thread.join()  # ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—å®Œäº†ã‚’å¾…ã¤
    producer_thread.join()
    consumer_thread.join()
    
    # çµ±è¨ˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœã‚’åé›†
    while not stats_queue.empty():
        stat_type, value = stats_queue.get()
        if stat_type == 'rust_time':
            total_rust_time += value
        elif stat_type == 'gpu_time':
            total_gpu_time += value
        elif stat_type == 'transfer_time':
            total_transfer_time += value
        elif stat_type == 'rows':
            total_rows += value
        elif stat_type == 'size':
            total_size += value
    
    total_time = time.time() - start_time
    
    return {
        'total_time': total_time,
        'total_rust_time': total_rust_time,
        'total_gpu_time': total_gpu_time,
        'total_transfer_time': total_transfer_time,
        'total_rows': total_rows,
        'total_size': total_size,
        'processed_chunks': len(chunk_stats)
    }


def gpu_warmup(columns):
    """GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ— - JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨CUDAåˆæœŸåŒ–"""
    try:
        print("\nğŸ”¥ GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ä¸­...")
        
        # PostgreSQL COPY BINARYãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ19ãƒã‚¤ãƒˆï¼‰
        header = [
            0x50, 0x47, 0x43, 0x4F, 0x50, 0x59, 0x0A, 0xFF, 0x0D, 0x0A, 0x00,  # PGCOPY
            0x00, 0x00, 0x00, 0x00,  # flags
            0x00, 0x00, 0x00, 0x00   # header extension
        ]
        
        # 1è¡Œåˆ†ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ17åˆ—ï¼‰ã‚’ä½œæˆ
        row_data = []
        row_data.extend([0x00, 0x11])  # 17ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        
        for i in range(17):
            if i < 8:  # æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆint64ï¼‰
                row_data.extend([0x00, 0x00, 0x00, 0x08])  # é•·ã•8
                row_data.extend([0x00] * 8)  # 8ãƒã‚¤ãƒˆã®ã‚¼ãƒ­
            else:  # æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                row_data.extend([0x00, 0x00, 0x00, 0x04])  # é•·ã•4
                row_data.extend([0x54, 0x45, 0x53, 0x54])  # "TEST"
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆ100KBç¨‹åº¦ - ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚µã‚¤ã‚ºï¼‰
        dummy_list = header + row_data * 1000  # 1000è¡Œåˆ†
        # çµ‚ç«¯ãƒãƒ¼ã‚«ãƒ¼è¿½åŠ ï¼ˆ0xFFFFï¼‰
        dummy_list.extend([0xFF, 0xFF])
        dummy_data = np.array(dummy_list, dtype=np.uint8)
        
        # GPUå‡¦ç†å®Ÿè¡Œï¼ˆJITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨CUDAåˆæœŸåŒ–ï¼‰
        # å®Ÿéš›ã®å‡¦ç†ã¨åŒã˜ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        os.environ['GPUPGPARSER_ROWS_PER_THREAD'] = os.environ.get('GPUPGPARSER_ROWS_PER_THREAD', '32')
        os.environ['GPUPGPARSER_STRING_ROWS_PER_THREAD'] = os.environ.get('GPUPGPARSER_STRING_ROWS_PER_THREAD', '1')
        
        # GPUè»¢é€
        import cupy as cp
        gpu_buffer = cp.asarray(dummy_data).view(dtype=cp.uint8)
        raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
        header_size = detect_pg_header_size(dummy_data)
        
        # DirectColumnExtractorã‚’ä½¿ç”¨ã—ã¦å‡¦ç†
        from src.postgres_to_cudf import DirectColumnExtractor
        from src.cuda_kernels.postgres_binary_parser import parse_binary_chunk_gpu_ultra_fast_v2_lite
        
        # GPUãƒ‘ãƒ¼ã‚¹ã‚«ãƒ¼ãƒãƒ«ã‚’ç›´æ¥å®Ÿè¡Œï¼ˆJITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç¢ºå®Ÿã«å®Ÿè¡Œï¼‰
        row_positions, field_offsets, field_lengths = parse_binary_chunk_gpu_ultra_fast_v2_lite(
            raw_dev, columns, header_size, debug=False, test_mode=False
        )
        
        # Extractorã§cuDF DataFrameä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‰
        extractor = DirectColumnExtractor()
        
        # å›ºå®šé•·åˆ—ã¨æ–‡å­—åˆ—åˆ—ã®å‡¦ç†ï¼ˆJITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰
        fixed_dfs = extractor.extract_fixed_columns(
            raw_dev, columns, row_positions, field_offsets, field_lengths
        )
        
        # æœ€å°é™ã®cuDFçµåˆæ“ä½œ
        import cudf
        if fixed_dfs:
            cudf_df = cudf.concat(fixed_dfs, axis=1)
        
        # çµæœã‚’ç ´æ£„
        if fixed_dfs:
            del cudf_df
        del row_positions
        del field_offsets
        del field_lengths
        del gpu_buffer
        del raw_dev
        
        print("âœ… GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—å®Œäº†\n")
        
    except Exception as e:
        print(f"âš ï¸  GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}\n")


def main(total_chunks=8, table_name=None, test_mode=False, test_duplicate_keys=None):
    global TABLE_NAME
    if table_name:
        TABLE_NAME = table_name
    else:
        table_name = TABLE_NAME  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€GPUç‰¹æ€§ã‚’è¡¨ç¤º
    if test_mode:
        from src.cuda_kernels.postgres_binary_parser import print_gpu_properties
        print_gpu_properties()
    
    # kvikioè¨­å®šç¢ºèª
    is_compat = os.environ.get("KVIKIO_COMPAT_MODE", "").lower() in ["on", "1", "true"]
    
    # RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š
    setup_rmm_pool()
    
    # CUDA contextç¢ºèª
    try:
        cuda.current_context()
    except Exception as e:
        print(f"âŒ CUDA context ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_files(total_chunks, table_name)
    
    # PostgreSQLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    columns = get_postgresql_metadata(table_name)
    
    try:
        # ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        print("\nä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        print("=" * 80)
        
        results = run_parallel_pipeline(columns, total_chunks, table_name, test_mode)
        
        # æœ€çµ‚çµ±è¨ˆã‚’æ§‹é€ åŒ–è¡¨ç¤º
        total_gb = results['total_size'] / 1024**3
        
        print(f"\n{'='*80}")
        print(f" âœ… å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")
        
        # å…¨ä½“çµ±è¨ˆ
        print(f"\nã€å…¨ä½“çµ±è¨ˆã€‘")
        print(f"â”œâ”€ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_gb:.2f} GB")
        print(f"â”œâ”€ ç·è¡Œæ•°: {results['total_rows']:,} è¡Œ")
        print(f"â”œâ”€ ç·å®Ÿè¡Œæ™‚é–“: {results['total_time']:.2f}ç§’")
        print(f"â””â”€ å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_gb / results['total_time']:.2f} GB/ç§’")
        
        # å‡¦ç†æ™‚é–“å†…è¨³
        print(f"\nã€å‡¦ç†æ™‚é–“å†…è¨³ã€‘")
        print(f"â”œâ”€ Rustè»¢é€åˆè¨ˆ: {results['total_rust_time']:.2f}ç§’ ({total_gb / results['total_rust_time']:.2f} GB/ç§’)")
        print(f"â””â”€ GPUå‡¦ç†åˆè¨ˆ: {results['total_gpu_time']:.2f}ç§’ ({total_gb / results['total_gpu_time']:.2f} GB/ç§’)")
        print(f"   â””â”€ kvikioè»¢é€: {results['total_transfer_time']:.2f}ç§’")
        
        
        # ãƒãƒ£ãƒ³ã‚¯æ¯ã®è©³ç´°çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        if chunk_stats:
            print(f"\nã€ãƒãƒ£ãƒ³ã‚¯æ¯ã®å‡¦ç†æ™‚é–“ã€‘")
            print(f"â”Œ{'â”€'*7}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*12}â”")
            print(f"â”‚ãƒãƒ£ãƒ³ã‚¯â”‚ Rustè»¢é€ â”‚kvikioè»¢é€â”‚ GPUãƒ‘ãƒ¼ã‚¹â”‚ æ–‡å­—åˆ—å‡¦ç†â”‚ Parquet â”‚   å‡¦ç†è¡Œæ•°  â”‚")
            print(f"â”œ{'â”€'*7}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*12}â”¤")
            
            # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
            sorted_stats = sorted(chunk_stats, key=lambda x: x['chunk_id'])
            for stat in sorted_stats:
                print(f"â”‚  {stat['chunk_id']:^3}  â”‚ {stat['rust_time']:>6.2f}ç§’ â”‚ {stat['transfer_time']:>6.2f}ç§’ â”‚"
                      f" {stat['parse_time']:>6.2f}ç§’ â”‚ {stat['string_time']:>6.2f}ç§’ â”‚"
                      f" {stat['write_time']:>6.2f}ç§’ â”‚{stat['rows']:>10,}è¡Œâ”‚")
            
            print(f"â””{'â”€'*7}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*12}â”˜")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ€§èƒ½æ¸¬å®šå®Œäº†å¾Œã€ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã‚’å®Ÿè¡Œ
        sample_parquet = f"output/{table_name}_chunk_0_queue.parquet"
        if os.path.exists(sample_parquet):
            # chunk_0ã®GPUå‡¦ç†è¡Œæ•°ã‚’å–å¾—
            gpu_rows_chunk0 = gpu_row_counts.get(0, None)
            validate_parquet_output(sample_parquet, num_rows=5, gpu_rows=gpu_rows_chunk0)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
        
        # å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®è¡Œæ•°ã‚’ç¢ºèª
        print("\nã€Parquetãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã€‘")
        actual_total_rows = 0
        parquet_files = sorted(Path("output").glob(f"{table_name}_chunk_*_queue.parquet"))
        for pf in parquet_files:
            try:
                table = pq.read_table(pf)
                actual_total_rows += table.num_rows
                print(f"â”œâ”€ {pf.name}: {table.num_rows:,} è¡Œ")
            except Exception as e:
                print(f"â”œâ”€ {pf.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
        print(f"â””â”€ å®Ÿéš›ã®ç·è¡Œæ•°: {actual_total_rows:,} è¡Œ")
        
        # --testãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã¨æ¯”è¼ƒ
        if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
            print("\nã€PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã¨ã®æ¯”è¼ƒã€‘")
            try:
                # PostgreSQLã®ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã‚’å–å¾—
                dsn = os.environ.get("GPUPASER_PG_DSN", "")
                with psycopg.connect(dsn) as conn:
                    with conn.cursor() as cur:
                        cur.execute(f"SELECT COUNT(*) FROM {table_name}")
                        pg_row_count = cur.fetchone()[0]
                        print(f"psql -c \"SELECT COUNT(*) FROM {table_name};\"ã®çµæœ: {pg_row_count:,} è¡Œ")
                        
                        # æ¯”è¼ƒçµæœã‚’è¡¨ç¤º
                        print(f"\nğŸ“Š Parquetå…¨ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: ")
                        if actual_total_rows == pg_row_count:
                            print(f"â””â”€ è¡Œæ•°: {actual_total_rows:,} OK (psqlã¨ä¸€è‡´)")
                        else:
                            print(f"â””â”€ è¡Œæ•°: {actual_total_rows:,} NG (psqlã¨ä¸ä¸€è‡´:{pg_row_count:,})")
                            
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚­ãƒ¼åˆ—ã‚’æ±ºå®š
                        key_columns = []
                        if test_duplicate_keys:
                            # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                            key_columns = [col.strip() for col in test_duplicate_keys.split(',')]
                            if len(key_columns) > 1:
                                print(f"\nã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆè¤‡åˆã‚­ãƒ¼: {', '.join(key_columns)}ï¼‰ã€‘")
                            else:
                                print(f"\nã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ¼åˆ—: {key_columns[0]}ï¼‰ã€‘")
                        else:
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ€åˆã®åˆ—ã‚’ä½¿ç”¨
                            cur.execute(f"""
                                SELECT column_name 
                                FROM information_schema.columns 
                                WHERE table_name = '{table_name}' 
                                AND ordinal_position = 1
                            """)
                            result = cur.fetchone()
                            if result:
                                key_columns = [result[0]]
                                print(f"\nã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ¼åˆ—: {key_columns[0]}ï¼‰ã€‘")
                        
                        if key_columns:
                            
                            # å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
                            import cudf
                            all_dfs = []
                            duplicate_info = {}
                            
                            for i, pf in enumerate(parquet_files):
                                try:
                                    df = cudf.read_parquet(pf)
                                    
                                    # åˆ—ã®å­˜åœ¨ç¢ºèª
                                    missing_cols = [col for col in key_columns if col not in df.columns]
                                    if missing_cols:
                                        print(f"â”œâ”€ {pf.name}: âš ï¸ è­¦å‘Š - æŒ‡å®šã•ã‚ŒãŸåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {', '.join(missing_cols)}")
                                        print(f"â”‚   åˆ©ç”¨å¯èƒ½ãªåˆ—: {', '.join(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}")
                                        continue
                                    
                                    # å„ãƒãƒ£ãƒ³ã‚¯å†…ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
                                    total_rows = len(df)
                                    
                                    if len(key_columns) == 1:
                                        # å˜ä¸€ã‚­ãƒ¼ã®å ´åˆ
                                        key_column_name = key_columns[0]
                                        unique_rows = df[key_column_name].nunique()
                                        duplicates_in_chunk = total_rows - unique_rows
                                    else:
                                        # è¤‡åˆã‚­ãƒ¼ã®å ´åˆ
                                        # è¤‡åˆã‚­ãƒ¼ã®å€¤ã‚’çµåˆã—ãŸä¸€æ™‚åˆ—ã‚’ä½œæˆ
                                        df['_composite_key'] = df[key_columns[0]].astype(str)
                                        for col in key_columns[1:]:
                                            df['_composite_key'] = df['_composite_key'] + '_' + df[col].astype(str)
                                        
                                        unique_rows = df['_composite_key'].nunique()
                                        duplicates_in_chunk = total_rows - unique_rows
                                        
                                    duplicate_info[pf.name] = {
                                        'total': total_rows,
                                        'unique': unique_rows,
                                        'duplicates': duplicates_in_chunk
                                    }
                                    
                                    if duplicates_in_chunk > 0:
                                            print(f"â”œâ”€ {pf.name}: {duplicates_in_chunk:,}å€‹ã®é‡è¤‡")
                                            # é‡è¤‡ã‚­ãƒ¼ã®ä¾‹ã‚’è¡¨ç¤º
                                            try:
                                                if len(key_columns) == 1:
                                                    # å˜ä¸€ã‚­ãƒ¼ã®å ´åˆ
                                                    key_column_name = key_columns[0]
                                                    # Decimalåˆ—ã®å ´åˆã¯int64ã«å¤‰æ›
                                                    if hasattr(df[key_column_name].dtype, 'precision'):
                                                        key_series = df[key_column_name].astype('int64')
                                                    else:
                                                        key_series = df[key_column_name]
                                                    key_counts = key_series.value_counts()
                                                    dup_keys = key_counts[key_counts > 1].head(5)
                                                else:
                                                    # è¤‡åˆã‚­ãƒ¼ã®å ´åˆ
                                                    key_counts = df['_composite_key'].value_counts()
                                                    dup_keys = key_counts[key_counts > 1].head(5)
                                            except Exception as e:
                                                print(f"â”‚   â””â”€ é‡è¤‡ã‚­ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                                                dup_keys = []
                                            
                                            # thread_idæƒ…å ±ãŒã‚ã‚Œã°è©³ç´°è¡¨ç¤º
                                            if '_thread_id' in df.columns and len(dup_keys) > 0:
                                                print(f"â”‚   â””â”€ ã‚¹ãƒ¬ãƒƒãƒ‰IDæƒ…å ±ä»˜ãé‡è¤‡åˆ†æ:")
                                                # dup_keysã‚’pandasã«å¤‰æ›ã—ã¦iterableã«ã™ã‚‹
                                                dup_keys_items = dup_keys.to_pandas().items()
                                                
                                                # æœ€åˆã®3ã¤ã®é‡è¤‡ã‚­ãƒ¼ã«ã¤ã„ã¦å…¨åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                                                sample_count = 0
                                                for key, count in dup_keys_items:
                                                    if sample_count >= 3:  # æœ€åˆã®3ã¤ã®ã¿
                                                        break
                                                    sample_count += 1
                                                    
                                                    # ã“ã®é‡è¤‡ã‚­ãƒ¼ã‚’æŒã¤è¡Œã®thread_idæƒ…å ±ã‚’å–å¾—
                                                    if len(key_columns) == 1:
                                                        # å˜ä¸€ã‚­ãƒ¼ã®å ´åˆ
                                                        key_column_name = key_columns[0]
                                                        # Decimalåˆ—ã®å ´åˆã¯int64ã«å¤‰æ›
                                                        if hasattr(df[key_column_name].dtype, 'precision'):
                                                            dup_rows = df[df[key_column_name].astype('int64') == int(key)]
                                                        else:
                                                            dup_rows = df[df[key_column_name] == key]
                                                        print(f"â”‚       â””â”€ {key_column_name}={key}: {count}å›å‡ºç¾")
                                                    else:
                                                        # è¤‡åˆã‚­ãƒ¼ã®å ´åˆ
                                                        dup_rows = df[df['_composite_key'] == key]
                                                        # è¤‡åˆã‚­ãƒ¼ã‚’åˆ†è§£ã—ã¦è¡¨ç¤º
                                                        key_parts = key.split('_')
                                                        key_display = ', '.join([f"{k}={v}" for k, v in zip(key_columns, key_parts)])
                                                        print(f"â”‚       â””â”€ {key_display}: {count}å›å‡ºç¾")
                                                    
                                                    thread_ids = dup_rows['_thread_id'].unique().to_pandas()
                                                    print(f"â”‚           å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰: {sorted(thread_ids)}")
                                                    
                                                    # å…¨åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°åˆ—ã‚’é™¤ãï¼‰
                                                    print(f"â”‚           ")
                                                    print(f"â”‚           ã€å…¨åˆ—ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºã€‘")
                                                    # ãƒ‡ãƒãƒƒã‚°åˆ—ã‚’é™¤å¤–
                                                    debug_cols = ['_thread_id', '_row_position', '_thread_start_pos', '_thread_end_pos', '_composite_key']
                                                    display_cols = [col for col in dup_rows.columns if col not in debug_cols]
                                                    
                                                    # å„è¡Œã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                                                    for row_idx in range(min(len(dup_rows), count)):  # å…¨ã¦ã®é‡è¤‡è¡Œã‚’è¡¨ç¤º
                                                        print(f"â”‚           ")
                                                        print(f"â”‚           è¡Œ{row_idx + 1}:")
                                                        
                                                        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º - ç°¡å˜ãªæ–¹æ³•ã§
                                                        for col in display_cols:
                                                            try:
                                                                # dup_rowsã‹ã‚‰ç›´æ¥å€¤ã‚’å–å¾—
                                                                value = dup_rows[col].iloc[row_idx]
                                                                
                                                                # æ–‡å­—åˆ—ã®å ´åˆã¯é•·ã•ã‚‚è¡¨ç¤º
                                                                if pd.api.types.is_string_dtype(dup_rows[col].dtype):
                                                                    # æœ«å°¾ã®ç©ºç™½ã‚’å¯è¦–åŒ–
                                                                    str_value = str(value)
                                                                    visible_value = str_value.replace(' ', 'Â·')
                                                                    print(f"â”‚             {col}: '{str_value}' (é•·ã•: {len(str_value)}, è¡¨ç¤º: '{visible_value}')")
                                                                else:
                                                                    print(f"â”‚             {col}: {value}")
                                                            except Exception as e:
                                                                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å˜ç´”ãªæ–‡å­—åˆ—è¡¨ç¾ã‚’è©¦ã™
                                                                try:
                                                                    simple_value = str(dup_rows[col].iloc[row_idx])
                                                                    print(f"â”‚             {col}: {simple_value}")
                                                                except:
                                                                    print(f"â”‚             {col}: (è¡¨ç¤ºã‚¨ãƒ©ãƒ¼)")
                                                        
                                                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
                                                        if '_thread_id' in dup_rows.columns:
                                                            print(f"â”‚             ---")
                                                            try:
                                                                print(f"â”‚             thread_id: {dup_rows['_thread_id'].iloc[row_idx]}")
                                                                if '_row_position' in dup_rows.columns:
                                                                    print(f"â”‚             row_position: {dup_rows['_row_position'].iloc[row_idx]:,}")
                                                                if '_thread_start_pos' in dup_rows.columns and '_thread_end_pos' in dup_rows.columns:
                                                                    start_pos = dup_rows['_thread_start_pos'].iloc[row_idx]
                                                                    end_pos = dup_rows['_thread_end_pos'].iloc[row_idx]
                                                                    print(f"â”‚             thread_range: [{start_pos:,} - {end_pos:,}]")
                                                            except:
                                                                pass
                                                    
                                                    print(f"â”‚           ")
                                                    
                                                    # å…ƒã®è©³ç´°è¡¨ç¤ºã‚‚ç¶­æŒï¼ˆç°¡ç•¥ç‰ˆï¼‰
                                                    for tid in sorted(thread_ids):
                                                        tid_rows = dup_rows[dup_rows['_thread_id'] == tid]
                                                        print(f"â”‚           â””â”€ thread_id={tid}: {len(tid_rows)}è¡Œ")
                                            else:
                                                # thread_idæƒ…å ±ãŒãªã„å ´åˆ
                                                for key, count in dup_keys.items():
                                                    if len(key_columns) == 1:
                                                        print(f"â”‚   â””â”€ {key_columns[0]}={key}: {count}å›å‡ºç¾")
                                                    else:
                                                        # è¤‡åˆã‚­ãƒ¼ã‚’åˆ†è§£ã—ã¦è¡¨ç¤º
                                                        key_parts = key.split('_')
                                                        key_display = ', '.join([f"{k}={v}" for k, v in zip(key_columns, key_parts)])
                                                        print(f"â”‚   â””â”€ {key_display}: {count}å›å‡ºç¾")
                                    
                                    all_dfs.append(df)
                                except Exception as e:
                                    print(f"â”œâ”€ {pf.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
                            
                            # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯
                            if all_dfs:
                                print(f"\nã€ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã€‘")
                                all_df = cudf.concat(all_dfs)
                                total_all = len(all_df)
                                
                                if len(key_columns) == 1:
                                    # å˜ä¸€ã‚­ãƒ¼ã®å ´åˆ
                                    key_column_name = key_columns[0]
                                    unique_all = all_df[key_column_name].nunique()
                                else:
                                    # è¤‡åˆã‚­ãƒ¼ã®å ´åˆ - å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦_composite_keyã‚’å†ä½œæˆ
                                    all_df['_composite_key'] = all_df[key_columns[0]].astype(str)
                                    for col in key_columns[1:]:
                                        all_df['_composite_key'] = all_df['_composite_key'] + '_' + all_df[col].astype(str)
                                    unique_all = all_df['_composite_key'].nunique()
                                
                                total_duplicates = total_all - unique_all
                                
                                print(f"â”œâ”€ å…¨ãƒãƒ£ãƒ³ã‚¯åˆè¨ˆè¡Œæ•°: {total_all:,}")
                                print(f"â”œâ”€ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼æ•°: {unique_all:,}")
                                print(f"â”œâ”€ ç·é‡è¤‡æ•°: {total_duplicates:,}")
                                
                                # ãƒãƒ£ãƒ³ã‚¯å†…é‡è¤‡ã®åˆè¨ˆ
                                chunk_duplicates = sum(info['duplicates'] for info in duplicate_info.values())
                                inter_chunk_duplicates = total_duplicates - chunk_duplicates
                                
                                if inter_chunk_duplicates > 0:
                                    print(f"â””â”€ ãƒãƒ£ãƒ³ã‚¯é–“é‡è¤‡: {inter_chunk_duplicates:,}å€‹")
                                    
                                    # ãƒãƒ£ãƒ³ã‚¯é–“ã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚­ãƒ¼ã®ç‰¹å®š
                                    key_appearances = {}
                                    for i, df in enumerate(all_dfs):
                                        if len(key_columns) == 1:
                                            # å˜ä¸€ã‚­ãƒ¼ã®å ´åˆ
                                            unique_keys = df[key_columns[0]].unique().to_pandas()
                                        else:
                                            # è¤‡åˆã‚­ãƒ¼ã®å ´åˆ
                                            unique_keys = df['_composite_key'].unique().to_pandas()
                                        
                                        for key in unique_keys:
                                            if key not in key_appearances:
                                                key_appearances[key] = []
                                            key_appearances[key].append(i)
                                    
                                    # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã«å‡ºç¾ã™ã‚‹ã‚­ãƒ¼
                                    multi_chunk_keys = {k: v for k, v in key_appearances.items() if len(v) > 1}
                                    if multi_chunk_keys:
                                        print(f"\n    ãƒãƒ£ãƒ³ã‚¯é–“é‡è¤‡ã‚­ãƒ¼ã®ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:")
                                        
                                        # thread_idæƒ…å ±ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                        has_thread_id = any('_thread_id' in df.columns for df in all_dfs)
                                        
                                        for j, (key, chunks) in enumerate(list(multi_chunk_keys.items())[:5]):
                                            if len(key_columns) == 1:
                                                print(f"    â””â”€ {key_columns[0]}={key}: ãƒãƒ£ãƒ³ã‚¯{chunks}ã«å‡ºç¾")
                                            else:
                                                # è¤‡åˆã‚­ãƒ¼ã‚’åˆ†è§£ã—ã¦è¡¨ç¤º
                                                key_parts = key.split('_')
                                                key_display = ', '.join([f"{k}={v}" for k, v in zip(key_columns, key_parts)])
                                                print(f"    â””â”€ {key_display}: ãƒãƒ£ãƒ³ã‚¯{chunks}ã«å‡ºç¾")
                                            
                                            # thread_idæƒ…å ±ãŒã‚ã‚Œã°è©³ç´°è¡¨ç¤º
                                            if has_thread_id:
                                                for chunk_idx in chunks:
                                                    df = all_dfs[chunk_idx]
                                                    if '_thread_id' in df.columns:
                                                        if len(key_columns) == 1:
                                                            # å˜ä¸€ã‚­ãƒ¼ã®å ´åˆ
                                                            key_column_name = key_columns[0]
                                                            # Decimalåˆ—ã®å ´åˆã¯int64ã«å¤‰æ›
                                                            if hasattr(df[key_column_name].dtype, 'precision'):
                                                                key_rows = df[df[key_column_name].astype('int64') == int(key)]
                                                            else:
                                                                key_rows = df[df[key_column_name] == key]
                                                        else:
                                                            # è¤‡åˆã‚­ãƒ¼ã®å ´åˆ
                                                            key_rows = df[df['_composite_key'] == key]
                                                        
                                                        thread_ids = key_rows['_thread_id'].unique().to_pandas()
                                                        print(f"        â””â”€ ãƒãƒ£ãƒ³ã‚¯{chunk_idx}: thread_id={sorted(thread_ids)}")
                                
                                # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                                del all_df
                                del all_dfs
            except Exception as e:
                print(f"PostgreSQLè¡Œæ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        if actual_total_rows != results['total_rows']:
            print(f"\nâš ï¸  è¡Œæ•°ä¸ä¸€è‡´: GPUå ±å‘Šå€¤ {results['total_rows']:,} vs Parquetå®Ÿéš›å€¤ {actual_total_rows:,}")
        
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if test_mode:
            import shutil
            
            # gpu_consumerã§ä½¿ç”¨ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            if hasattr(gpu_consumer, 'test_save_dir'):
                save_dir = gpu_consumer.test_save_dir
                print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ä¸­...")
            else:
                # gpu_consumerãŒå®Ÿè¡Œã•ã‚Œãªã‹ã£ãŸå ´åˆã®å‡¦ç†
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                save_dir = f"test_binaries/{timestamp}"
                os.makedirs(save_dir, exist_ok=True)
                print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªä¸­...")
                
                # ç¾æ™‚ç‚¹ã§å­˜åœ¨ã™ã‚‹ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆé€šå¸¸ã¯gpu_consumerã§ä¿å­˜æ¸ˆã¿ï¼‰
                saved_count = 0
                for i in range(total_chunks):
                    src = f"{OUTPUT_DIR}/{table_name}_chunk_{i}.bin"
                    if os.path.exists(src):
                        dst = f"{save_dir}/{table_name}_chunk_{i}.bin"
                        shutil.copy2(src, dst)
                        size = os.path.getsize(src) / (1024**3)
                        print(f"  âœ“ {table_name}_chunk_{i}.bin ã‚’ä¿å­˜ ({size:.2f} GB)")
                        saved_count += 1
            
            # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            meta_src = f"{OUTPUT_DIR}/{table_name}_meta_0.json"
            if os.path.exists(meta_src):
                shutil.copy2(meta_src, f"{save_dir}/{table_name}_meta_0.json")
                print(f"  âœ“ {table_name}_meta_0.json ã‚’ä¿å­˜")
            
            # å®Ÿè¡Œæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            from datetime import datetime
            metadata = {
                "timestamp": save_dir.split('/')[-1],  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                "table_name": table_name,
                "total_chunks": total_chunks,
                "saved_chunks": total_chunks,  # gpu_consumerã§å…¨ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã¯ãš
                "note": "Binary files saved before deletion in gpu_consumer",
                "parallel_connections": int(os.environ.get('RUST_PARALLEL_CONNECTIONS', 16)),
                "command": f"python cu_pg_parquet.py --test --table {table_name} --parallel {int(os.environ.get('RUST_PARALLEL_CONNECTIONS', 16))} --chunks {total_chunks}"
            }
            metadata_path = f"{save_dir}/execution_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"  âœ“ execution_metadata.json ã‚’ä¿å­˜")
            
            print(f"\nğŸ“ ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {save_dir} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleanup_files(total_chunks, table_name)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
    parser.add_argument('--table', type=str, default='lineorder', help='å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å')
    parser.add_argument('--parallel', type=int, default=16, help='ä¸¦åˆ—æ¥ç¶šæ•°')
    parser.add_argument('--chunks', type=int, default=8, help='ãƒãƒ£ãƒ³ã‚¯æ•°')
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆGPUç‰¹æ€§ãƒ»ã‚«ãƒ¼ãƒãƒ«æƒ…å ±è¡¨ç¤ºï¼‰')
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ['RUST_PARALLEL_CONNECTIONS'] = str(args.parallel)
    os.environ['TOTAL_CHUNKS'] = str(args.chunks)
    os.environ['TABLE_NAME'] = args.table  # Rustå´ã«ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä¼ãˆã‚‹
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    if args.test:
        os.environ['GPUPGPARSER_TEST_MODE'] = '1'
    
    main(total_chunks=args.chunks, table_name=args.table, test_mode=args.test)