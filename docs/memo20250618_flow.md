(cudf_dev) ubuntu@ubuntu:~/gpupgparser$ python benchmark/benchmark_lineorder_5m.py --rows 50000000
CUDA context OK
ベンチマーク開始: テーブル=lineorder
メタデータを取得中...
メタデータ取得完了 (0.0013秒)
COPY BINARY を実行中...
COPY BINARY 完了 (70.9564秒), データサイズ: 11040.12 MB
GPUにデータを転送中...
GPU転送完了 (0.8836秒)
ヘッダーサイズ: 19 バイト
GPU最適化処理中...
RMM メモリプール初期化完了 (最大8GB)
=== GPU並列パース開始 ===
/home/ubuntu/miniconda3/envs/cudf_dev/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.
  warn(NumbaPerformanceWarning(msg))
✅ Ultra Fast GPU並列パーサー使用（8.94倍高速化達成）
GPUパース完了: 2000000 行 (1.0549秒)
=== 文字列最適化デコード開始 ===
/home/ubuntu/miniconda3/envs/cudf_dev/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.
  warn(NumbaPerformanceWarning(msg))
文字列列 lo_orderpriority: 直接コピー最適化カーネル実行
✅ 文字列列 lo_orderpriority: 最適化バッファ作成完了 (30000000 bytes)
/home/ubuntu/miniconda3/envs/cudf_dev/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.
  warn(NumbaPerformanceWarning(msg))
文字列列 lo_shippriority: 直接コピー最適化カーネル実行
✅ 文字列列 lo_shippriority: 最適化バッファ作成完了 (2000000 bytes)
/home/ubuntu/miniconda3/envs/cudf_dev/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.
  warn(NumbaPerformanceWarning(msg))
文字列列 lo_commit_date: 直接コピー最適化カーネル実行
✅ 文字列列 lo_commit_date: 最適化バッファ作成完了 (16000000 bytes)
/home/ubuntu/miniconda3/envs/cudf_dev/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.
  warn(NumbaPerformanceWarning(msg))
文字列列 lo_shipmode: 直接コピー最適化カーネル実行
✅ 文字列列 lo_shipmode: 最適化バッファ作成完了 (20000000 bytes)
/home/ubuntu/miniconda3/envs/cudf_dev/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:605: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.
  warn(NumbaPerformanceWarning(msg))
統合カーネル実行（固定長データのみ）: 7813 blocks × 256 threads

=== パフォーマンス統計（文字列最適化版） ===
処理データ: 2,000,000 行 × 17 列
データサイズ: 11040.12 MB

--- 詳細タイミング ---
  gpu_parsing         : 1.0549 秒
  decode_and_export   : 2.4774 秒
    ├─ preparation   : 0.4517 秒
    │  └─ string_opt  : 0.4517 秒
    ├─ gpu_decode      : 1.2774 秒
    └─ cudf_creation : 0.4895 秒
  parquet_export      : 0.2587 秒
  total               : 2.4773 秒
  overall_total       : 3.5323 秒

--- スループット ---
  セル処理速度: 9,625,356 cells/sec
  データ処理速度: 3125.44 MB/sec
  文字列最適化効率: 12.8%
  GPU使用効率: 36.2%
==============================
GPU最適化処理完了 (3.5374秒), 行数: 2000000

ベンチマーク完了: 総時間 = 75.3868 秒
--- 時間内訳 ---
  メタデータ取得: 0.0013 秒
  COPY BINARY   : 70.9564 秒
  GPU転送       : 0.8836 秒
  GPUパース     : 1.0549 秒
  GPUデコード   : 0.4895 秒
  Parquet書き込み: 0.2587 秒
--- 統計情報 ---
  処理行数      : 2,000,000 行
  処理列数      : 17 列
  Decimal列数   : 10 列
  データサイズ  : 11040.12 MB
  スループット  : 69,459,335 cells/sec
----------------

cuDFでParquetファイルを読み込み中: benchmark/lineorder_5m.output.parquet
cuDF読み込み完了 (0.0702秒)
--- cuDF DataFrame Info ---
<class 'cudf.core.dataframe.DataFrame'>
RangeIndex: 2000000 entries, 0 to 1999999
Data columns (total 17 columns):
 #   Column              Dtype
---  ------              -----
 0   lo_orderkey         decimal128
 1   lo_linenumber       int32
 2   lo_custkey          decimal128
 3   lo_partkey          int32
 4   lo_suppkey          decimal128
 5   lo_orderdate        int32
 6   lo_orderpriority    object
 7   lo_shippriority     object
 8   lo_quantity         decimal128
 9   lo_extendedprice    decimal128
 10  lo_ordertotalprice  decimal128
 11  lo_discount         decimal128
 12  lo_revenue          decimal128
 13  lo_supplycost       decimal128
 14  lo_tax              decimal128
 15  lo_commit_date      object
 16  lo_shipmode         object
dtypes: decimal128(10), int32(3), object(4)
memory usage: 423.4+ MB
読み込み結果: 2,000,000 行 × 17 列
データ型:
  lo_orderkey: decimal128
  lo_linenumber: int32
  lo_custkey: decimal128
  lo_partkey: int32
  lo_suppkey: decimal128
  lo_orderdate: int32
  lo_orderpriority: object
  lo_shippriority: object
  lo_quantity: decimal128
  lo_extendedprice: decimal128
  lo_ordertotalprice: decimal128
  lo_discount: decimal128
  lo_revenue: decimal128
  lo_supplycost: decimal128
  lo_tax: decimal128
  lo_commit_date: object
  lo_shipmode: object

--- cuDF DataFrame Head (全列表示) ---
  lo_orderkey  lo_linenumber lo_custkey  lo_partkey lo_suppkey  lo_orderdate lo_orderpriority lo_shippriority lo_quantity lo_extendedprice lo_ordertotalprice lo_discount lo_revenue lo_supplycost lo_tax lo_commit_date lo_shipmode
0   190944645              2    1038160      702608      95778      19960705  1-URGENT                      0          44          7086508            7382264           7    6590452         96634      0       19960912  REG AIR   
1   190944645              3    1038160      731124      69553      19960705  1-URGENT                      0           4           462036            7382264          10     415832         69305      4       19960909  TRUCK     
2   190944646              1     793309      113919      95372      19940702  2-HIGH                        0          33          6378603           15731755           4    6123458        115974      1       19940809  AIR       
3   190944646              2     793309      221000     214681      19940702  2-HIGH                        0          31          2855069           15731755          10    2569562         55259      6       19940909  MAIL      
4   190944646              3     793309      733236     133269      19940702  2-HIGH                        0          27          3426840           15731755           3    3324034         76152      4       19940902  RAIL      

基本統計:
  lo_linenumber: 平均=3.00, 最小=1.00, 最大=7.00
  lo_partkey: 平均=399857.61, 最小=1.00, 最大=800000.00
  lo_orderdate: 平均=19948758.83, 最小=19920101.00, 最大=19980802.00
-------------------------
cuDF検証: 成功



PostgreSQLのCOPY BINARYが全体の94%を占めており、GPU側の高速処理（合計2.7秒）が完全に無駄になっています。

  解決策の優先順位

  1. libpq C API + Cython実装（最優先）
    - PQgetCopyData()で64MB単位の大容量読み込み
    - Pinned Memoryへの直接書き込み
    - 期待効果：10倍以上の高速化
  2. 並列ctid分割
    - 16並列で異なる範囲を同時読み込み
    - 期待効果：10-16倍の高速化
  3. kvikio（ヒープファイル直接読み込み）
    - PostgreSQLを完全にバイパス
    - 期待効果：100倍以上（ただし実装難易度高）

  現在のGPU処理は既に極めて高速なので、ネットワーク転送の改善に注力すべきですね。

# to memorize
  ⎿  Good to know.

> 並列citdのベンチマークはどう実行すればよいでしょうか。

● 並列ctid分割ベンチマークの実行方法を説明します。

  並列ctid分割ベンチマークの実行

  利用可能なスクリプト

  1. benchmark_parallel_ctid_fixed.py （推奨・最新版）
  2. benchmark_parallel_ctid.py （旧版）

  実行コマンド

  # 環境設定
  source /home/ubuntu/miniconda3/etc/profile.d/conda.sh && conda activate cudf_dev
  export PYTHONPATH=$PYTHONPATH:/home/ubuntu/gpupgparser
  export GPUPASER_PG_DSN='dbname=postgres user=postgres host=localhost port=5432'

  # 基本実行（4並列、100万行）
  python benchmark/benchmark_parallel_ctid_fixed.py --rows 1000000 --parallel 4

  # 16並列で200万行
  python benchmark/benchmark_parallel_ctid_fixed.py --rows 2000000 --parallel 16

  # 全データ処理（LIMIT無し）
  python benchmark/benchmark_parallel_ctid_fixed.py --no-limit --parallel 16

  オプション説明

  - --rows: 処理する行数制限
  - --parallel: 並列数（デフォルト4、最大16推奨）
  - --no-limit: 全データ処理（行数制限なし）
  - --bandwidth-test: 帯域テストのみ実行
  - --check-blocks: テーブルブロック数確認

  事前確認コマンド

  # テーブルのブロック数を確認
  python benchmark/benchmark_parallel_ctid_fixed.py --check-blocks

  # GPU Direct Storageサポート確認
  python benchmark/benchmark_parallel_ctid_fixed.py --check-support

  並列処理の仕組み

  1. ctid範囲分割: テーブルを並列数で均等分割
  2. 非同期処理: 各ワーカーが独立してデータ取得
  3. 一時ファイル経由: 各ワーカーが一時ファイルに書き込み
  4. GPU統合: 全データをGPUで結合

  期待される効果

  - 単一接続（155.6 MB/sec）→ 16並列（2.5 GB/sec目標）
  - COPY BINARY時間を1/16に削減
  - 200万行処理: 75秒 → 5秒以下を目標

  実行してみますか？

> ctid処理について速度が伸びていません。処理フロー図を図示してもらえませんか。

● ctid並列処理のフローを図示します。

  現在のctid並列処理フロー図

  ┌─────────────────────────────────────────────────────────────────┐
  │                     PostgreSQL テーブル                          │
  │  ┌─────────┬─────────┬─────────┬─────────┬─────────┬─────────┐│
  │  │Block 0-n│Block n-m│Block m-o│Block o-p│Block p-q│Block q-r││
  │  └─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘│
  └─────────────────────────────────────────────────────────────────┘
                                  │
                      ctid範囲分割（例：16分割）
                                  ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │                      16並列 psycopg3接続                         │
  ├─────────┬─────────┬─────────┬─────────┬─────┬─────────────────┤
  │Worker 0 │Worker 1 │Worker 2 │Worker 3 │ ... │    Worker 15    │
  │ctid 0-n │ctid n-m │ctid m-o │ctid o-p │     │   ctid q-r      │
  └─────────┴─────────┴─────────┴─────────┴─────┴─────────────────┘
       ↓         ↓         ↓         ↓               ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │              問題点1: psycopg3の小さなチャンク                   │
  │  各ワーカー: 8KB単位で受信 → 16MBバッファに蓄積                 │
  │  ⚠️ 非効率: read()回数が多い（数万回）                          │
  └─────────────────────────────────────────────────────────────────┘
       ↓         ↓         ↓         ↓               ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │           問題点2: ディスクI/O（一時ファイル）                   │
  ├─────────┬─────────┬─────────┬─────────┬─────┬─────────────────┤
  │ /tmp/   │ /tmp/   │ /tmp/   │ /tmp/   │     │    /tmp/        │
  │file_0   │file_1   │file_2   │file_3   │ ... │    file_15      │
  └─────────┴─────────┴─────────┴─────────┴─────┴─────────────────┘
       ↓         ↓         ↓         ↓               ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │              問題点3: 順次GPU転送                                │
  │  各ファイル → GPUバッファ（順番に転送）                          │
  │  ⚠️ 並列性が失われる                                            │
  └─────────────────────────────────────────────────────────────────┘
                                  ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │                        GPU統合バッファ                           │
  │                    （全データを結合）                            │
  └─────────────────────────────────────────────────────────────────┘
                                  ↓
                            GPU処理（高速）

  ボトルネック分析

  1. psycopg3の制限

  PostgreSQL → [8KB] → psycopg3内部バッファ → [8KB] → Pythonオブジェクト
                  ↑                                ↑
              小さすぎる                    多数のシステムコール

  2. 一時ファイルI/O

  メモリ → ディスク書き込み → ディスク読み込み → GPU
        ↑                                    ↑
     不要なI/O                          追加レイテンシ

  3. 並列性の喪失

  16並列読み込み → 順次ファイル書き込み → 順次GPU転送
              ↑                        ↑
          待ち時間発生              並列性なし

  理想的なフロー（libpq C API + Cython）

  ┌─────────────────────────────────────────────────────────────────┐
  │                     PostgreSQL テーブル                          │
  └─────────────────────────────────────────────────────────────────┘
                                  │
                      ctid範囲分割（16分割）
                                  ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │              16並列 libpq直接接続（Cython）                      │
  ├─────────┬─────────┬─────────┬─────────┬─────┬─────────────────┤
  │Worker 0 │Worker 1 │Worker 2 │Worker 3 │ ... │    Worker 15    │
  │64MB単位 │64MB単位 │64MB単位 │64MB単位 │     │   64MB単位      │
  └─────────┴─────────┴─────────┴─────────┴─────┴─────────────────┘
       ↓         ↓         ↓         ↓               ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │                  Pinned Memory リングバッファ                    │
  │  各ワーカー専用: 64MB×2（ダブルバッファ）                       │
  │  ✅ ゼロコピー、✅ DMA転送準備完了                              │
  └─────────────────────────────────────────────────────────────────┘
       ↓         ↓         ↓         ↓               ↓
  ┌─────────────────────────────────────────────────────────────────┐
  │              非同期GPU転送（cudaMemcpyAsync）                    │
  │  16ストリーム並列転送 → GPU統合バッファ                         │
  │  ✅ 真の並列転送、✅ CPU非ブロッキング                          │
  └─────────────────────────────────────────────────────────────────┘

  この改善により、COPY BINARYの時間を70秒から5秒以下に短縮できる見込みです。

  50,000,000