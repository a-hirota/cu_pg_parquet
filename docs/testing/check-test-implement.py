#!/usr/bin/env python3
"""
Check Test Implementation Tool
ãƒ†ã‚¹ãƒˆå®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«

ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. testPlan.mdã®å†…å®¹ã‚’è§£æ
2. å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨æ¯”è¼ƒ
3. ç¨¼åƒãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ­£ã—ããƒ†ã‚¹ãƒˆã—ã¦ã„ã‚‹ã‹ãƒ¬ãƒ“ãƒ¥ãƒ¼
4. ãƒ†ã‚¹ãƒˆå®Ÿè£…çŠ¶æ³ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
"""

import ast
import json
import os
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¸ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class TestImplementationChecker:
    """ãƒ†ã‚¹ãƒˆå®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«"""

    def __init__(self):
        self.project_root = project_root
        self.test_plan_path = self.project_root / "docs/testing/testPlan.md"
        self.test_dirs = ["tests"]
        self.planned_tests = {}
        self.implemented_tests = {}
        self.test_quality = {}

    def parse_test_plan(self):
        """testPlan.mdã‚’è§£æ"""
        print("ğŸ“– Parsing test plan...")

        if not self.test_plan_path.exists():
            print(f"  âŒ Test plan not found: {self.test_plan_path}")
            return

        with open(self.test_plan_path, "r", encoding="utf-8") as f:
            content = f.read()

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚’æŠ½å‡º
        functions = re.findall(r"### (F\d+): ([^\n]+)", content)
        for func_id, func_name in functions:
            self.planned_tests[func_id] = {
                "name": func_name,
                "e2e_test": None,
                "unit_tests": [],
                "integration_tests": [],
            }

        # ãƒ‡ãƒ¼ã‚¿å‹ãƒ†ã‚¹ãƒˆã‚’æŠ½å‡º
        type_section = re.search(r"## Supported Data Types.*?(?=##|\Z)", content, re.DOTALL)
        if type_section:
            type_rows = re.findall(
                r"\| (\w+)\s*\| (\d+)\s*\| (\w+)\s*\| ([^|]+)\|", type_section.group()
            )
            self.planned_tests["data_types"] = [
                {"pg_type": row[0], "oid": row[1], "arrow_type": row[2], "status": row[3].strip()}
                for row in type_rows
                if row[0] != "PostgreSQL Type"
            ]

    def analyze_test_files(self):
        """å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
        print("\nğŸ” Analyzing test implementation...")

        # E2Eãƒ†ã‚¹ãƒˆ
        e2e_dir = self.project_root / "tests/e2e"
        if e2e_dir.exists():
            for test_file in e2e_dir.glob("test_*.py"):
                self._analyze_test_file(test_file, "e2e")

        # å‹ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ãƒ†ã‚¹ãƒˆ
        type_matrix = self.project_root / "tests/test_type_matrix.py"
        if type_matrix.exists():
            self._analyze_test_file(type_matrix, "type_matrix")

        # çµ±åˆãƒ†ã‚¹ãƒˆ
        integration_dir = self.project_root / "tests/integration"
        if integration_dir.exists():
            for test_file in integration_dir.glob("test_*.py"):
                self._analyze_test_file(test_file, "integration")

    def _analyze_test_file(self, file_path, test_type):
        """å€‹åˆ¥ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            rel_path = file_path.relative_to(self.project_root)
            file_info = {
                "path": str(rel_path),
                "type": test_type,
                "imports": self._extract_imports(content),
                "test_classes": [],
                "test_functions": [],
                "uses_production_code": False,
                "uses_mocks": False,
                "coverage": [],
            }

            # ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ã‚’æŠ½å‡º
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef) and node.name.startswith("Test"):
                        test_methods = [
                            n.name
                            for n in node.body
                            if isinstance(n, ast.FunctionDef) and n.name.startswith("test_")
                        ]
                        file_info["test_classes"].append(
                            {"name": node.name, "methods": test_methods}
                        )
                    elif isinstance(node, ast.FunctionDef) and node.name.startswith("test_"):
                        file_info["test_functions"].append(node.name)
            except:
                pass

            # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã®ä½¿ç”¨ã‚’ãƒã‚§ãƒƒã‚¯
            production_imports = [
                "src.",
                "processors.",
                "rust",
                "cuda_kernels",
                "DirectProcessor",
                "parse_postgres_raw_binary",
                "write_parquet_from_cudf",
                "ColumnMeta",
            ]
            for imp in production_imports:
                if imp in content:
                    file_info["uses_production_code"] = True
                    break

            # ãƒ¢ãƒƒã‚¯ã®ä½¿ç”¨ã‚’ãƒã‚§ãƒƒã‚¯
            mock_indicators = ["mock", "Mock", "MagicMock", "patch", "@patch"]
            for mock in mock_indicators:
                if mock in content:
                    file_info["uses_mocks"] = True
                    break

            # ãƒ†ã‚¹ãƒˆå†…å®¹ã®å“è³ªãƒã‚§ãƒƒã‚¯
            quality_checks = {
                "has_assertions": bool(re.search(r"assert\s+", content)),
                "tests_edge_cases": bool(
                    re.search(r"(None|null|empty|boundary|edge|max|min)", content, re.I)
                ),
                "tests_errors": bool(re.search(r"(raises|except|error|fail)", content, re.I)),
                "has_setup_teardown": bool(
                    re.search(r"(setUp|tearDown|setup_|teardown_|fixture)", content)
                ),
                "uses_gpu": bool(re.search(r"(cuda|gpu|cudf|cupy)", content, re.I)),
                "tests_performance": bool(
                    re.search(r"(benchmark|performance|throughput|speed)", content, re.I)
                ),
            }
            file_info["quality_checks"] = quality_checks

            self.implemented_tests[str(rel_path)] = file_info

        except Exception as e:
            print(f"  âš ï¸  Error analyzing {file_path}: {e}")

    def _extract_imports(self, content):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æŠ½å‡º"""
        imports = []
        import_lines = re.findall(r"^(from .+ import .+|import .+)$", content, re.MULTILINE)
        for line in import_lines[:10]:  # æœ€åˆã®10å€‹ã®ã¿
            imports.append(line.strip())
        return imports

    def check_test_coverage(self):
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("\nğŸ“Š Checking test coverage...")

        # æ©Ÿèƒ½ã”ã¨ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
        self.test_quality["function_coverage"] = {
            "F1": self._check_function_coverage("postgres_to_binary"),
            "F2": self._check_function_coverage("binary_to_arrow"),
            "F3": self._check_function_coverage("arrow_to_parquet"),
        }

        # ãƒ‡ãƒ¼ã‚¿å‹ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
        type_coverage = self._check_type_coverage()
        self.test_quality["type_coverage"] = type_coverage

        # å…¨ä½“çš„ãªå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        all_tests = list(self.implemented_tests.values())
        self.test_quality["overall"] = {
            "total_test_files": len(all_tests),
            "using_production_code": sum(1 for t in all_tests if t["uses_production_code"]),
            "using_mocks": sum(1 for t in all_tests if t["uses_mocks"]),
            "with_assertions": sum(
                1 for t in all_tests if t.get("quality_checks", {}).get("has_assertions")
            ),
            "testing_edge_cases": sum(
                1 for t in all_tests if t.get("quality_checks", {}).get("tests_edge_cases")
            ),
            "testing_errors": sum(
                1 for t in all_tests if t.get("quality_checks", {}).get("tests_errors")
            ),
            "using_gpu": sum(1 for t in all_tests if t.get("quality_checks", {}).get("uses_gpu")),
        }

    def _check_function_coverage(self, function_name):
        """ç‰¹å®šã®æ©Ÿèƒ½ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯"""
        coverage = {
            "e2e_test": False,
            "unit_tests": False,
            "integration_tests": False,
            "uses_real_code": False,
            "test_files": [],
        }

        for path, info in self.implemented_tests.items():
            if function_name.lower() in path.lower():
                coverage["test_files"].append(path)
                if info["type"] == "e2e":
                    coverage["e2e_test"] = True
                if info["uses_production_code"]:
                    coverage["uses_real_code"] = True

        return coverage

    def _check_type_coverage(self):
        """ãƒ‡ãƒ¼ã‚¿å‹ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯"""
        type_matrix_file = "tests/test_type_matrix.py"
        coverage = {"has_type_matrix": False, "tested_types": [], "coverage_percentage": 0}

        if type_matrix_file in self.implemented_tests:
            coverage["has_type_matrix"] = True
            test_info = self.implemented_tests[type_matrix_file]

            # test_type_matrix.pyã®å†…å®¹ã‚’ç¢ºèª
            matrix_path = self.project_root / type_matrix_file
            if matrix_path.exists():
                with open(matrix_path, "r") as f:
                    content = f.read()

                # POSTGRES_TYPESã‹ã‚‰å‹ã‚’æŠ½å‡º
                types_match = re.search(r"POSTGRES_TYPES\s*=\s*{([^}]+)}", content, re.DOTALL)
                if types_match:
                    type_definitions = re.findall(r"(\d+):\s*\(\"(\w+)\"", types_match.group(1))
                    coverage["tested_types"] = [
                        {"oid": oid, "type": typename} for oid, typename in type_definitions
                    ]

                if self.planned_tests.get("data_types"):
                    planned_count = len(self.planned_tests["data_types"])
                    tested_count = len(coverage["tested_types"])
                    coverage["coverage_percentage"] = (
                        (tested_count / planned_count * 100) if planned_count > 0 else 0
                    )

        return coverage

    def generate_report(self):
        """ãƒ†ã‚¹ãƒˆå®Ÿè£…ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\nğŸ“ Generating implementation report...")

        report = f"""# Test Implementation Check Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary

This report analyzes the test implementation against the test plan and verifies that production code is being properly tested.

## Overall Statistics

- **Total Test Files**: {self.test_quality['overall']['total_test_files']}
- **Tests Using Production Code**: {self.test_quality['overall']['using_production_code']} ({self.test_quality['overall']['using_production_code'] / self.test_quality['overall']['total_test_files'] * 100:.1f}%)
- **Tests Using Mocks**: {self.test_quality['overall']['using_mocks']} ({self.test_quality['overall']['using_mocks'] / self.test_quality['overall']['total_test_files'] * 100:.1f}%)
- **Tests with Assertions**: {self.test_quality['overall']['with_assertions']}
- **Tests Checking Edge Cases**: {self.test_quality['overall']['testing_edge_cases']}
- **Tests Using GPU**: {self.test_quality['overall']['using_gpu']}

## Function Coverage Analysis

"""
        # æ©Ÿèƒ½åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸
        for func_id, coverage in self.test_quality["function_coverage"].items():
            func_name = self.planned_tests.get(func_id, {}).get("name", func_id)
            report += f"### {func_id}: {func_name}\n\n"

            status_e2e = "âœ…" if coverage["e2e_test"] else "âŒ"
            status_real = "âœ…" if coverage["uses_real_code"] else "âŒ"

            report += f"- **E2E Test**: {status_e2e}\n"
            report += f"- **Uses Real Code**: {status_real}\n"
            report += f"- **Test Files**: {len(coverage['test_files'])}\n"

            if coverage["test_files"]:
                report += "  - " + "\n  - ".join(coverage["test_files"]) + "\n"
            report += "\n"

        # ãƒ‡ãƒ¼ã‚¿å‹ã‚«ãƒãƒ¬ãƒƒã‚¸
        type_cov = self.test_quality["type_coverage"]
        report += "## Data Type Coverage\n\n"
        report += f"- **Type Matrix Test**: {'âœ… Implemented' if type_cov['has_type_matrix'] else 'âŒ Not Found'}\n"
        report += f"- **Tested Types**: {len(type_cov['tested_types'])}\n"
        report += f"- **Coverage**: {type_cov['coverage_percentage']:.1f}%\n\n"

        if type_cov["tested_types"]:
            report += "### Tested Data Types\n\n"
            report += "| OID | Type Name |\n"
            report += "|-----|----------|\n"
            for t in sorted(type_cov["tested_types"], key=lambda x: int(x["oid"])):
                report += f"| {t['oid']} | {t['type']} |\n"
            report += "\n"

        # è©³ç´°ãªå®Ÿè£…çŠ¶æ³
        report += "## Detailed Test Implementation\n\n"

        # å®Ÿãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ†ã‚¹ãƒˆ
        report += "### Tests Using Production Code âœ…\n\n"
        real_tests = [
            (path, info)
            for path, info in self.implemented_tests.items()
            if info["uses_production_code"]
        ]
        for path, info in sorted(real_tests):
            report += f"#### {path}\n"
            report += f"- Type: {info['type']}\n"
            if info["test_classes"]:
                report += f"- Classes: {len(info['test_classes'])}\n"
                for cls in info["test_classes"][:2]:  # æœ€åˆã®2ã‚¯ãƒ©ã‚¹ã®ã¿
                    report += f"  - {cls['name']} ({len(cls['methods'])} methods)\n"
            quality = info.get("quality_checks", {})
            if quality.get("tests_edge_cases"):
                report += "- âœ… Tests edge cases\n"
            if quality.get("tests_errors"):
                report += "- âœ… Tests error handling\n"
            if quality.get("uses_gpu"):
                report += "- âœ… GPU testing\n"
            report += "\n"

        # ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ†ã‚¹ãƒˆ
        report += "### Tests Using Mocks âš ï¸\n\n"
        mock_tests = [
            (path, info)
            for path, info in self.implemented_tests.items()
            if info["uses_mocks"] and not info["uses_production_code"]
        ]
        if mock_tests:
            report += "These tests may not be testing real production behavior:\n\n"
            for path, info in sorted(mock_tests):
                report += f"- {path}\n"
        else:
            report += "No tests found that only use mocks without production code.\n"
        report += "\n"

        # æ¨å¥¨äº‹é …
        report += "## Recommendations\n\n"

        recommendations = []

        # E2Eãƒ†ã‚¹ãƒˆã®æ¨å¥¨
        for func_id, coverage in self.test_quality["function_coverage"].items():
            if not coverage["e2e_test"]:
                func_name = self.planned_tests.get(func_id, {}).get("name", func_id)
                recommendations.append(f"- Add E2E test for {func_id}: {func_name}")

        # å®Ÿã‚³ãƒ¼ãƒ‰ä½¿ç”¨ã®æ¨å¥¨
        mock_only_count = len(
            [
                t
                for t in self.implemented_tests.values()
                if t["uses_mocks"] and not t["uses_production_code"]
            ]
        )
        if mock_only_count > 0:
            recommendations.append(
                f"- Convert {mock_only_count} mock-only tests to use production code"
            )

        # GPU ãƒ†ã‚¹ãƒˆã®æ¨å¥¨
        gpu_test_count = self.test_quality["overall"]["using_gpu"]
        if gpu_test_count < 5:
            recommendations.append("- Add more GPU-specific tests for CUDA kernel validation")

        # ãƒ‡ãƒ¼ã‚¿å‹ã‚«ãƒãƒ¬ãƒƒã‚¸ã®æ¨å¥¨
        if type_cov["coverage_percentage"] < 80:
            recommendations.append(
                f"- Increase data type test coverage (currently {type_cov['coverage_percentage']:.1f}%)"
            )

        if recommendations:
            for rec in recommendations:
                report += rec + "\n"
        else:
            report += "âœ… Test implementation looks comprehensive!\n"

        return report

    def save_report(self, report):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        report_path = self.project_root / "docs/testing/test-implementation-report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"  âœ… Report saved to: {report_path}")

        # JSONå½¢å¼ã§ã‚‚ä¿å­˜
        json_path = self.project_root / "docs/testing/test-implementation-data.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "timestamp": datetime.now().isoformat(),
                    "quality": self.test_quality,
                    "implemented_tests": self.implemented_tests,
                },
                f,
                indent=2,
            )
        print(f"  ğŸ“Š Data saved to: {json_path}")

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸ” Test Implementation Checker")
        print("=" * 60)

        # ãƒ†ã‚¹ãƒˆè¨ˆç”»ã‚’è§£æ
        self.parse_test_plan()
        print(f"  Found {len(self.planned_tests)} planned test categories")

        # å®Ÿè£…ã‚’è§£æ
        self.analyze_test_files()
        print(f"  Analyzed {len(self.implemented_tests)} test files")

        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
        self.check_test_coverage()

        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        report = self.generate_report()

        # ä¿å­˜
        self.save_report(report)

        print("\nâœ¨ Test implementation check completed!")

        # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\nğŸ“Š Quick Summary:")
        print(
            f"  - Production code usage: {self.test_quality['overall']['using_production_code']}/{self.test_quality['overall']['total_test_files']} tests"
        )
        print(
            f"  - Type coverage: {self.test_quality['type_coverage']['coverage_percentage']:.1f}%"
        )
        print(f"  - GPU tests: {self.test_quality['overall']['using_gpu']} files")


if __name__ == "__main__":
    checker = TestImplementationChecker()
    checker.run()
