"""
PostgreSQL â†’ GPU Rayä¸¦åˆ— é †æ¬¡GPUå‡¦ç†ç‰ˆï¼ˆãƒãƒƒãƒåŒ–GPUè»¢é€ï¼‰
psycopg3ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒåŒ–ã—ã¦GPUè»¢é€å›æ•°ã‚’å‰Šæ¸›

æœ€é©åŒ–:
- psycopg3ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸€å®šã‚µã‚¤ã‚ºã”ã¨ã«ãƒãƒƒãƒåŒ–
- ãƒãƒƒãƒå˜ä½ã§GPUè»¢é€ï¼ˆè»¢é€å›æ•°ã‚’å¤§å¹…å‰Šæ¸›ï¼‰
- ä¸­é–“ãƒãƒƒãƒ•ã‚¡ã¯æœ€å°é™ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†ã®ã¿ï¼‰
- è©³ç´°ãªãƒ¡ãƒ¢ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²

ç’°å¢ƒå¤‰æ•°:
GPUPASER_PG_DSN  : PostgreSQLæ¥ç¶šæ–‡å­—åˆ—
"""

import os
import time
import math
import psycopg
import ray
import gc
import numpy as np
from numba import cuda
import argparse
from typing import List, Dict, Tuple, Optional
import psutil
import json
import cupy as cp

from src.metadata import fetch_column_meta
from src.cuda_kernels.postgresql_binary_parser import detect_pg_header_size
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet

TABLE_NAME = "lineorder"
OUTPUT_PARQUET_PATH = "benchmark/lineorder_parallel_ctid_ray_sequential_gpu_batch.output"

# ä¸¦åˆ—è¨­å®š
DEFAULT_PARALLEL = 16
CHUNK_COUNT = 4

# ãƒãƒƒãƒè¨­å®š
DEFAULT_BATCH_SIZE_MB = 64  # 64MBã”ã¨ã«GPUè»¢é€

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ç”¨
class DetailedMetrics:
    def __init__(self, is_worker=False):
        self.is_worker = is_worker  # Ray Workerå†…ã‹ã©ã†ã‹
        self.metrics = {
            'memory': {
                'initial_system_mb': 0,
                'peak_system_mb': 0,
                'initial_gpu_mb': 0,
                'peak_gpu_mb': 0,
                'chunk_count': 0,
                'avg_chunk_size': 0,
                'total_data_size_mb': 0,
                'memory_overhead_ratio': 0,
                'batch_count': 0,
                'avg_batch_size_mb': 0
            },
            'performance': {
                'copy_time_sec': 0,
                'gpu_transfer_time_sec': 0,
                'gpu_transfer_count': 0,
                'gpu_processing_time_sec': 0,
                'total_time_sec': 0,
                'throughput_mb_sec': 0
            },
            'details': []
        }
    
    def log_memory_snapshot(self, stage: str):
        import psutil
        
        process = psutil.Process()
        system_memory_mb = process.memory_info().rss / (1024**2)
        
        # Ray Workerå†…ã§ã¯GPUãƒ¡ãƒ¢ãƒªã‚’å–å¾—ã—ãªã„
        gpu_memory_mb = 0
        if not self.is_worker:
            try:
                mempool = cp.get_default_memory_pool()
                gpu_memory_mb = mempool.used_bytes() / (1024**2)
            except:
                gpu_memory_mb = 0
        
        self.metrics['details'].append({
            'timestamp': time.time(),
            'stage': stage,
            'system_memory_mb': system_memory_mb,
            'gpu_memory_mb': gpu_memory_mb
        })
        
        # ãƒ”ãƒ¼ã‚¯å€¤ã‚’æ›´æ–°
        if system_memory_mb > self.metrics['memory']['peak_system_mb']:
            self.metrics['memory']['peak_system_mb'] = system_memory_mb
        if gpu_memory_mb > self.metrics['memory']['peak_gpu_mb']:
            self.metrics['memory']['peak_gpu_mb'] = gpu_memory_mb
    
    def save_to_json(self, filename: str):
        with open(filename, 'w') as f:
            json.dump(self.metrics, f, indent=2)

@ray.remote  # CPUã®ã¿ã€GPUã¯ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ä½¿ç”¨
class PostgreSQLWorker:
    """Rayä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: ãƒãƒƒãƒåŒ–GPUè»¢é€ç‰ˆ"""
    
    def __init__(self, worker_id: int, dsn: str):
        self.worker_id = worker_id
        self.dsn = dsn
        
    def copy_data_to_memory_batch(
        self, 
        table_name: str,
        start_block: int, 
        end_block: int,
        chunk_idx: int,
        total_chunks: int,
        limit_rows: Optional[int] = None,
        batch_size_mb: int = DEFAULT_BATCH_SIZE_MB
    ) -> Dict[str, any]:
        """PostgreSQLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’COPYã—ã¦ãƒãƒƒãƒåŒ–GPUè»¢é€"""
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆæœŸåŒ–
        metrics = DetailedMetrics(is_worker=True)  # Ray Workerå†…ãªã®ã§True
        process = psutil.Process()
        initial_memory = process.memory_info().rss / (1024**2)  # MB
        metrics.metrics['memory']['initial_system_mb'] = initial_memory
        metrics.log_memory_snapshot("start")
        
        # ãƒãƒ£ãƒ³ã‚¯ã«å¿œã˜ã¦ctidç¯„å›²ã‚’åˆ†å‰²
        block_range = end_block - start_block
        chunk_block_size = block_range // total_chunks
        
        chunk_start_block = start_block + (chunk_idx * chunk_block_size)
        chunk_end_block = start_block + ((chunk_idx + 1) * chunk_block_size) if chunk_idx < total_chunks - 1 else end_block
        
        print(f"Worker {self.worker_id}-Chunk{chunk_idx}: COPYé–‹å§‹ ctidç¯„å›² ({chunk_start_block},{chunk_end_block})...")
        print(f"  åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory:.2f} MB")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size_mb} MB")
        
        start_time = time.time()
        
        try:
            # PostgreSQLæ¥ç¶š
            conn = psycopg.connect(self.dsn)
            
            # ãƒãƒƒãƒåŒ–ç”¨ã®å¤‰æ•°
            batch_buffer = bytearray()
            chunk_count = 0
            total_size = 0
            chunk_sizes = []
            batch_count = 0
            batch_sizes = []
            
            try:
                # COPY SQLç”Ÿæˆ
                copy_sql = self._make_copy_sql(table_name, chunk_start_block, chunk_end_block, limit_rows)
                
                # ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨ãƒãƒƒãƒåŒ–GPUè»¢é€
                with conn.cursor() as cur:
                    with cur.copy(copy_sql) as copy_obj:
                        print(f"  ãƒãƒƒãƒåŒ–ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¢ãƒ¼ãƒ‰é–‹å§‹...")
                        
                        for chunk in copy_obj:
                            if chunk:
                                chunk_count += 1
                                chunk_size = len(chunk)
                                chunk_sizes.append(chunk_size)
                                total_size += chunk_size
                                
                                # ãƒãƒƒãƒãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                                if isinstance(chunk, memoryview):
                                    batch_buffer.extend(chunk)
                                else:
                                    batch_buffer.extend(chunk)
                                
                                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¶…ãˆãŸå ´åˆã®ãƒ­ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªç®¡ç†ã®ãŸã‚ï¼‰
                                if len(batch_buffer) >= batch_size_mb * 1024 * 1024:
                                    batch_count += 1
                                    batch_size_actual = len(batch_buffer)
                                    batch_sizes.append(batch_size_actual / (1024*1024))
                                    
                                    # ãƒ¡ãƒ¢ãƒªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                                    if batch_count % 100 == 0:
                                        metrics.log_memory_snapshot(f"batch_{batch_count}")
                                
                                # æœ€åˆã®10ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º
                                if chunk_count <= 10:
                                    print(f"    ãƒãƒ£ãƒ³ã‚¯{chunk_count}: {chunk_size} bytes")
                
                # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
                if batch_buffer:
                    batch_count += 1
                    batch_size_actual = len(batch_buffer)
                    batch_sizes.append(batch_size_actual / (1024*1024))
                    
                    print(f"    æœ€çµ‚ãƒãƒƒãƒ{batch_count}: {batch_size_actual/(1024*1024):.1f} MB")
                
                # CPUãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’æœ€çµ‚å½¢å¼ã«å¤‰æ›
                final_data = bytes(batch_buffer)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                metrics.log_memory_snapshot("after_concat")
                copy_time = time.time() - start_time
                
                # çµ±è¨ˆè¨ˆç®—
                if chunk_sizes:
                    avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes)
                else:
                    avg_chunk_size = 0
                
                if batch_sizes:
                    avg_batch_size = sum(batch_sizes) / len(batch_sizes)
                else:
                    avg_batch_size = 0
                
                metrics.metrics['memory']['chunk_count'] = chunk_count
                metrics.metrics['memory']['avg_chunk_size'] = avg_chunk_size
                metrics.metrics['memory']['batch_count'] = batch_count
                metrics.metrics['memory']['avg_batch_size_mb'] = avg_batch_size
                metrics.metrics['memory']['total_data_size_mb'] = total_size / (1024**2)
                metrics.metrics['performance']['copy_time_sec'] = copy_time
                
                # ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆç®—
                current_memory = process.memory_info().rss / (1024**2)
                memory_used = current_memory - initial_memory
                if total_size > 0:
                    overhead_ratio = memory_used / (total_size / (1024**2))
                else:
                    overhead_ratio = 1.0
                metrics.metrics['memory']['memory_overhead_ratio'] = overhead_ratio
                
                print(f"Worker {self.worker_id}-Chunk{chunk_idx}: COPYå®Œäº†")
                print(f"  ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
                print(f"  å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {avg_chunk_size:.0f} bytes")
                print(f"  ãƒãƒƒãƒæ•°: {batch_count}")
                print(f"  å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚º: {avg_batch_size:.1f} MB")
                print(f"  ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_size/(1024*1024):.1f} MB")
                print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {current_memory:.2f} MB (å¢—åŠ : +{memory_used:.2f} MB)")
                print(f"  ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ç‡: {overhead_ratio:.2f}x")
                
                return {
                    'worker_id': self.worker_id,
                    'chunk_idx': chunk_idx,
                    'data': final_data,
                    'data_size': len(final_data),
                    'copy_time': copy_time,
                    'metrics': metrics.metrics,
                    'status': 'success'
                }
                
            finally:
                conn.close()
                # ãƒ¡ãƒ¢ãƒªè§£æ”¾
                del batch_buffer
                gc.collect()
                
        except Exception as e:
            print(f"Worker {self.worker_id}-Chunk{chunk_idx}: âŒCOPYå¤±æ•— - {e}")
            return {
                'worker_id': self.worker_id,
                'chunk_idx': chunk_idx,
                'status': 'error',
                'error': str(e),
                'copy_time': time.time() - start_time,
                'metrics': metrics.metrics
            }
    
    def _make_copy_sql(self, table_name: str, start_block: int, end_block: int, limit_rows: Optional[int]) -> str:
        """COPY SQLç”Ÿæˆ"""
        sql = f"""
        COPY (
            SELECT * FROM {table_name}
            WHERE ctid >= '({start_block},1)'::tid
              AND ctid < '({end_block+1},1)'::tid
        ) TO STDOUT (FORMAT binary)"""
        
        return sql


def process_batch_on_gpu(
    batch_tasks: List[Dict],
    columns: List,
    gds_supported: bool,
    output_base: str,
    true_batch_mode: bool = False
) -> List[Dict]:
    """è¤‡æ•°ã®ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã‚’çµåˆã—ã¦1å›ã®GPUå‡¦ç†ã§å®Ÿè¡Œ"""
    
    import cupy as cp
    
    num_tasks = len(batch_tasks)
    print(f"\nğŸ“Š ãƒãƒƒãƒGPUå‡¦ç†é–‹å§‹: {num_tasks}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’çµ±åˆå‡¦ç†")
    
    results = []
    start_total_time = time.time()
    
    try:
        # 1. å…¨ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        total_size = sum(task['data_size'] for task in batch_tasks)
        print(f"  çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_size/(1024*1024):.2f} MB")
        
        # 2. GPUé…åˆ—ã‚’ç›´æ¥ä½œæˆ
        start_gpu_transfer_time = time.time()
        gpu_array = cp.empty(total_size, dtype=cp.uint8)
        
        # 3. å„ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥GPUé…åˆ—ã«ã‚³ãƒ”ãƒ¼
        task_offsets = []
        current_offset = 0
        
        for task in batch_tasks:
            data_len = task['data_size']
            gpu_array[current_offset:current_offset + data_len] = cp.frombuffer(task['data'], dtype=cp.uint8)
            
            task_offsets.append({
                'worker_id': task['worker_id'],
                'chunk_idx': task['chunk_idx'],
                'offset': current_offset,
                'size': task['data_size']
            })
            current_offset += task['data_size']
        
        gpu_transfer_time = time.time() - start_gpu_transfer_time
        print(f"  GPUè»¢é€å®Œäº† ({gpu_transfer_time:.2f}ç§’, {(total_size/(1024*1024))/gpu_transfer_time:.2f} MB/sec)")
        
        # ä»¥ä¸‹ã€æ—¢å­˜ã®GPUå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        start_gpu_processing_time = time.time()
        
        if true_batch_mode:
            # çœŸã®ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
            print(f"\n  ğŸš€ çœŸã®ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {num_tasks}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’1å›ã®GPUå‡¦ç†ã§å®Ÿè¡Œ")
            
            raw_dev = cuda.as_cuda_array(gpu_array)
            header_sample = gpu_array[:min(128, len(gpu_array))].get()
            header_size = detect_pg_header_size(header_sample)
            
            output_path = f"{output_base}_batch_integrated.parquet"
            
            try:
                cudf_df, detailed_timing = postgresql_to_cudf_parquet(
                    raw_dev=raw_dev,
                    columns=columns,
                    ncols=len(columns),
                    header_size=header_size,
                    output_path=output_path,
                    compression='snappy',
                    use_rmm=False,
                    optimize_gpu=True
                )
                
                print(f"  âœ… ãƒãƒƒãƒå‡¦ç†æˆåŠŸ: {len(cudf_df):,} è¡Œå‡¦ç†æ¸ˆã¿")
                
                results.append({
                    'worker_id': 'batch',
                    'chunk_idx': 0,
                    'gpu_transfer_time': gpu_transfer_time,
                    'gpu_processing_time': detailed_timing.get('overall_total', 0),
                    'rows_processed': len(cudf_df),
                    'status': 'success'
                })
                
            except Exception as e:
                print(f"  âŒ ãƒãƒƒãƒå‡¦ç†å¤±æ•—: {e}")
                true_batch_mode = False
        
        if not true_batch_mode:
            # å€‹åˆ¥å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
            for i, (task, offset_info) in enumerate(zip(batch_tasks, task_offsets)):
                print(f"\n  å‡¦ç†ä¸­ [{i+1}/{num_tasks}]: Worker{offset_info['worker_id']}-Chunk{offset_info['chunk_idx']}")
                
                dataset_gpu = gpu_array[offset_info['offset']:offset_info['offset'] + offset_info['size']]
                raw_dev = cuda.as_cuda_array(dataset_gpu)
                header_sample = dataset_gpu[:min(128, len(dataset_gpu))].get()
                header_size = detect_pg_header_size(header_sample)
                
                output_path = f"{output_base}_worker{offset_info['worker_id']}_chunk{offset_info['chunk_idx']}.parquet"
                
                cudf_df, detailed_timing = postgresql_to_cudf_parquet(
                    raw_dev=raw_dev,
                    columns=columns,
                    ncols=len(columns),
                    header_size=header_size,
                    output_path=output_path,
                    compression='snappy',
                    use_rmm=False,
                    optimize_gpu=True
                )
                
                results.append({
                    'worker_id': offset_info['worker_id'],
                    'chunk_idx': offset_info['chunk_idx'],
                    'gpu_transfer_time': gpu_transfer_time / num_tasks,
                    'gpu_processing_time': detailed_timing.get('overall_total', 0),
                    'rows_processed': len(cudf_df),
                    'status': 'success'
                })
        
        gpu_processing_time = time.time() - start_gpu_processing_time
        total_time = time.time() - start_total_time
        
        print(f"\nâœ… ãƒãƒƒãƒGPUå‡¦ç†å®Œäº†:")
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        print(f"  GPUè»¢é€: {gpu_transfer_time:.2f}ç§’")
        print(f"  GPUå‡¦ç†: {gpu_processing_time:.2f}ç§’")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {(total_size/(1024*1024))/total_time:.2f} MB/sec")
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del gpu_array
        gc.collect()
        cp.get_default_memory_pool().free_all_blocks()
        cuda.synchronize()
        
        return results
        
    except Exception as e:
        print(f"  âŒãƒãƒƒãƒGPUå‡¦ç†å¤±æ•—: {e}")
        for task in batch_tasks:
            results.append({
                'worker_id': task['worker_id'],
                'chunk_idx': task['chunk_idx'],
                'status': 'error',
                'error': str(e)
            })
        
        try:
            gc.collect()
            cp.get_default_memory_pool().free_all_blocks()
            cuda.synchronize()
        except:
            pass
            
        return results


def get_table_blocks(dsn: str, table_name: str) -> int:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç·ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’å–å¾—"""
    conn = psycopg.connect(dsn)
    try:
        with conn.cursor() as cur:
            cur.execute(f"SELECT pg_relation_size('{table_name}') / 8192 AS blocks")
            blocks = cur.fetchone()[0]
            return int(blocks)
    finally:
        conn.close()


def make_ctid_ranges(total_blocks: int, parallel_count: int) -> List[Tuple[int, int]]:
    """ctidç¯„å›²ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    chunk_size = math.ceil(total_blocks / parallel_count)
    ranges = []
    
    for i in range(parallel_count):
        start_block = i * chunk_size
        end_block = min((i + 1) * chunk_size, total_blocks)
        if start_block < total_blocks:
            ranges.append((start_block, end_block))
    
    return ranges


def check_gpu_direct_support() -> bool:
    """GPU Direct ã‚µãƒãƒ¼ãƒˆç¢ºèª"""
    print("\n=== GPU Direct ã‚µãƒãƒ¼ãƒˆç¢ºèª ===")
    
    try:
        if os.path.exists("/proc/driver/nvidia-fs/stats"):
            print("âœ… nvidia-fs ãƒ‰ãƒ©ã‚¤ãƒæ¤œå‡º")
        else:
            print("âš ï¸  nvidia-fs ãƒ‰ãƒ©ã‚¤ãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
    except Exception:
        print("âš ï¸  nvidia-fs ç¢ºèªã‚¨ãƒ©ãƒ¼")
        return False
    
    try:
        import kvikio
        print(f"âœ… kvikio ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {kvikio.__version__}")
        os.environ["KVIKIO_COMPAT_MODE"] = "OFF"
        print("âœ… KVIKIO_COMPAT_MODE=OFF è¨­å®šå®Œäº†")
        return True
    except ImportError:
        print("âš ï¸  kvikio ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False


def run_ray_parallel_sequential_gpu(
    limit_rows: int = 10000000,
    parallel_count: int = DEFAULT_PARALLEL,
    use_gpu_direct: bool = True,
    batch_size: int = 4,
    batch_size_mb: int = DEFAULT_BATCH_SIZE_MB,
    true_batch_mode: bool = False
):
    """Rayä¸¦åˆ—COPY + é †æ¬¡GPUå‡¦ç†ï¼ˆãƒãƒƒãƒåŒ–GPUè»¢é€ç‰ˆï¼‰"""
    
    # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    global_metrics = DetailedMetrics()
    global_metrics.log_memory_snapshot("start")
    
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    print(f"=== PostgreSQL â†’ GPU Rayä¸¦åˆ— é †æ¬¡GPUå‡¦ç†ç‰ˆï¼ˆãƒãƒƒãƒåŒ–GPUè»¢é€ï¼‰ ===")
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME}")
    print(f"è¡Œæ•°åˆ¶é™: {limit_rows:,}" if limit_rows else "è¡Œæ•°åˆ¶é™: ãªã—ï¼ˆå…¨ä»¶å‡¦ç†ï¼‰")
    print(f"ä¸¦åˆ—æ•°: {parallel_count}")
    print(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {CHUNK_COUNT}")
    print(f"ç·ã‚¿ã‚¹ã‚¯æ•°: {parallel_count * CHUNK_COUNT}")
    print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"GPUè»¢é€ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size_mb} MB")
    print(f"è»¢é€æ–¹å¼: ğŸ“¦ ãƒãƒƒãƒåŒ–GPUè»¢é€ï¼ˆ{batch_size_mb}MBã”ã¨ï¼‰")
    print(f"GPUå‡¦ç†æ–¹å¼: {'ğŸš€ çœŸã®ãƒãƒƒãƒå‡¦ç†ï¼ˆå®Ÿé¨“çš„ï¼‰' if true_batch_mode else 'å¾“æ¥ã®å€‹åˆ¥å‡¦ç†'}")
    
    # GPU Direct ã‚µãƒãƒ¼ãƒˆç¢ºèª
    gds_supported = check_gpu_direct_support() if use_gpu_direct else False
    
    # RayåˆæœŸåŒ–
    print("\n=== RayåˆæœŸåŒ– ===")
    if not ray.is_initialized():
        ray.init(num_cpus=parallel_count * 2)
        print(f"âœ… RayåˆæœŸåŒ–å®Œäº†")
    
    start_total_time = time.time()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    print("\nãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    start_meta_time = time.time()
    conn = psycopg.connect(dsn)
    try:
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        meta_time = time.time() - start_meta_time
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")
    finally:
        conn.close()
    
    global_metrics.log_memory_snapshot("after_metadata")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ–ãƒ­ãƒƒã‚¯æ•°å–å¾—
    print("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’å–å¾—ä¸­...")
    total_blocks = get_table_blocks(dsn, TABLE_NAME)
    print(f"ç·ãƒ–ãƒ­ãƒƒã‚¯æ•°: {total_blocks:,}")
    
    # ctidç¯„å›²åˆ†å‰²
    ranges = make_ctid_ranges(total_blocks, parallel_count)
    print(f"ctidç¯„å›²åˆ†å‰²: {len(ranges)}å€‹ã®ç¯„å›²")
    
    # Rayä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼ä½œæˆ
    workers = []
    for i in range(len(ranges)):
        worker = PostgreSQLWorker.remote(i, dsn)
        workers.append(worker)
    
    # å…¨ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«å…¥ã‚Œã‚‹
    print(f"\n=== ãƒ•ã‚§ãƒ¼ã‚º1: PostgreSQL COPYä¸¦åˆ—å®Ÿè¡Œï¼ˆãƒãƒƒãƒåŒ–GPUè»¢é€ï¼‰ ===")
    all_copy_futures = []
    
    for worker_idx, (start_block, end_block) in enumerate(ranges):
        for chunk_idx in range(CHUNK_COUNT):
            future = workers[worker_idx].copy_data_to_memory_batch.remote(
                TABLE_NAME, start_block, end_block, chunk_idx, CHUNK_COUNT, limit_rows,
                batch_size_mb=batch_size_mb
            )
            all_copy_futures.append(future)
    
    print(f"âœ… {len(all_copy_futures)}å€‹ã®COPYã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥")
    
    # ã‚»ãƒŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
    copy_results = []
    gpu_results = []
    pending_gpu_tasks = []
    
    total_copy_time = 0
    total_data_size = 0
    total_gpu_transfer_time = 0
    total_gpu_processing_time = 0
    total_rows_processed = 0
    all_worker_metrics = []
    
    output_base = OUTPUT_PARQUET_PATH
    
    print(f"\n=== ã‚»ãƒŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†é–‹å§‹ ===")
    print(f"COPYãŒ{batch_size}å€‹å®Œäº†ã™ã‚‹ã”ã¨ã«GPUå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    
    while all_copy_futures or pending_gpu_tasks:
        # COPYã‚¿ã‚¹ã‚¯ãŒã¾ã ã‚ã‚‹å ´åˆ
        if all_copy_futures:
            num_to_wait = min(batch_size, len(all_copy_futures))
            ready_futures, remaining_futures = ray.wait(all_copy_futures, num_returns=num_to_wait, timeout=0.1)
            all_copy_futures = remaining_futures
            
            for future in ready_futures:
                result = ray.get(future)
                
                if result['status'] == 'success':
                    total_copy_time += result['copy_time']
                    total_data_size += result['data_size']
                    pending_gpu_tasks.append(result)
                    all_worker_metrics.append(result['metrics'])
                    print(f"âœ… COPYå®Œäº†: Worker{result['worker_id']}-Chunk{result['chunk_idx']} "
                          f"({result['data_size']/(1024*1024):.1f}MB)")
                    
                    # çµ±è¨ˆæƒ…å ±ã®ã¿ã‚’ä¿æŒï¼ˆå¤§ããªãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–ï¼‰
                    copy_results.append({
                        'worker_id': result['worker_id'],
                        'chunk_idx': result['chunk_idx'],
                        'status': 'success',
                        'copy_time': result['copy_time'],
                        'data_size': result['data_size']
                    })
                else:
                    print(f"âŒ COPYå¤±æ•—: Worker{result['worker_id']}-Chunk{result['chunk_idx']} - "
                          f"{result.get('error', 'Unknown')}")
                    copy_results.append({
                        'worker_id': result['worker_id'],
                        'chunk_idx': result['chunk_idx'],
                        'status': 'error',
                        'error': result.get('error', 'Unknown')
                    })
        
        # GPUå‡¦ç†
        if (len(pending_gpu_tasks) >= batch_size) or (not all_copy_futures and pending_gpu_tasks):
            tasks_to_process = pending_gpu_tasks[:batch_size] if len(pending_gpu_tasks) >= batch_size else pending_gpu_tasks
            pending_gpu_tasks = pending_gpu_tasks[len(tasks_to_process):]
            
            global_metrics.log_memory_snapshot(f"before_gpu_batch_{len(gpu_results)}")
            
            batch_results = process_batch_on_gpu(
                tasks_to_process,
                columns,
                gds_supported,
                output_base,
                true_batch_mode=true_batch_mode
            )
            
            global_metrics.log_memory_snapshot(f"after_gpu_batch_{len(gpu_results)}")
            
            for gpu_result in batch_results:
                gpu_results.append(gpu_result)
                
                if gpu_result['status'] == 'success':
                    total_gpu_transfer_time += gpu_result['gpu_transfer_time']
                    total_gpu_processing_time += gpu_result['gpu_processing_time']
                    total_rows_processed += gpu_result['rows_processed']
            
            # GPUå‡¦ç†å¾Œã«CPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
            for task in tasks_to_process:
                if 'data' in task:
                    del task['data']  # å¤§ããªãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡ã‚’è§£æ”¾
            gc.collect()
            
            print(f"å‡¦ç†æ¸ˆã¿ã‚¿ã‚¹ã‚¯ç´¯è¨ˆ: {len(gpu_results)}/{len(copy_results)}")
    
    # æˆåŠŸã—ãŸå‡¦ç†ã®æ•°ã‚’é›†è¨ˆ
    successful_copies = [r for r in copy_results if r['status'] == 'success']
    successful_gpu = [r for r in gpu_results if r['status'] == 'success']
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†è¨ˆ
    if all_worker_metrics:
        total_chunk_count = sum(m['memory']['chunk_count'] for m in all_worker_metrics)
        avg_chunk_size = sum(m['memory']['avg_chunk_size'] * m['memory']['chunk_count'] for m in all_worker_metrics) / total_chunk_count
        total_batch_count = sum(m['memory']['batch_count'] for m in all_worker_metrics)
        avg_batch_size = sum(m['memory']['avg_batch_size_mb'] * m['memory']['batch_count'] for m in all_worker_metrics) / total_batch_count
        total_gpu_transfer_count = sum(m['performance']['gpu_transfer_count'] for m in all_worker_metrics)
        
        global_metrics.metrics['memory']['chunk_count'] = total_chunk_count
        global_metrics.metrics['memory']['avg_chunk_size'] = avg_chunk_size
        global_metrics.metrics['memory']['batch_count'] = total_batch_count
        global_metrics.metrics['memory']['avg_batch_size_mb'] = avg_batch_size
        global_metrics.metrics['performance']['gpu_transfer_count'] = total_gpu_transfer_count
    
    global_metrics.log_memory_snapshot("end")
    
    print(f"\nâœ… å…¨å‡¦ç†å®Œäº†")
    print(f"COPY: {len(successful_copies)}/{len(copy_results)} ã‚¿ã‚¹ã‚¯æˆåŠŸ")
    print(f"GPU: {len(successful_gpu)}/{len(gpu_results)} ã‚¿ã‚¹ã‚¯æˆåŠŸ")
    print(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_data_size / (1024*1024):.2f} MB")
    
    # ç·åˆçµæœ
    total_time = time.time() - start_total_time
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
    global_metrics.metrics['memory']['total_data_size_mb'] = total_data_size / (1024*1024)
    global_metrics.metrics['performance']['copy_time_sec'] = total_copy_time
    global_metrics.metrics['performance']['gpu_processing_time_sec'] = total_gpu_processing_time
    global_metrics.metrics['performance']['total_time_sec'] = total_time
    global_metrics.metrics['performance']['throughput_mb_sec'] = (total_data_size / (1024*1024)) / total_time if total_time > 0 else 0
    
    # ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆç®—
    peak_memory = global_metrics.metrics['memory']['peak_system_mb']
    initial_memory = global_metrics.metrics['memory']['initial_system_mb']
    memory_used = peak_memory - initial_memory
    if total_data_size > 0:
        global_metrics.metrics['memory']['memory_overhead_ratio'] = memory_used / (total_data_size / (1024*1024))
    
    print(f"\n{'='*60}")
    print(f"=== Rayä¸¦åˆ—ã‚»ãƒŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³GPUå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼ˆãƒãƒƒãƒåŒ–GPUè»¢é€ï¼‰ ===")
    print(f"{'='*60}")
    print(f"ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—    : {meta_time:.4f} ç§’")
    print(f"  PostgreSQL COPY   : {total_copy_time:.4f} ç§’ (ç´¯ç©)")
    print(f"  GPUè»¢é€          : {total_gpu_transfer_time:.4f} ç§’ (ç´¯ç©)")
    print(f"  GPUå‡¦ç†          : {total_gpu_processing_time:.4f} ç§’ (ç´¯ç©)")
    print("--- çµ±è¨ˆæƒ…å ± ---")
    print(f"  å‡¦ç†è¡Œæ•°ï¼ˆåˆè¨ˆï¼‰  : {total_rows_processed:,} è¡Œ")
    print(f"  å‡¦ç†åˆ—æ•°         : {len(columns)} åˆ—")
    print(f"  ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º    : {total_data_size / (1024*1024):.2f} MB")
    print(f"  æˆåŠŸç‡           : COPY {len(successful_copies)}/{len(copy_results)}, "
          f"GPU {len(successful_gpu)}/{len(gpu_results)}")
    
    print("\n--- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ---")
    print(f"  åˆæœŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª : {initial_memory:.2f} MB")
    print(f"  ãƒ”ãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {peak_memory:.2f} MB")
    print(f"  ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡      : {memory_used:.2f} MB")
    print(f"  ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {global_metrics.metrics['memory']['memory_overhead_ratio']:.2f}x")
    print(f"  ç·ãƒãƒ£ãƒ³ã‚¯æ•°      : {global_metrics.metrics['memory']['chunk_count']:,}")
    print(f"  å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º : {global_metrics.metrics['memory']['avg_chunk_size']:.0f} bytes")
    print(f"  ç·ãƒãƒƒãƒæ•°        : {global_metrics.metrics['memory']['batch_count']:,}")
    print(f"  å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚º  : {global_metrics.metrics['memory']['avg_batch_size_mb']:.1f} MB")
    print(f"  GPUè»¢é€å›æ•°       : {global_metrics.metrics['performance']['gpu_transfer_count']:,}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
    if total_data_size > 0:
        overall_throughput = (total_data_size / (1024*1024)) / total_time
        print("\n--- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ ---")
        print(f"  å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ  : {overall_throughput:.2f} MB/sec")
    
    print("\n--- å‡¦ç†æ–¹å¼ã®ç‰¹å¾´ ---")
    print(f"  âœ… ãƒãƒƒãƒåŒ–GPUè»¢é€: {batch_size_mb}MBã”ã¨ã«GPUè»¢é€")
    print("  âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ä¸­é–“ãƒãƒƒãƒ•ã‚¡ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†ã®ã¿")
    print("  âœ… GPUè»¢é€æœ€é©åŒ–: è»¢é€å›æ•°ã‚’å¤§å¹…å‰Šæ¸›")
    print("=========================================")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    metrics_file = f"{OUTPUT_PARQUET_PATH}_metrics_batch.json"
    global_metrics.save_to_json(metrics_file)
    print(f"\nâœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_file}")
    
    # Rayçµ‚äº†
    ray.shutdown()
    print("\nâœ… Rayçµ‚äº†")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU Rayä¸¦åˆ— ãƒãƒƒãƒåŒ–GPUè»¢é€ç‰ˆ')
    parser.add_argument('--rows', type=int, default=10000000, help='å‡¦ç†è¡Œæ•°åˆ¶é™')
    parser.add_argument('--parallel', type=int, default=DEFAULT_PARALLEL, help='ä¸¦åˆ—æ•°')
    parser.add_argument('--chunks', type=int, default=4, help='ãƒãƒ£ãƒ³ã‚¯æ•°')
    parser.add_argument('--batch-size', type=int, default=4, help='ã‚»ãƒŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--gpu-batch-mb', type=int, default=DEFAULT_BATCH_SIZE_MB, help='GPUè»¢é€ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆMBï¼‰')
    parser.add_argument('--true-batch', action='store_true', help='çœŸã®ãƒãƒƒãƒGPUå‡¦ç†ã‚’æœ‰åŠ¹åŒ–ï¼ˆå®Ÿé¨“çš„ï¼‰')
    parser.add_argument('--no-limit', action='store_true', help='LIMITç„¡ã—ï¼ˆå…¨ä»¶é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰')
    parser.add_argument('--check-support', action='store_true', help='GPU Directã‚µãƒãƒ¼ãƒˆç¢ºèªã®ã¿')
    parser.add_argument('--no-gpu-direct', action='store_true', help='GPU Directç„¡åŠ¹åŒ–')
    
    args = parser.parse_args()
    
    # CUDAç¢ºèª
    try:
        cuda.current_context()
        print("âœ… CUDA context OK")
    except Exception as e:
        print(f"âŒ CUDAåˆæœŸåŒ–å¤±æ•—: {e}")
        exit(1)
    
    if args.check_support:
        check_gpu_direct_support()
        return
    
    # ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’è¨­å®š
    global CHUNK_COUNT
    CHUNK_COUNT = args.chunks
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    final_limit_rows = None if args.no_limit else args.rows
    run_ray_parallel_sequential_gpu(
        limit_rows=final_limit_rows,
        parallel_count=args.parallel,
        use_gpu_direct=not args.no_gpu_direct,
        batch_size=args.batch_size,
        batch_size_mb=args.gpu_batch_mb,
        true_batch_mode=args.true_batch
    )

if __name__ == "__main__":
    main()