"""
Decimal128æœ€é©åŒ–ã®åŠ¹æœæ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

ä¿®æ­£å‰å¾Œã§ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã«å¤‰åŒ–ãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ã€æ”¹ä¿®æ–¹æ³•ãŒæ­£ã—ã„ã“ã¨ã‚’ç¤ºã™ã€‚
ç’°å¢ƒå¤‰æ•° USE_DECIMAL_OPTIMIZATION ã§æœ€é©åŒ–ã®ON/OFFã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
"""

import os
import time
import numpy as np
import psycopg
import pyarrow as pa
import pyarrow.parquet as pq
import cudf
from numba import cuda
import hashlib

# Import necessary functions from the correct modules using absolute paths from root
from src.meta_fetch import fetch_column_meta, ColumnMeta
from src.gpu_parse_wrapper import parse_binary_chunk_gpu, detect_pg_header_size
from src.gpu_decoder_v2 import decode_chunk

# ãƒ†ã‚¹ãƒˆè¨­å®š
TABLE_NAME = "lineorder"
OUTPUT_DIR = "benchmark/comparison_output"
LIMIT_ROWS = 100000  # ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™

def setup_test_environment():
    """ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # CUDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
    try:
        cuda.current_context()
        print("âœ“ CUDA context OK")
    except Exception as e:
        print(f"âœ— CUDA context initialization failed: {e}")
        exit(1)

def run_benchmark_with_optimization(use_optimization: bool):
    """
    æœ€é©åŒ–ã®ON/OFFã‚’åˆ‡ã‚Šæ›¿ãˆã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    
    Parameters
    ----------
    use_optimization : bool
        True: æœ€é©åŒ–ç‰ˆã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨, False: å¾“æ¥ç‰ˆã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨
    """
    # ç’°å¢ƒå¤‰æ•°ã§ã‚«ãƒ¼ãƒãƒ«é¸æŠã‚’åˆ¶å¾¡
    os.environ["USE_DECIMAL_OPTIMIZATION"] = "1" if use_optimization else "0"
    
    optimization_label = "æœ€é©åŒ–ç‰ˆ" if use_optimization else "å¾“æ¥ç‰ˆ"
    print(f"\n{'='*60}")
    print(f"{optimization_label} ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
    print(f"{'='*60}")
    
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None, None

    prefix = os.environ.get("PG_TABLE_PREFIX", "")
    tbl = f"{prefix}{TABLE_NAME}" if prefix else TABLE_NAME

    start_total_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    conn = psycopg.connect(dsn)
    try:
        print("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        start_meta_time = time.time()
        columns = fetch_column_meta(conn, f"SELECT * FROM {tbl}")
        meta_time = time.time() - start_meta_time
        
        # DECIMALåˆ—ã®æƒ…å ±è¡¨ç¤º
        decimal_columns = [col for col in columns if col.arrow_id == 5]  # DECIMAL128
        if decimal_columns:
            print(f"DECIMALåˆ—æ¤œå‡º: {len(decimal_columns)}åˆ—")
            for col in decimal_columns:
                precision, scale = col.arrow_param or (38, 0)
                optimization_type = "Decimal64" if precision <= 18 and use_optimization else "Decimal128"
                print(f"  {col.name}: precision={precision}, scale={scale} â†’ {optimization_type}")
        
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")
        ncols = len(columns)

        # COPY BINARYå®Ÿè¡Œ
        print(f"COPY BINARY ã‚’å®Ÿè¡Œä¸­ (LIMIT {LIMIT_ROWS})...")
        start_copy_time = time.time()
        copy_sql = f"COPY (SELECT * FROM {tbl} LIMIT {LIMIT_ROWS}) TO STDOUT (FORMAT binary)"
        buf = bytearray()
        with conn.cursor().copy(copy_sql) as cpy:
            while True:
                chunk = cpy.read()
                if not chunk:
                    break
                buf.extend(chunk)
            raw_host = np.frombuffer(buf, dtype=np.uint8)
        copy_time = time.time() - start_copy_time
        print(f"COPY BINARY å®Œäº† ({copy_time:.4f}ç§’), ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(raw_host) / (1024*1024):.2f} MB")

    finally:
        conn.close()

    # GPUå‡¦ç†
    print("GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ä¸­...")
    start_transfer_time = time.time()
    raw_dev = cuda.to_device(raw_host)
    transfer_time = time.time() - start_transfer_time
    print(f"GPUè»¢é€å®Œäº† ({transfer_time:.4f}ç§’)")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
    header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
    header_size = detect_pg_header_size(header_sample)
    print(f"æ¤œå‡ºã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º: {header_size} ãƒã‚¤ãƒˆ")

    # GPUãƒ‘ãƒ¼ã‚¹
    print("GPUã§ãƒ‘ãƒ¼ã‚¹ä¸­...")
    start_parse_time = time.time()
    field_offsets_dev, field_lengths_dev = parse_binary_chunk_gpu(
        raw_dev,
        ncols=ncols,
        header_size=header_size
    )
    parse_time = time.time() - start_parse_time
    rows = field_offsets_dev.shape[0]
    print(f"GPUãƒ‘ãƒ¼ã‚¹å®Œäº† ({parse_time:.4f}ç§’), è¡Œæ•°: {rows}")

    # GPUãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆã“ã“ã§æœ€é©åŒ–ãŒé©ç”¨ã•ã‚Œã‚‹ï¼‰
    print(f"GPUã§ãƒ‡ã‚³ãƒ¼ãƒ‰ä¸­ ({optimization_label})...")
    start_decode_time = time.time()
    batch = decode_chunk(raw_dev, field_offsets_dev, field_lengths_dev, columns)
    decode_time = time.time() - start_decode_time
    print(f"GPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† ({decode_time:.4f}ç§’)")

    # Arrow Tableä½œæˆ
    result_table = pa.Table.from_batches([batch])
    print(f"Arrow Table ä½œæˆå®Œäº†: {result_table.num_rows} è¡Œ, {result_table.num_columns} åˆ—")

    # Parquetå‡ºåŠ›
    output_path = f"{OUTPUT_DIR}/lineorder_{optimization_label.replace('ç‰ˆ', '')}.parquet"
    print(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ä¸­: {output_path}")
    start_write_time = time.time()
    pq.write_table(result_table, output_path)
    write_time = time.time() - start_write_time
    print(f"Parquetæ›¸ãè¾¼ã¿å®Œäº† ({write_time:.4f}ç§’)")

    total_time = time.time() - start_total_time
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœ
    performance_result = {
        "optimization": use_optimization,
        "total_time": total_time,
        "meta_time": meta_time,
        "copy_time": copy_time,
        "transfer_time": transfer_time,
        "parse_time": parse_time,
        "decode_time": decode_time,
        "write_time": write_time,
        "rows": rows,
        "columns": ncols,
        "decimal_columns": len(decimal_columns) if decimal_columns else 0
    }
    
    print(f"\n{optimization_label} å®Œäº†: ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—: {meta_time:.4f} ç§’")
    print(f"  COPY BINARY   : {copy_time:.4f} ç§’")
    print(f"  GPUè»¢é€       : {transfer_time:.4f} ç§’")
    print(f"  GPUãƒ‘ãƒ¼ã‚¹     : {parse_time:.4f} ç§’")
    print(f"  GPUãƒ‡ã‚³ãƒ¼ãƒ‰   : {decode_time:.4f} ç§’")
    print(f"  Parquetæ›¸ãè¾¼ã¿: {write_time:.4f} ç§’")
    print("----------------")
    
    return result_table, performance_result

def calculate_table_hash(table: pa.Table) -> str:
    """Arrow Tableã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸€è‡´ç¢ºèªç”¨ï¼‰"""
    # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒŠãƒªå½¢å¼ã§çµåˆã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
    hash_input = bytearray()
    
    for i in range(table.num_columns):
        column = table.column(i)
        # Arrowé…åˆ—ã‚’PyListã«å¤‰æ›ã—ã¦ãƒã‚¤ãƒˆåˆ—åŒ–
        try:
            data_bytes = str(column.to_pylist()).encode('utf-8')
            hash_input.extend(data_bytes)
        except Exception as e:
            print(f"Warning: ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ (åˆ— {table.column_names[i]}): {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ—åã¨nullæƒ…å ±ã®ã¿
            hash_input.extend(table.column_names[i].encode('utf-8'))
            hash_input.extend(str(column.null_count).encode('utf-8'))
    
    return hashlib.md5(hash_input).hexdigest()

def verify_data_consistency(table1: pa.Table, table2: pa.Table):
    """2ã¤ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ä¸€è‡´ã‚’æ¤œè¨¼"""
    print("\n" + "="*60)
    print("ãƒ‡ãƒ¼ã‚¿ä¸€è‡´æ¤œè¨¼")
    print("="*60)
    
    # åŸºæœ¬æ§‹é€ æ¯”è¼ƒ
    print(f"è¡Œæ•°: {table1.num_rows} vs {table2.num_rows}")
    print(f"åˆ—æ•°: {table1.num_columns} vs {table2.num_columns}")
    
    if table1.num_rows != table2.num_rows or table1.num_columns != table2.num_columns:
        print("âœ— ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ãŒç•°ãªã‚Šã¾ã™")
        return False
    
    # åˆ—åæ¯”è¼ƒ
    if table1.column_names != table2.column_names:
        print("âœ— åˆ—åãŒç•°ãªã‚Šã¾ã™")
        return False
    
    # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒ
    all_match = True
    decimal_column_results = []
    
    for i, col_name in enumerate(table1.column_names):
        col1 = table1.column(i)
        col2 = table2.column(i)
        
        # null countæ¯”è¼ƒ
        if col1.null_count != col2.null_count:
            print(f"âœ— åˆ— {col_name}: nullæ•°ãŒç•°ãªã‚Šã¾ã™ ({col1.null_count} vs {col2.null_count})")
            all_match = False
            continue
        
        # ãƒ‡ãƒ¼ã‚¿å‹æ¯”è¼ƒ
        if str(col1.type) != str(col2.type):
            print(f"âœ— åˆ— {col_name}: ãƒ‡ãƒ¼ã‚¿å‹ãŒç•°ãªã‚Šã¾ã™ ({col1.type} vs {col2.type})")
            all_match = False
            continue
        
        # DECIMALåˆ—ã®è©³ç´°æ¯”è¼ƒ
        if pa.types.is_decimal(col1.type):
            try:
                # PyArrowã®compareæ©Ÿèƒ½ã‚’ä½¿ç”¨
                equals = col1.equals(col2)
                if equals:
                    print(f"âœ“ DECIMALåˆ— {col_name}: ãƒ‡ãƒ¼ã‚¿ä¸€è‡´")
                    decimal_column_results.append((col_name, True, "å®Œå…¨ä¸€è‡´"))
                else:
                    print(f"âœ— DECIMALåˆ— {col_name}: ãƒ‡ãƒ¼ã‚¿ä¸ä¸€è‡´")
                    decimal_column_results.append((col_name, False, "ãƒ‡ãƒ¼ã‚¿ä¸ä¸€è‡´"))
                    all_match = False
                    
                    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    print(f"  ã‚µãƒ³ãƒ—ãƒ« (æœ€é©åŒ–å‰): {col1.slice(0, min(5, col1.length)).to_pylist()}")
                    print(f"  ã‚µãƒ³ãƒ—ãƒ« (æœ€é©åŒ–å¾Œ): {col2.slice(0, min(5, col2.length)).to_pylist()}")
            except Exception as e:
                print(f"âš  DECIMALåˆ— {col_name}: æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ ({e})")
                decimal_column_results.append((col_name, False, f"æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}"))
        else:
            # éDECIMALåˆ—ã®æ¯”è¼ƒ
            try:
                equals = col1.equals(col2)
                if not equals:
                    print(f"âœ— åˆ— {col_name}: ãƒ‡ãƒ¼ã‚¿ä¸ä¸€è‡´")
                    all_match = False
            except Exception as e:
                print(f"âš  åˆ— {col_name}: æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ ({e})")
    
    # ãƒãƒƒã‚·ãƒ¥å€¤æ¯”è¼ƒ
    print("\nãƒãƒƒã‚·ãƒ¥å€¤æ¯”è¼ƒ...")
    hash1 = calculate_table_hash(table1)
    hash2 = calculate_table_hash(table2)
    print(f"å¾“æ¥ç‰ˆãƒãƒƒã‚·ãƒ¥: {hash1}")
    print(f"æœ€é©åŒ–ç‰ˆãƒãƒƒã‚·ãƒ¥: {hash2}")
    
    if hash1 == hash2:
        print("âœ“ ãƒãƒƒã‚·ãƒ¥å€¤ä¸€è‡´ - ãƒ‡ãƒ¼ã‚¿ã¯åŒä¸€ã§ã™")
    else:
        print("âœ— ãƒãƒƒã‚·ãƒ¥å€¤ä¸ä¸€è‡´")
        all_match = False
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n--- DECIMALåˆ—æ¤œè¨¼çµæœ ---")
    for col_name, match, detail in decimal_column_results:
        status = "âœ“" if match else "âœ—"
        print(f"{status} {col_name}: {detail}")
    
    return all_match

def run_comparison_benchmark():
    """æœ€é©åŒ–å‰å¾Œã®æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    setup_test_environment()
    
    print("Decimal128æœ€é©åŒ–åŠ¹æœæ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    print("=" * 80)
    
    # å¾“æ¥ç‰ˆå®Ÿè¡Œ
    table_original, perf_original = run_benchmark_with_optimization(use_optimization=False)
    
    # æœ€é©åŒ–ç‰ˆå®Ÿè¡Œ
    table_optimized, perf_optimized = run_benchmark_with_optimization(use_optimization=True)
    
    if table_original is None or table_optimized is None:
        print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ãƒ‡ãƒ¼ã‚¿ä¸€è‡´æ¤œè¨¼
    data_consistent = verify_data_consistency(table_original, table_optimized)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    print("\n" + "="*60)
    print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
    print("="*60)
    
    decode_speedup = perf_original["decode_time"] / perf_optimized["decode_time"] if perf_optimized["decode_time"] > 0 else 1.0
    total_speedup = perf_original["total_time"] / perf_optimized["total_time"] if perf_optimized["total_time"] > 0 else 1.0
    
    print(f"ç·å®Ÿè¡Œæ™‚é–“:")
    print(f"  å¾“æ¥ç‰ˆ: {perf_original['total_time']:.4f}ç§’")
    print(f"  æœ€é©åŒ–ç‰ˆ: {perf_optimized['total_time']:.4f}ç§’")
    print(f"  é«˜é€ŸåŒ–ç‡: {total_speedup:.2f}x")
    
    print(f"\nGPUãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚é–“:")
    print(f"  å¾“æ¥ç‰ˆ: {perf_original['decode_time']:.4f}ç§’")
    print(f"  æœ€é©åŒ–ç‰ˆ: {perf_optimized['decode_time']:.4f}ç§’")
    print(f"  é«˜é€ŸåŒ–ç‡: {decode_speedup:.2f}x")
    
    print(f"\nDECIMALåˆ—æ•°: {perf_original['decimal_columns']}")
    print(f"ç·è¡Œæ•°: {perf_original['rows']:,}")
    print(f"ç·åˆ—æ•°: {perf_original['columns']}")
    
    # æœ€çµ‚çµæœ
    print("\n" + "="*60)
    print("æ¤œè¨¼çµæœ")
    print("="*60)
    
    if data_consistent:
        print("âœ… ãƒ‡ãƒ¼ã‚¿ä¸€è‡´ç¢ºèª: æˆåŠŸ")
        print("âœ… æœ€é©åŒ–ã®å®Ÿè£…ã¯æ­£ã—ãå‹•ä½œã—ã¦ã„ã¾ã™")
        
        if decode_speedup > 1.1:
            print(f"ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š: {decode_speedup:.2f}x é«˜é€ŸåŒ–é”æˆ!")
        elif decode_speedup > 0.9:
            print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: åŒç­‰ãƒ¬ãƒ™ãƒ« (æœ€é©åŒ–åŠ¹æœã¯é™å®šçš„)")
        else:
            print("âš ï¸  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: è‹¥å¹²ä½ä¸‹ (è¦èª¿æŸ»)")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ä¸€è‡´ç¢ºèª: å¤±æ•—")
        print("âŒ æœ€é©åŒ–ã®å®Ÿè£…ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
    
    return data_consistent, decode_speedup

if __name__ == "__main__":
    try:
        success, speedup = run_comparison_benchmark()
        if success:
            print(f"\nğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æˆåŠŸ! é«˜é€ŸåŒ–ç‡: {speedup:.2f}x")
            exit(0)
        else:
            print(f"\nğŸ’¥ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¤±æ•—")
            exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
