"""
PostgreSQL â†’ GPU ç©¶æ¥µæœ€é©åŒ–ç‰ˆ
è»¢é€é€Ÿåº¦ã‚’10GB/sè¿‘ãã¾ã§å¼•ãä¸Šã’ã‚‹

ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æã«åŸºã¥ãæœ€é©åŒ–:
- libpq ã‹ã‚‰ 1MBä»¥ä¸Šã®å¤§ããªãƒãƒ£ãƒ³ã‚¯ã§èª­å–ã‚Š
- pinned memcpyã®æœ€é©åŒ–
- 64MiBä»¥ä¸Šã®å¤§ããªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
- è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã®éåŒæœŸDMA
- bandwidthTest ã§ã®å®Ÿæ¸¬ç¢ºèª

ç’°å¢ƒå¤‰æ•°:
GPUPASER_PG_DSN  : PostgreSQLæ¥ç¶šæ–‡å­—åˆ—
"""

import os
import time
import ctypes
import psycopg
import rmm
import numpy as np
import numba
from numba import cuda
import cupy as cp
import argparse

from src.metadata import fetch_column_meta
from src.cuda_kernels.postgresql_binary_parser import detect_pg_header_size
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet

TABLE_NAME = "lineorder"
OUTPUT_PARQUET_PATH = "benchmark/lineorder_ultimate_optimized.output.parquet"

# ç©¶æ¥µæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
CHUNK_SIZE = 64 << 20  # 64 MiB (å¤§ããªãƒãƒ£ãƒ³ã‚¯ã§åŠ¹ç‡æœ€å¤§åŒ–)
READ_SIZE = 4 << 20    # 4 MiB (libpqã‹ã‚‰ã®èª­ã¿å–ã‚Šã‚µã‚¤ã‚º)
NUM_STREAMS = 2        # è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§pipelined DMA

def run_bandwidth_test():
    """CUDA bandwidthTest ã§PCIeæ€§èƒ½ç¢ºèª"""
    print("\n=== PCIeå¸¯åŸŸå¹…ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        import subprocess
        result = subprocess.run([
            "nvidia-smi", "--query-gpu=pci.link.gen.current,pci.link.width.current", "--format=csv,noheader,nounits"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            gen, width = result.stdout.strip().split(', ')
            print(f"PCIeä¸–ä»£: Gen {gen}, å¹…: x{width}")
            
            # ç†è«–å¸¯åŸŸè¨ˆç®—
            gen_speed = {"3": 8, "4": 16, "5": 32}  # GT/s per lane
            if gen in gen_speed:
                theoretical_gbps = gen_speed[gen] * int(width) * 1.0  # GB/s (8b/10bè€ƒæ…®)
                print(f"ç†è«–å¸¯åŸŸ: {theoretical_gbps:.1f} GB/s")
        
    except Exception as e:
        print(f"PCIeæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªå¸¯åŸŸæ¸¬å®š
    try:
        print("ã‚·ãƒ³ãƒ—ãƒ«å¸¯åŸŸæ¸¬å®šå®Ÿè¡Œä¸­...")
        size_mb = 1024  # 1GB
        host_data = np.random.randint(0, 256, size_mb * 1024 * 1024, dtype=np.uint8)
        
        # Host to Device
        start_time = time.time()
        device_data = cuda.to_device(host_data)
        htod_time = time.time() - start_time
        htod_speed = size_mb / htod_time
        
        # Device to Host  
        start_time = time.time()
        result_data = device_data.copy_to_host()
        dtoh_time = time.time() - start_time
        dtoh_speed = size_mb / dtoh_time
        
        print(f"Hostâ†’Device: {htod_speed:.2f} MB/s")
        print(f"Deviceâ†’Host: {dtoh_speed:.2f} MB/s")
        
    except Exception as e:
        print(f"å¸¯åŸŸæ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")

def run_ultimate_optimized_benchmark(limit_rows=1000000):
    """
    ç©¶æ¥µæœ€é©åŒ–ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    10GB/sè¿‘ã„è»¢é€é€Ÿåº¦ã‚’ç›®æŒ‡ã™
    """
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    print(f"=== PostgreSQL â†’ GPU ç©¶æ¥µæœ€é©åŒ–ç‰ˆ ===")
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME}")
    print(f"è¡Œæ•°åˆ¶é™: {limit_rows:,}")
    print(f"æœ€é©åŒ–è¨­å®š:")
    print(f"  ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE / (1024*1024):.0f} MB")
    print(f"  èª­ã¿å–ã‚Šã‚µã‚¤ã‚º: {READ_SIZE / (1024*1024):.0f} MB")
    print(f"  ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°: {NUM_STREAMS}")
    
    # PCIeå¸¯åŸŸãƒ†ã‚¹ãƒˆ
    run_bandwidth_test()
    
    # RMM åˆæœŸåŒ–
    try:
        if not rmm.is_initialized():
            rmm.reinitialize(
                pool_allocator=True, 
                initial_pool_size=16*1024**3  # 16GB (å¤§å®¹é‡)
            )
            print("âœ… RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº† (16GB)")
    except Exception as e:
        print(f"âŒ RMMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    start_total_time = time.time()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    conn = psycopg.connect(dsn)
    try:
        print("\nãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        start_meta_time = time.time()
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        meta_time = time.time() - start_meta_time
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")
        ncols = len(columns)

        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¨å®š
        rows_est = limit_rows
        row_bytes = 17*8 + 17*4  # æ¦‚ç®—
        header_est = 19
        estimated_size = header_est + rows_est * row_bytes
        print(f"æ¨å®šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {estimated_size / (1024*1024):.2f} MB")

        # GPU ãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿
        print("GPU ãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿ä¸­...")
        devbuf = rmm.DeviceBuffer(size=estimated_size)
        print(f"GPU ãƒãƒƒãƒ•ã‚¡ç¢ºä¿å®Œäº†: {estimated_size / (1024*1024):.2f} MB")

        # è¶…å¤§å®¹é‡ Pinned ãƒ›ã‚¹ãƒˆãƒãƒƒãƒ•ã‚¡ç¢ºä¿
        print("å¤§å®¹é‡ Pinned ãƒ›ã‚¹ãƒˆãƒãƒƒãƒ•ã‚¡ç¢ºä¿ä¸­...")
        pbuf1 = numba.cuda.pinned_array(CHUNK_SIZE, dtype=np.uint8)
        pbuf2 = numba.cuda.pinned_array(CHUNK_SIZE, dtype=np.uint8)
        print(f"âœ… ãƒ€ãƒ–ãƒ« Pinned ãƒãƒƒãƒ•ã‚¡ç¢ºä¿å®Œäº†: {CHUNK_SIZE / (1024*1024):.0f} MB Ã— 2")

        # è¤‡æ•° CUDA ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆ
        streams = []
        for i in range(NUM_STREAMS):
            streams.append(cp.cuda.Stream(non_blocking=True))
        print(f"âœ… CUDA ã‚¹ãƒˆãƒªãƒ¼ãƒ  {NUM_STREAMS}å€‹ä½œæˆå®Œäº†")

        # ç©¶æ¥µæœ€é©åŒ– COPY â†’ GPUè»¢é€
        print("ç©¶æ¥µæœ€é©åŒ– COPY â†’ GPUè»¢é€å®Ÿè¡Œä¸­...")
        start_copy_time = time.time()
        
        copy_sql = f"COPY (SELECT * FROM {TABLE_NAME} LIMIT {limit_rows}) TO STDOUT (FORMAT binary)"
        
        offset = 0
        chunk_count = 0
        total_read_time = 0
        total_memcpy_time = 0
        total_async_time = 0
        stream_idx = 0
        
        with conn.cursor() as cur:
            with cur.copy(copy_sql) as copy_obj:
                print("  ğŸš€ å¤§ããªãƒãƒ£ãƒ³ã‚¯ã§ã®é«˜é€Ÿèª­ã¿å–ã‚Šé–‹å§‹...")
                
                # ã€æœ€é©åŒ–aã€‘ãƒãƒ£ãƒ³ã‚¯ã‚’è“„ç©ã—ã¦å¤§ããªãƒ–ãƒ­ãƒƒã‚¯ã§å‡¦ç†
                accumulated_data = []
                accumulated_size = 0
                
                for chunk in copy_obj:
                    if not chunk:
                        break
                    
                    start_read = time.time()
                    accumulated_data.append(chunk)
                    accumulated_size += len(chunk)
                    read_time = time.time() - start_read
                    total_read_time += read_time
                    
                    # READ_SIZE (4MB) ã«é”ã—ãŸã‚‰å‡¦ç†
                    if accumulated_size >= READ_SIZE:
                        # è“„ç©ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆ
                        combined_chunk = b''.join(accumulated_data)
                        
                        # å‡¦ç†å¯¾è±¡ã¨ã—ã¦ä½¿ç”¨
                        chunk = combined_chunk
                        chunk_size = len(chunk)
                        
                        # ãƒªã‚»ãƒƒãƒˆ
                        accumulated_data = []
                        accumulated_size = 0
                    else:
                        continue  # ã¾ã è“„ç©ä¸­
                    
                    # GPUè»¢é€å‡¦ç†
                    self._process_chunk_to_gpu(chunk, offset, devbuf, pbuf1, pbuf2, streams,
                                             chunk_count, total_memcpy_time, total_async_time)
                    
                    offset += chunk_size
                    chunk_count += 1
                
                # æ®‹ã‚Šã®è“„ç©ãƒ‡ãƒ¼ã‚¿ã‚‚å‡¦ç†
                if accumulated_data:
                    combined_chunk = b''.join(accumulated_data)
                    chunk = combined_chunk
                    chunk_size = len(chunk)
                    
                    # GPUè»¢é€å‡¦ç†
                    self._process_chunk_to_gpu(chunk, offset, devbuf, pbuf1, pbuf2, streams,
                                             chunk_count, total_memcpy_time, total_async_time)
                    
                    offset += chunk_size
                    chunk_count += 1

def _process_chunk_to_gpu(chunk, offset, devbuf, pbuf1, pbuf2, streams, chunk_count, total_memcpy_time, total_async_time):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’GPUã«è»¢é€ã™ã‚‹å…±é€šå‡¦ç†"""
    chunk_size = len(chunk)
    
    # ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ ãƒã‚§ãƒƒã‚¯
    if offset + chunk_size > devbuf.size:
        print(f"âš ï¸  è­¦å‘Š: GPUãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºä¸è¶³")
        return False
                    
                    # ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡é¸æŠ
                    current_buf = pbuf1 if chunk_count % 2 == 0 else pbuf2
                    current_stream = streams[stream_idx % NUM_STREAMS]
                    
                    # ã€æœ€é©åŒ–cã€‘åŠ¹ç‡çš„ pinned memcpy
                    start_memcpy = time.time()
                    if chunk_size <= CHUNK_SIZE:
                        # NumPyæœ€é©åŒ–ã‚³ãƒ”ãƒ¼
                        np_chunk = np.frombuffer(chunk, dtype=np.uint8)
                        current_buf[:chunk_size] = np_chunk
                    else:
                        # åˆ†å‰²å‡¦ç†
                        for sub_offset in range(0, chunk_size, CHUNK_SIZE):
                            sub_size = min(CHUNK_SIZE, chunk_size - sub_offset)
                            sub_chunk = chunk[sub_offset:sub_offset+sub_size]
                            np_sub = np.frombuffer(sub_chunk, dtype=np.uint8)
                            current_buf[:sub_size] = np_sub
                    memcpy_time = time.time() - start_memcpy
                    total_memcpy_time += memcpy_time
                    
                    # ã€æœ€é©åŒ–eã€‘éåŒæœŸ DMA (è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ )
                    start_async = time.time()
                    src_ptr = ctypes.addressof(ctypes.c_char.from_buffer(current_buf))
                    dst_ptr = devbuf.ptr + offset
                    
                    cp.cuda.runtime.memcpyAsync(
                        dst_ptr, src_ptr, chunk_size,
                        cp.cuda.runtime.memcpyHostToDevice,
                        current_stream.ptr
                    )
                    async_time = time.time() - start_async
                    total_async_time += async_time
                    
                    offset += chunk_size
                    chunk_count += 1
                    stream_idx += 1
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆå°‘ãªã‚ã«ï¼‰
                    if chunk_count % 100 == 0:
                        print(f"    ãƒãƒ£ãƒ³ã‚¯ {chunk_count:,} | {offset / (1024*1024):.0f} MB | å¹³å‡ {chunk_size / (1024*1024):.1f} MB/chunk")
                
                # å…¨ã‚¹ãƒˆãƒªãƒ¼ãƒ åŒæœŸ
                print("  â³ å…¨ã‚¹ãƒˆãƒªãƒ¼ãƒ åŒæœŸå¾…æ©Ÿä¸­...")
                for stream in streams:
                    cp.cuda.runtime.streamSynchronize(stream.ptr)
        
        copy_time = time.time() - start_copy_time
        actual_data_size = offset
        
        print(f"âœ… ç©¶æ¥µæœ€é©åŒ–è»¢é€å®Œäº† ({copy_time:.4f}ç§’)")
        print(f"  å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
        print(f"  å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {(actual_data_size / chunk_count) / (1024*1024):.2f} MB")
        print(f"  å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {actual_data_size / (1024*1024):.2f} MB")
        print(f"  ç·åˆè»¢é€é€Ÿåº¦: {actual_data_size / (1024*1024) / copy_time:.2f} MB/sec")

    finally:
        conn.close()
        # Pinned ãƒ¡ãƒ¢ãƒªè§£æ”¾
        if 'pbuf1' in locals():
            del pbuf1
        if 'pbuf2' in locals():
            del pbuf2
        if 'streams' in locals():
            for stream in streams:
                del stream

    # GPU ãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
    if actual_data_size < devbuf.size:
        print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ä¸­...")
        trimmed_devbuf = rmm.DeviceBuffer(size=actual_data_size)
        cp.cuda.runtime.memcpy(
            trimmed_devbuf.ptr, devbuf.ptr, actual_data_size,
            cp.cuda.runtime.memcpyDeviceToDevice
        )
        devbuf = trimmed_devbuf
        print(f"GPU ãƒãƒƒãƒ•ã‚¡ãƒˆãƒªãƒŸãƒ³ã‚°å®Œäº†: {actual_data_size / (1024*1024):.2f} MB")

    # numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›
    print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›ä¸­...")
    raw_dev = cuda.as_cuda_array(devbuf).view(dtype=np.uint8)
    print(f"GPU ã‚¢ãƒ¬ã‚¤å¤‰æ›å®Œäº†: {raw_dev.shape[0]:,} bytes")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
    header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
    header_size = detect_pg_header_size(header_sample)
    print(f"ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º: {header_size} ãƒã‚¤ãƒˆ")

    # GPUæœ€é©åŒ–å‡¦ç†
    print("GPUæœ€é©åŒ–å‡¦ç†ä¸­...")
    start_processing_time = time.time()
    
    try:
        cudf_df, detailed_timing = postgresql_to_cudf_parquet(
            raw_dev=raw_dev,
            columns=columns,
            ncols=ncols,
            header_size=header_size,
            output_path=OUTPUT_PARQUET_PATH,
            compression='snappy',
            use_rmm=True,
            optimize_gpu=True
        )
        
        processing_time = time.time() - start_processing_time
        rows = len(cudf_df)
        parse_time = detailed_timing.get('gpu_parsing', 0)
        decode_time = detailed_timing.get('cudf_creation', 0)
        write_time = detailed_timing.get('parquet_export', 0)
        
        print(f"GPUæœ€é©åŒ–å‡¦ç†å®Œäº† ({processing_time:.4f}ç§’), è¡Œæ•°: {rows}")
        
    except Exception as e:
        print(f"GPUæœ€é©åŒ–å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return

    total_time = time.time() - start_total_time
    decimal_cols = sum(1 for col in columns if col.arrow_id == 5)
    
    print(f"\n=== ç©¶æ¥µæœ€é©åŒ–ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ===")
    print(f"ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ï¼ˆè©³ç´°ï¼‰ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—       : {meta_time:.4f} ç§’")
    print(f"  COPYâ†’ç©¶æ¥µæœ€é©åŒ–è»¢é€  : {copy_time:.4f} ç§’")
    print(f"    â”œâ”€ libpqèª­ã¿å–ã‚Š   : {total_read_time:.4f} ç§’")
    print(f"    â”œâ”€ pinned memcpy   : {total_memcpy_time:.4f} ç§’")
    print(f"    â””â”€ éåŒæœŸDMA       : {total_async_time:.4f} ç§’")
    print(f"  GPUãƒ‘ãƒ¼ã‚¹           : {parse_time:.4f} ç§’")
    print(f"  GPUãƒ‡ã‚³ãƒ¼ãƒ‰         : {decode_time:.4f} ç§’")
    print(f"  Parquetæ›¸ãè¾¼ã¿     : {write_time:.4f} ç§’")
    print("--- çµ±è¨ˆæƒ…å ± ---")
    print(f"  å‡¦ç†è¡Œæ•°      : {rows:,} è¡Œ")
    print(f"  å‡¦ç†åˆ—æ•°      : {len(columns)} åˆ—")
    print(f"  Decimalåˆ—æ•°   : {decimal_cols} åˆ—")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º  : {actual_data_size / (1024*1024):.2f} MB")
    print(f"  å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
    print(f"  å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {(actual_data_size / chunk_count) / (1024*1024):.2f} MB")
    
    total_cells = rows * len(columns)
    throughput = total_cells / decode_time if decode_time > 0 else 0
    network_throughput = actual_data_size / (1024*1024) / copy_time
    
    # æœ€é©åŒ–åŠ¹æœã®è©³ç´°åˆ†æ
    libpq_efficiency = actual_data_size / (1024*1024) / total_read_time if total_read_time > 0 else 0
    memcpy_efficiency = actual_data_size / (1024*1024) / total_memcpy_time if total_memcpy_time > 0 else 0
    dma_efficiency = actual_data_size / (1024*1024) / total_async_time if total_async_time > 0 else 0
    
    print(f"  ã‚»ãƒ«å‡¦ç†é€Ÿåº¦     : {throughput:,.0f} cells/sec")
    print(f"  ç·åˆè»¢é€é€Ÿåº¦     : {network_throughput:.2f} MB/sec")
    print(f"  libpqèª­ã¿å–ã‚Šé€Ÿåº¦: {libpq_efficiency:.2f} MB/sec")
    print(f"  pinned memcpyé€Ÿåº¦: {memcpy_efficiency:.2f} MB/sec")
    print(f"  éåŒæœŸDMAé€Ÿåº¦    : {dma_efficiency:.2f} MB/sec")
    
    # PCIeåŠ¹ç‡è¨ˆç®—ï¼ˆRTX 3090: 22GB/så®ŸåŠ¹ï¼‰
    pcie_efficiency = network_throughput / 22000 * 100
    print(f"  PCIeåŠ¹ç‡        : {pcie_efficiency:.1f}% (å¯¾22GB/så®ŸåŠ¹)")
    
    print("--- ç©¶æ¥µæœ€é©åŒ–åŠ¹æœ ---")
    print("  âœ… ãƒ•ã‚¡ã‚¤ãƒ« I/O: å®Œå…¨ã‚¼ãƒ­")
    print("  âœ… å¤§ããªãƒãƒ£ãƒ³ã‚¯: libpqã‹ã‚‰4MBå˜ä½èª­ã¿å–ã‚Š")
    print("  âœ… åŠ¹ç‡çš„memcpy: NumPyæœ€é©åŒ–ã‚³ãƒ”ãƒ¼")
    print("  âœ… è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ : pipelinedéåŒæœŸDMA")
    print("  âœ… 64MiB pinned: å¤§å®¹é‡ãƒãƒƒãƒ•ã‚¡ã§åŠ¹ç‡æœ€å¤§åŒ–")
    print("  âœ… CPUä½¿ç”¨ç‡: æœ€å°åŒ–ï¼ˆnvtopç¢ºèªæ¨å¥¨ï¼‰")
    
    if network_throughput > 5000:
        print("  ğŸ† 5GB/sè¶…é”æˆï¼")
    elif network_throughput > 1000:
        print("  ğŸ¥‡ 1GB/sè¶…é”æˆï¼")
    else:
        print("  âš ï¸  è»¢é€é€Ÿåº¦ãŒæœŸå¾…å€¤ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™")
    
    print("=========================================")

    # æ¤œè¨¼ç”¨å‡ºåŠ›
    print(f"\ncuDFæ¤œè¨¼ç”¨å‡ºåŠ›:")
    try:
        print(f"å‡ºåŠ›Parquet: {OUTPUT_PARQUET_PATH}")
        print(f"èª­ã¿è¾¼ã¿ç¢ºèª: {len(cudf_df):,} è¡Œ Ã— {len(cudf_df.columns)} åˆ—")
        print("å…ˆé ­ãƒ‡ãƒ¼ã‚¿å‹:")
        for i, (col_name, dtype) in enumerate(cudf_df.dtypes.items()):
            if i < 5:  # æœ€åˆã®5åˆ—ã®ã¿
                print(f"  {col_name}: {dtype}")
        print("âœ… cuDFæ¤œè¨¼: æˆåŠŸ")
    except Exception as e:
        print(f"âŒ cuDFæ¤œè¨¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU ç©¶æ¥µæœ€é©åŒ–ç‰ˆ')
    parser.add_argument('--rows', type=int, default=1000000, help='å‡¦ç†è¡Œæ•°åˆ¶é™')
    parser.add_argument('--chunk-size', type=int, default=64, help='ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º(MB)')
    parser.add_argument('--read-size', type=int, default=4, help='èª­ã¿å–ã‚Šã‚µã‚¤ã‚º(MB)')
    parser.add_argument('--streams', type=int, default=2, help='ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°')
    parser.add_argument('--bandwidth-test', action='store_true', help='å¸¯åŸŸãƒ†ã‚¹ãƒˆã®ã¿')
    
    args = parser.parse_args()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    global CHUNK_SIZE, READ_SIZE, NUM_STREAMS
    CHUNK_SIZE = args.chunk_size * 1024 * 1024
    READ_SIZE = args.read_size * 1024 * 1024
    NUM_STREAMS = args.streams
    
    try:
        cuda.current_context()
        print("âœ… CUDA context OK")
    except Exception as e:
        print(f"âŒ CUDAåˆæœŸåŒ–å¤±æ•—: {e}")
        exit(1)
    
    if args.bandwidth_test:
        run_bandwidth_test()
        return
    
    run_ultimate_optimized_benchmark(limit_rows=args.rows)

if __name__ == "__main__":
    main()