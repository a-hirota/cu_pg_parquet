"""
Benchmark: PostgreSQL â†’ COPY BINARY â†’ GPU 2â€‘pass â†’ Arrow RecordBatch â†’ Parquet

ç’°å¢ƒå¤‰æ•°
--------
GPUPASER_PG_DSN  : psycopg2.connect äº’æ› DSN æ–‡å­—åˆ—
PG_TABLE_PREFIX  : lineorder ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚¹ã‚­ãƒ¼ãƒåä»˜ãã®å ´åˆã«æŒ‡å®š (optional)

å‡¦ç†å†…å®¹
----------
lineorder ãƒ†ãƒ¼ãƒ–ãƒ« (ç´„500ä¸‡è¡Œã‚’æƒ³å®š) ã‚’
1. COPY BINARY ã§å–å¾—
2. GPU ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ decode_chunk ã§ Arrow RecordBatch ã«å¤‰æ›
3. å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬
4. çµæœã‚’ Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
5. å‡ºåŠ›ã•ã‚ŒãŸ Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ cuDF ã§èª­ã¿è¾¼ã¿ã€åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤ºã—ã¦æ¤œè¨¼
"""

import os
import time
import numpy as np
import psycopg
import pyarrow as pa
import pyarrow.parquet as pq
from numba import cuda

# cuDFã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import cudf
    CUDF_AVAILABLE = True
except ImportError:
    cudf = None
    CUDF_AVAILABLE = False

# Import necessary functions from the correct modules using absolute paths from root
from src.meta_fetch import fetch_column_meta, ColumnMeta # Import ColumnMeta as well
from src.gpu_parse_wrapper import parse_binary_chunk_gpu, detect_pg_header_size
from src.gpu_decoder_v2 import decode_chunk, decode_chunk_cudf_optimized
# Remove CPU row start calculator import
# from test.test_single_row_pg_parser import calculate_row_starts_cpu


TABLE_NAME = "lineorder"
OUTPUT_PARQUET_PATH = "benchmark/lineorder_5m.output.parquet" # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

def run_benchmark():
    # ç’°å¢ƒå¤‰æ•°ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    print(f"=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šãƒ‡ãƒãƒƒã‚° ===")
    print(f"ç’°å¢ƒå¤‰æ•° USE_CUDF_PIPELINE: '{os.environ.get('USE_CUDF_PIPELINE', 'NOT_SET')}'")
    
    # cuDFæœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½¿ç”¨ãƒ•ãƒ©ã‚°
    use_cudf_pipeline = os.environ.get("USE_CUDF_PIPELINE", "0") == "1"
    print(f"use_cudf_pipeline ãƒ•ãƒ©ã‚°: {use_cudf_pipeline}")
    
    # cuDFãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    try:
        import cudf
        cudf_available = True
        print(f"cuDF ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cudf.__version__}")
    except ImportError:
        cudf_available = False
        print("cuDF: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼")
        if use_cudf_pipeline:
            print("è­¦å‘Š: cuDFãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚PyArrowæ¨™æº–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
            use_cudf_pipeline = False
    
    print(f"ğŸ“Š ä½¿ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {'ğŸš€ cuDFæœ€é©åŒ–' if use_cudf_pipeline else 'ğŸ“ˆ PyArrowæ¨™æº–'}")
    print(f"cuDFåˆ©ç”¨å¯èƒ½: {cudf_available}")
    print("===============================\n")
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    prefix = os.environ.get("PG_TABLE_PREFIX", "")
    tbl = f"{prefix}{TABLE_NAME}" if prefix else TABLE_NAME

    print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: ãƒ†ãƒ¼ãƒ–ãƒ«={tbl}")
    start_total_time = time.time()

    conn = psycopg.connect(dsn)
    try:
        # -------------------------------
        # ColumnMeta
        # -------------------------------
        print("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        start_meta_time = time.time()
        columns = fetch_column_meta(conn, f"SELECT * FROM {tbl}")
        meta_time = time.time() - start_meta_time
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")

        # --- Add debug print for column metadata ---
        print("\n--- Column Metadata ---")
        for col in columns:
            print(f"Name: {col.name}, OID: {col.pg_oid}, Typmod: {col.pg_typmod}, Arrow ID: {col.arrow_id}, Elem Size: {col.elem_size}, Arrow Param: {col.arrow_param}")
        print("-----------------------\n")
        # --- End debug print ---
        
        ncols = len(columns)

        # -------------------------------
        # COPY BINARY chunk
        # -------------------------------
        print("COPY BINARY ã‚’å®Ÿè¡Œä¸­ (LIMIT 1,000,000)...") # Add limit info
        start_copy_time = time.time()
        # æ³¨æ„: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ãŸã‚ã€å·¨å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã¯ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ã‚ã‚Š
        limit_rows = 1000000
        copy_sql = f"COPY (SELECT * FROM {tbl} LIMIT {limit_rows}) TO STDOUT (FORMAT binary)" # Add LIMIT clause
        buf = bytearray()
        with conn.cursor().copy(copy_sql) as cpy:
            while True:
                chunk = cpy.read()
                if not chunk:
                    break
                buf.extend(chunk)
            raw_host = np.frombuffer(buf, dtype=np.uint8)
        copy_time = time.time() - start_copy_time
        print(f"COPY BINARY å®Œäº† ({copy_time:.4f}ç§’), ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(raw_host) / (1024*1024):.2f} MB")

    finally:
        conn.close()

    # -------------------------------
    # GPU å‡¦ç†
    # -------------------------------
    print("GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ä¸­...")
    start_transfer_time = time.time()
    raw_dev = cuda.to_device(raw_host)
    transfer_time = time.time() - start_transfer_time
    print(f"GPUè»¢é€å®Œäº† ({transfer_time:.4f}ç§’)")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºã‚’æ¤œå‡º
    header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
    header_size = detect_pg_header_size(header_sample)
    print(f"æ¤œå‡ºã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º: {header_size} ãƒã‚¤ãƒˆ")

    # GPU Parse (includes row counting, offset calculation, field parsing)
    print("GPUã§ãƒ‘ãƒ¼ã‚¹ä¸­ (è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆã€ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—ã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è§£æ)...")
    start_parse_time = time.time()
    # Call the updated wrapper which now handles row counting and offset calculation internally
    field_offsets_dev, field_lengths_dev = parse_binary_chunk_gpu(
        raw_dev,
        ncols=ncols,
        header_size=header_size
        # use_gpu_row_detection parameter is now removed
        # rows and row_start_positions are no longer passed
    )
    parse_time = time.time() - start_parse_time
    # The row count is now determined inside parse_binary_chunk_gpu
    rows = field_offsets_dev.shape[0] # Get actual row count from output array shape
    print(f"GPUãƒ‘ãƒ¼ã‚¹å®Œäº† ({parse_time:.4f}ç§’), è¡Œæ•°: {rows}")

    if use_cudf_pipeline:
        # cuDFæœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: GPUä¸Šã§ç›´æ¥Parquetå‡ºåŠ›
        print("\nğŸš€ ===== cuDFæœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ =====")
        print("cuDFæœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ä¸­ (Pass 1 & 2 â†’ GPU-direct Parquet)...")
        start_decode_time = time.time()
        # cuDFæœ€é©åŒ–ç‰ˆã¯ç›´æ¥Parquetãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
        result_df = decode_chunk_cudf_optimized(
            raw_dev, field_offsets_dev, field_lengths_dev, columns,
            output_path=OUTPUT_PARQUET_PATH
        )
        decode_time = time.time() - start_decode_time
        print(f"ğŸš€ cuDF GPUãƒ‡ã‚³ãƒ¼ãƒ‰+Parquetå‡ºåŠ›å®Œäº† ({decode_time:.4f}ç§’)")
        print("========================================\n")
        
        # cuDFãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯çµ±åˆå‡¦ç†ã®ãŸã‚ã€write_timeã¯0ã«è¨­å®š
        write_time = 0.0
        
    else:
        # å¾“æ¥ã®PyArrowãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        print("\nğŸ“ˆ ===== PyArrowæ¨™æº–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ =====")
        print("PyArrowæ¨™æº–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ä¸­ (Pass 1 & 2)...")
        start_decode_time = time.time()
        # decode_chunk ã¯å˜ä¸€ã® RecordBatch ã‚’è¿”ã™æƒ³å®š
        batch = decode_chunk(raw_dev, field_offsets_dev, field_lengths_dev, columns)
        decode_time = time.time() - start_decode_time
        print(f"ğŸ“ˆ PyArrow GPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† ({decode_time:.4f}ç§’)")

        # Arrow Table ã«å¤‰æ› (è¤‡æ•°ãƒãƒƒãƒã®å ´åˆã¯çµåˆãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯å˜ä¸€ãƒãƒƒãƒã¨ä»®å®š)
        result_table = pa.Table.from_batches([batch])
        print(f"Arrow Table ä½œæˆå®Œäº†: {result_table.num_rows} è¡Œ, {result_table.num_columns} åˆ—")

        # -------------------------------
        # Parquet å‡ºåŠ›
        # -------------------------------
        print(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ä¸­: {OUTPUT_PARQUET_PATH}")
        start_write_time = time.time()
        pq.write_table(result_table, OUTPUT_PARQUET_PATH)
        write_time = time.time() - start_write_time
        print(f"Parquetæ›¸ãè¾¼ã¿å®Œäº† ({write_time:.4f}ç§’)")
        print("==========================================\n")

    total_time = time.time() - start_total_time
    print(f"\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—: {meta_time:.4f} ç§’")
    print(f"  COPY BINARY   : {copy_time:.4f} ç§’")
    print(f"  GPUè»¢é€       : {transfer_time:.4f} ç§’")
    # print(f"  è¡Œæ•°è¨ˆç®—(CPU) : {row_calc_time:.4f} ç§’") # Removed CPU calculation time
    print(f"  GPUãƒ‘ãƒ¼ã‚¹(å« è¡Œæ•°/ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—): {parse_time:.4f} ç§’")
    
    if use_cudf_pipeline:
        print(f"  cuDF GPUãƒ‡ã‚³ãƒ¼ãƒ‰+Parquetå‡ºåŠ›: {decode_time:.4f} ç§’")
    else:
        print(f"  GPUãƒ‡ã‚³ãƒ¼ãƒ‰   : {decode_time:.4f} ç§’")
        print(f"  Parquetæ›¸ãè¾¼ã¿: {write_time:.4f} ç§’")
    print("----------------")


    # -------------------------------
    # cuDF ã§ã®æ¤œè¨¼
    # -------------------------------
    print(f"\ncuDFã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¤œè¨¼ä¸­: {OUTPUT_PARQUET_PATH}")
    try:
        start_cudf_read_time = time.time()
        gdf = cudf.read_parquet(OUTPUT_PARQUET_PATH)
        cudf_read_time = time.time() - start_cudf_read_time
        print(f"cuDFèª­ã¿è¾¼ã¿å®Œäº† ({cudf_read_time:.4f}ç§’)")
        print("--- cuDF DataFrame Info ---")
        gdf.info()
        print("\n--- cuDF DataFrame Head ---")
        print(gdf.head())
        print("-------------------------")
        print("cuDFã§ã®èª­ã¿è¾¼ã¿æ¤œè¨¼: æˆåŠŸ")
    except Exception as e:
        print(f"cuDFã§ã®èª­ã¿è¾¼ã¿æ¤œè¨¼: å¤±æ•— - {e}")

if __name__ == "__main__":
    # CUDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
    try:
        cuda.current_context()
        print("CUDA context OK")
    except Exception as e:
        print(f"CUDA context initialization failed: {e}")
        exit(1)
    run_benchmark()
