"""
PostgreSQL â†’ GPU çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ç‰ˆ
joinã‚‚æ’é™¤ã—ãŸå®Œå…¨ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å®Ÿè£…

æœ€é©åŒ–:
- b''.join() ã®æ’é™¤ï¼ˆãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼å®Œå…¨å›é¿ï¼‰
- ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ç›´æ¥GPUè»¢é€
- 22GB RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
- çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å®Ÿè£…

ç’°å¢ƒå¤‰æ•°:
GPUPASER_PG_DSN  : PostgreSQLæ¥ç¶šæ–‡å­—åˆ—
"""

import os
import time
import ctypes
import psycopg
import rmm
import numpy as np
import numba
from numba import cuda
import cupy as cp
import argparse

from src.metadata import fetch_column_meta
from src.cuda_kernels.postgresql_binary_parser import detect_pg_header_size
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet

TABLE_NAME = "lineorder"
OUTPUT_PARQUET_PATH = "benchmark/lineorder_true_zero_copy.output.parquet"

def run_bandwidth_test():
    """CUDA bandwidthTest ã§PCIeæ€§èƒ½ç¢ºèª"""
    print("\n=== PCIeå¸¯åŸŸå¹…ãƒ†ã‚¹ãƒˆ ===")
    
    try:
        print("ã‚·ãƒ³ãƒ—ãƒ«å¸¯åŸŸæ¸¬å®šå®Ÿè¡Œä¸­...")
        size_mb = 1024  # 1GB
        host_data = np.random.randint(0, 256, size_mb * 1024 * 1024, dtype=np.uint8)
        
        # Host to Device
        start_time = time.time()
        device_data = cuda.to_device(host_data)
        htod_time = time.time() - start_time
        htod_speed = size_mb / htod_time
        
        # Device to Host  
        start_time = time.time()
        result_data = device_data.copy_to_host()
        dtoh_time = time.time() - start_time
        dtoh_speed = size_mb / dtoh_time
        
        print(f"Hostâ†’Device: {htod_speed:.2f} MB/s")
        print(f"Deviceâ†’Host: {dtoh_speed:.2f} MB/s")
        
    except Exception as e:
        print(f"å¸¯åŸŸæ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")

def run_true_zero_copy_benchmark(limit_rows=1000000):
    """çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ - joinãªã—ã€ãƒãƒ£ãƒ³ã‚¯ç›´æ¥è»¢é€"""
    dsn = os.environ.get("GPUPASER_PG_DSN")
    if not dsn:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° GPUPASER_PG_DSN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    print(f"=== PostgreSQL â†’ GPU çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ç‰ˆ ===")
    print(f"ãƒ†ãƒ¼ãƒ–ãƒ«: {TABLE_NAME}")
    print(f"è¡Œæ•°åˆ¶é™: {limit_rows:,}")
    print(f"æœ€é©åŒ–è¨­å®š:")
    print(f"  ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼: å®Œå…¨ã‚¼ãƒ­ (joinæ’é™¤)")
    print(f"  è»¢é€æ–¹å¼: ãƒãƒ£ãƒ³ã‚¯ç›´æ¥GPUè»¢é€")
    print(f"  ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«: 22GB")
    
    # PCIeå¸¯åŸŸãƒ†ã‚¹ãƒˆ
    run_bandwidth_test()
    
    # RMM 22GBåˆæœŸåŒ–
    try:
        if not rmm.is_initialized():
            rmm.reinitialize(
                pool_allocator=True, 
                initial_pool_size=22*1024**3  # 22GB
            )
            print("âœ… RMM ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–å®Œäº† (22GB)")
    except Exception as e:
        print(f"âŒ RMMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return

    start_total_time = time.time()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    conn = psycopg.connect(dsn)
    try:
        print("\nãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        start_meta_time = time.time()
        columns = fetch_column_meta(conn, f"SELECT * FROM {TABLE_NAME}")
        meta_time = time.time() - start_meta_time
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº† ({meta_time:.4f}ç§’)")
        ncols = len(columns)

        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæ¨å®š
        rows_est = limit_rows
        row_bytes = 17*8 + 17*4
        header_est = 19
        estimated_size = header_est + rows_est * row_bytes
        print(f"æ¨å®šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {estimated_size / (1024*1024):.2f} MB")

        # GPU ãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿
        print("GPU ãƒãƒƒãƒ•ã‚¡äº‹å‰ç¢ºä¿ä¸­...")
        devbuf = rmm.DeviceBuffer(size=estimated_size)
        print(f"GPU ãƒãƒƒãƒ•ã‚¡ç¢ºä¿å®Œäº†: {estimated_size / (1024*1024):.2f} MB")

        # COPY BINARY â†’ ãƒãƒ£ãƒ³ã‚¯ç›´æ¥GPUè»¢é€ (çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼)
        print("COPY BINARY â†’ ãƒãƒ£ãƒ³ã‚¯ç›´æ¥GPUè»¢é€å®Ÿè¡Œä¸­...")
        start_copy_time = time.time()
        
        copy_sql = f"COPY (SELECT * FROM {TABLE_NAME} LIMIT {limit_rows}) TO STDOUT (FORMAT binary)"
        
        offset = 0
        chunk_count = 0
        total_gpu_time = 0
        
        print("  ğŸš€ çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼: ãƒãƒ£ãƒ³ã‚¯â†’GPUç›´æ¥è»¢é€é–‹å§‹...")
        
        with conn.cursor() as cur:
            with cur.copy(copy_sql) as copy_obj:
                for chunk in copy_obj:
                    if not chunk:
                        break
                    
                    chunk_size = len(chunk)
                    
                    # ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ ãƒã‚§ãƒƒã‚¯
                    if offset + chunk_size > devbuf.size:
                        print(f"âš ï¸  è­¦å‘Š: GPUãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºä¸è¶³")
                        break
                    
                    # ã€çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€‘ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥GPUè»¢é€
                    start_gpu = time.time()
                    
                    # memoryview â†’ byteså¤‰æ›ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼çš„ï¼‰
                    if isinstance(chunk, memoryview):
                        chunk_bytes = chunk.tobytes()
                    else:
                        chunk_bytes = bytes(chunk)
                    
                    # cupy.cuda.runtime.memcpy ã§ãƒ›ã‚¹ãƒˆâ†’ãƒ‡ãƒã‚¤ã‚¹ç›´æ¥ã‚³ãƒ”ãƒ¼
                    src_ptr = ctypes.cast(ctypes.c_char_p(chunk_bytes), ctypes.c_void_p).value
                    dst_ptr = devbuf.ptr + offset
                    
                    cp.cuda.runtime.memcpy(
                        dst_ptr, src_ptr, chunk_size,
                        cp.cuda.runtime.memcpyHostToDevice
                    )
                    
                    gpu_time = time.time() - start_gpu
                    total_gpu_time += gpu_time
                    
                    offset += chunk_size
                    chunk_count += 1
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆå¤§å¹…å‰Šæ¸›ï¼‰
                    if chunk_count % 50000 == 0:
                        print(f"    ãƒãƒ£ãƒ³ã‚¯ {chunk_count:,} | {offset / (1024*1024):.0f} MB")

        copy_time = time.time() - start_copy_time
        actual_data_size = offset
        
        print(f"âœ… COPY BINARY â†’ çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€å®Œäº† ({copy_time:.4f}ç§’)")
        print(f"  å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
        print(f"  å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {actual_data_size / (1024*1024):.2f} MB")
        print(f"  ç·åˆè»¢é€é€Ÿåº¦: {actual_data_size / (1024*1024) / copy_time:.2f} MB/sec")
        print(f"  GPUè»¢é€æ™‚é–“: {total_gpu_time:.4f} ç§’")
        print(f"  GPUè»¢é€é€Ÿåº¦: {actual_data_size / (1024*1024) / total_gpu_time:.2f} MB/sec")

    finally:
        conn.close()

    # GPU ãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
    if actual_data_size < devbuf.size:
        print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ä¸­...")
        trimmed_devbuf = rmm.DeviceBuffer(size=actual_data_size)
        cp.cuda.runtime.memcpy(
            trimmed_devbuf.ptr, devbuf.ptr, actual_data_size,
            cp.cuda.runtime.memcpyDeviceToDevice
        )
        devbuf = trimmed_devbuf
        print(f"GPU ãƒãƒƒãƒ•ã‚¡ãƒˆãƒªãƒŸãƒ³ã‚°å®Œäº†: {actual_data_size / (1024*1024):.2f} MB")

    # numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›
    print("GPU ãƒãƒƒãƒ•ã‚¡ã‚’ numba GPU ã‚¢ãƒ¬ã‚¤ã«å¤‰æ›ä¸­...")
    raw_dev = cuda.as_cuda_array(devbuf).view(dtype=np.uint8)
    print(f"GPU ã‚¢ãƒ¬ã‚¤å¤‰æ›å®Œäº†: {raw_dev.shape[0]:,} bytes")

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
    header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
    header_size = detect_pg_header_size(header_sample)
    print(f"ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º: {header_size} ãƒã‚¤ãƒˆ")

    # GPUæœ€é©åŒ–å‡¦ç†
    print("GPUæœ€é©åŒ–å‡¦ç†ä¸­...")
    start_processing_time = time.time()
    
    try:
        cudf_df, detailed_timing = postgresql_to_cudf_parquet(
            raw_dev=raw_dev,
            columns=columns,
            ncols=ncols,
            header_size=header_size,
            output_path=OUTPUT_PARQUET_PATH,
            compression='snappy',
            use_rmm=True,
            optimize_gpu=True
        )
        
        processing_time = time.time() - start_processing_time
        rows = len(cudf_df)
        parse_time = detailed_timing.get('gpu_parsing', 0)
        decode_time = detailed_timing.get('cudf_creation', 0)
        write_time = detailed_timing.get('parquet_export', 0)
        
        print(f"GPUæœ€é©åŒ–å‡¦ç†å®Œäº† ({processing_time:.4f}ç§’), è¡Œæ•°: {rows}")
        
    except Exception as e:
        print(f"GPUæœ€é©åŒ–å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
        return

    total_time = time.time() - start_total_time
    decimal_cols = sum(1 for col in columns if col.arrow_id == 5)
    
    print(f"\n=== çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ===")
    print(f"ç·æ™‚é–“ = {total_time:.4f} ç§’")
    print("--- æ™‚é–“å†…è¨³ ---")
    print(f"  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—       : {meta_time:.4f} ç§’")
    print(f"  COPYâ†’ç›´æ¥GPUè»¢é€    : {copy_time:.4f} ç§’")
    print(f"    â””â”€ GPUè»¢é€æ™‚é–“    : {total_gpu_time:.4f} ç§’")
    print(f"  GPUãƒ‘ãƒ¼ã‚¹           : {parse_time:.4f} ç§’")
    print(f"  GPUãƒ‡ã‚³ãƒ¼ãƒ‰         : {decode_time:.4f} ç§’")
    print(f"  Parquetæ›¸ãè¾¼ã¿     : {write_time:.4f} ç§’")
    print("--- çµ±è¨ˆæƒ…å ± ---")
    print(f"  å‡¦ç†è¡Œæ•°      : {rows:,} è¡Œ")
    print(f"  å‡¦ç†åˆ—æ•°      : {len(columns)} åˆ—")
    print(f"  Decimalåˆ—æ•°   : {decimal_cols} åˆ—")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º  : {actual_data_size / (1024*1024):.2f} MB")
    print(f"  å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {chunk_count:,}")
    
    total_cells = rows * len(columns)
    throughput = total_cells / decode_time if decode_time > 0 else 0
    network_throughput = actual_data_size / (1024*1024) / copy_time
    gpu_transfer_speed = actual_data_size / (1024*1024) / total_gpu_time
    
    print(f"  ã‚»ãƒ«å‡¦ç†é€Ÿåº¦     : {throughput:,.0f} cells/sec")
    print(f"  ç·åˆè»¢é€é€Ÿåº¦     : {network_throughput:.2f} MB/sec")
    print(f"  GPUè»¢é€é€Ÿåº¦     : {gpu_transfer_speed:.2f} MB/sec")
    
    # PCIeåŠ¹ç‡è¨ˆç®—ï¼ˆå®Ÿæ¸¬11.9GB/såŸºæº–ï¼‰
    pcie_efficiency = network_throughput / 11900 * 100
    print(f"  PCIeåŠ¹ç‡        : {pcie_efficiency:.1f}% (å¯¾11.9GB/så®Ÿæ¸¬)")
    
    print("--- çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æœ€é©åŒ–åŠ¹æœ ---")
    print("  âœ… ãƒ•ã‚¡ã‚¤ãƒ« I/O: å®Œå…¨ã‚¼ãƒ­")
    print("  âœ… ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼: å®Œå…¨ã‚¼ãƒ­ (joinæ’é™¤)")
    print("  âœ… ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: ç›´æ¥GPUè»¢é€")
    print("  âœ… ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«: 22GB (ååˆ†ãªå®¹é‡)")
    print("  âœ… çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼: å®Œå…¨å®Ÿç¾")
    
    # æ€§èƒ½è©•ä¾¡
    if network_throughput > 5000:
        print("  ğŸ† 5GB/sè¶…é”æˆï¼")
    elif network_throughput > 1000:
        print("  ğŸ¥‡ 1GB/sè¶…é”æˆï¼")
    elif network_throughput > 500:
        print("  ğŸ¥ˆ 500MB/sè¶…é”æˆï¼")
    elif network_throughput > 200:
        print("  ğŸ¥‰ 200MB/sè¶…é”æˆï¼")
    else:
        print("  âš ï¸  è»¢é€é€Ÿåº¦æ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
    
    print("=========================================")

    # æ¤œè¨¼ç”¨å‡ºåŠ›
    print(f"\ncuDFæ¤œè¨¼ç”¨å‡ºåŠ›:")
    try:
        print(f"å‡ºåŠ›Parquet: {OUTPUT_PARQUET_PATH}")
        print(f"èª­ã¿è¾¼ã¿ç¢ºèª: {len(cudf_df):,} è¡Œ Ã— {len(cudf_df.columns)} åˆ—")
        print("å…ˆé ­ãƒ‡ãƒ¼ã‚¿å‹:")
        for i, (col_name, dtype) in enumerate(cudf_df.dtypes.items()):
            if i < 3:  # æœ€åˆã®3åˆ—ã®ã¿
                print(f"  {col_name}: {dtype}")
        print("âœ… cuDFæ¤œè¨¼: æˆåŠŸ")
    except Exception as e:
        print(f"âŒ cuDFæ¤œè¨¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU çœŸã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ç‰ˆ')
    parser.add_argument('--rows', type=int, default=1000000, help='å‡¦ç†è¡Œæ•°åˆ¶é™')
    parser.add_argument('--bandwidth-test', action='store_true', help='å¸¯åŸŸãƒ†ã‚¹ãƒˆã®ã¿')
    
    args = parser.parse_args()
    
    try:
        cuda.current_context()
        print("âœ… CUDA context OK")
    except Exception as e:
        print(f"âŒ CUDAåˆæœŸåŒ–å¤±æ•—: {e}")
        exit(1)
    
    if args.bandwidth_test:
        run_bandwidth_test()
        return
    
    run_true_zero_copy_benchmark(limit_rows=args.rows)

if __name__ == "__main__":
    main()