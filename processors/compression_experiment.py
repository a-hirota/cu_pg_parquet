#!/usr/bin/env python3
"""
Parquetåœ§ç¸®æ–¹å¼ã®æ€§èƒ½å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å„åœ§ç¸®æ–¹å¼ï¼ˆsnappyã€gzipã€lz4ã€brotliã€zstdã€noneï¼‰ã§
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
"""

import os
import sys
import json
import time
import subprocess
import shutil
from pathlib import Path
from datetime import datetime
import argparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))


def cleanup_output_dir():
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    output_dir = Path("./output")
    if output_dir.exists():
        parquet_files = list(output_dir.glob("*.parquet"))
        if parquet_files:
            print(f"Cleaning up {len(parquet_files)} parquet files...")
            for f in parquet_files:
                f.unlink()


def run_benchmark(table_name: str, parallel: int, chunks: int, compression: str, timeout: int = 300):
    """å˜ä¸€ã®åœ§ç¸®æ–¹å¼ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
    print(f"\n{'='*80}")
    print(f"Testing compression: {compression}")
    print(f"{'='*80}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_output_dir()
    
    # ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
    cmd = [
        sys.executable,
        "cu_pg_parquet.py",
        "--table", table_name,
        "--parallel", str(parallel),
        "--chunks", str(chunks),
        "--compression", compression,
        "--yes"  # è‡ªå‹•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    ]
    
    # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬
    start_time = time.time()
    
    # gzipã®å ´åˆã¯è­¦å‘Šã‚’è¡¨ç¤º
    if compression == 'gzip':
        print("âš ï¸  Warning: gzip is not GPU-accelerated in cuDF and will use CPU fallback")
        print("   This may take significantly longer than other compression methods")
        print(f"   Timeout set to {timeout} seconds")
    
    try:
        # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            check=True,
            timeout=timeout
        )
        
        elapsed_time = time.time() - start_time
        
        # å‡ºåŠ›ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
        output_lines = result.stdout.split('\n')
        stats = {
            'compression': compression,
            'elapsed_time': elapsed_time,
            'success': True,
            'stdout': result.stdout,
            'stderr': result.stderr
        }
        
        # çµ±è¨ˆæƒ…å ±ã®æŠ½å‡º
        for line in output_lines:
            if 'ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:' in line:
                size_str = line.split(':')[1].strip().split()[0]
                stats['total_size_gb'] = float(size_str)
            elif 'ç·è¡Œæ•°:' in line:
                rows_str = line.split(':')[1].strip().replace(',', '').split()[0]
                stats['total_rows'] = int(rows_str)
            elif 'ç·å®Ÿè¡Œæ™‚é–“:' in line:
                time_str = line.split(':')[1].strip().replace('ç§’', '')
                stats['total_time'] = float(time_str)
            elif 'å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ:' in line:
                throughput_str = line.split(':')[1].strip().split()[0]
                stats['throughput_gb_s'] = float(throughput_str)
        
        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã‚’æ¸¬å®š
        output_dir = Path("./output")
        parquet_files = list(output_dir.glob("*.parquet"))
        if parquet_files:
            total_parquet_size = sum(f.stat().st_size for f in parquet_files)
            stats['parquet_total_size_mb'] = total_parquet_size / (1024**2)
            stats['parquet_file_count'] = len(parquet_files)
            
            # åœ§ç¸®ç‡ã‚’è¨ˆç®—
            if 'total_size_gb' in stats:
                original_size_mb = stats['total_size_gb'] * 1024
                stats['compression_ratio'] = original_size_mb / stats['parquet_total_size_mb']
        
        print(f"\nâœ… Benchmark completed successfully")
        print(f"Elapsed time: {elapsed_time:.2f} seconds")
        if 'parquet_total_size_mb' in stats:
            print(f"Parquet size: {stats['parquet_total_size_mb']:.2f} MB")
            if 'compression_ratio' in stats:
                print(f"Compression ratio: {stats['compression_ratio']:.2f}x")
        
        return stats
        
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ Benchmark failed for {compression}")
        print(f"Error: {e}")
        if e.stderr:
            print(f"Stderr: {e.stderr}")
        
        return {
            'compression': compression,
            'elapsed_time': time.time() - start_time,
            'success': False,
            'error': str(e),
            'stdout': e.stdout if e.stdout else '',
            'stderr': e.stderr if e.stderr else ''
        }
    except subprocess.TimeoutExpired as e:
        print(f"\nâ±ï¸  Benchmark timed out after {timeout} seconds for {compression}")
        print("   This is expected for gzip compression with large datasets")
        
        return {
            'compression': compression,
            'elapsed_time': timeout,
            'success': False,
            'error': f'Timeout after {timeout} seconds',
            'timeout': True
        }
    except Exception as e:
        print(f"\nâŒ Unexpected error for {compression}: {e}")
        return {
            'compression': compression,
            'elapsed_time': time.time() - start_time,
            'success': False,
            'error': str(e)
        }


def save_results(results: list, output_file: str):
    """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nResults saved to: {output_file}")


def print_summary(results: list):
    """çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print(f"\n{'='*80}")
    print("COMPRESSION BENCHMARK SUMMARY")
    print(f"{'='*80}")
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    print(f"{'Compression':<12} {'Time (s)':<10} {'Size (MB)':<12} {'Ratio':<8} {'Throughput':<12} {'Status'}")
    print("-" * 80)
    
    # å„çµæœã‚’è¡¨ç¤º
    for r in results:
        if r['success']:
            time_str = f"{r.get('total_time', r['elapsed_time']):.2f}"
            size_str = f"{r.get('parquet_total_size_mb', 0):.2f}"
            ratio_str = f"{r.get('compression_ratio', 0):.2f}x"
            throughput_str = f"{r.get('throughput_gb_s', 0):.2f} GB/s"
            status = "âœ…"
        else:
            time_str = f"{r['elapsed_time']:.2f}"
            size_str = "N/A"
            ratio_str = "N/A"
            throughput_str = "N/A"
            status = "âŒ"
        
        print(f"{r['compression']:<12} {time_str:<10} {size_str:<12} {ratio_str:<8} {throughput_str:<12} {status}")
    
    print("-" * 80)
    
    # æœ€é©ãªåœ§ç¸®æ–¹å¼ã‚’ç‰¹å®š
    successful_results = [r for r in results if r['success']]
    if successful_results:
        # é€Ÿåº¦æœ€å„ªå…ˆ
        fastest = min(successful_results, key=lambda x: x.get('total_time', x['elapsed_time']))
        print(f"\nğŸš€ Fastest: {fastest['compression']} ({fastest.get('total_time', fastest['elapsed_time']):.2f}s)")
        
        # åœ§ç¸®ç‡æœ€å„ªå…ˆ
        if any('compression_ratio' in r for r in successful_results):
            best_ratio = max(successful_results, key=lambda x: x.get('compression_ratio', 0))
            print(f"ğŸ“¦ Best compression: {best_ratio['compression']} ({best_ratio.get('compression_ratio', 0):.2f}x)")
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å„ªå…ˆ
        if any('throughput_gb_s' in r for r in successful_results):
            best_throughput = max(successful_results, key=lambda x: x.get('throughput_gb_s', 0))
            print(f"âš¡ Best throughput: {best_throughput['compression']} ({best_throughput.get('throughput_gb_s', 0):.2f} GB/s)")


def main():
    parser = argparse.ArgumentParser(
        description="Parquetåœ§ç¸®æ–¹å¼ã®æ€§èƒ½å®Ÿé¨“"
    )
    parser.add_argument(
        "--table",
        type=str,
        default="lineorder",
        help="å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: lineorderï¼‰"
    )
    parser.add_argument(
        "--parallel",
        type=int,
        default=16,
        help="ä¸¦åˆ—æ¥ç¶šæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 16ï¼‰"
    )
    parser.add_argument(
        "--chunks",
        type=int,
        default=16,
        help="ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 16ï¼‰"
    )
    parser.add_argument(
        "--compressions",
        type=str,
        nargs='+',
        default=["snappy", "lz4", "zstd", "none"],
        help="ãƒ†ã‚¹ãƒˆã™ã‚‹åœ§ç¸®æ–¹å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: snappy lz4 zstd noneï¼‰â€»gzipã¯éæ¨å¥¨"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆ"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=300,
        help="å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ç§’ï¼‰"
    )
    parser.add_argument(
        "--include-gzip",
        action="store_true",
        help="gzipåœ§ç¸®ã‚’ãƒ†ã‚¹ãƒˆã«å«ã‚ã‚‹ï¼ˆéæ¨å¥¨ã€éå¸¸ã«é…ã„ï¼‰"
    )
    
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
    if "GPUPASER_PG_DSN" not in os.environ:
        print("âŒ Error: GPUPASER_PG_DSN environment variable not set")
        print("Example: export GPUPASER_PG_DSN=\"dbname=postgres user=postgres host=localhost port=5432\"")
        return 1
    
    # gzipã‚’å«ã‚ã‚‹å ´åˆã®å‡¦ç†
    compressions = args.compressions.copy()
    if args.include_gzip and 'gzip' not in compressions:
        compressions.append('gzip')
        print("\nâš ï¸  Note: gzip compression included. This will be MUCH slower than other methods.")
    
    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    results = []
    
    # å„åœ§ç¸®æ–¹å¼ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
    for compression in compressions:
        stats = run_benchmark(
            table_name=args.table,
            parallel=args.parallel,
            chunks=args.chunks,
            compression=compression,
            timeout=args.timeout
        )
        results.append(stats)
        
        # å°‘ã—å¾…æ©Ÿï¼ˆGPUã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
        if compression != args.compressions[-1]:
            print("\nWaiting 5 seconds for GPU cooldown...")
            time.sleep(5)
    
    # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print_summary(results)
    
    # çµæœã‚’ä¿å­˜
    if args.output:
        output_file = args.output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"compression_benchmark_{timestamp}.json"
    
    save_results(results, output_file)
    
    return 0


if __name__ == "__main__":
    exit(main())