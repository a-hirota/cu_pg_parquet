"""
PostgreSQL â†’ Rust â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆ
Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã§çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
"""

import os
import time
import subprocess
import json
import numpy as np
from numba import cuda
import rmm
import cudf
import cupy as cp
import kvikio
from pathlib import Path
from typing import List, Dict, Any, Optional
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys
import pandas as pd

from src.types import ColumnMeta, PG_OID_TO_ARROW, UNKNOWN
from src.main_postgres_to_parquet import postgresql_to_cudf_parquet_direct
from src.cuda_kernels.postgres_binary_parser import detect_pg_header_size
import pyarrow.parquet as pq

TABLE_NAME = "lineorder"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå®Ÿè¡Œæ™‚ã«ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
OUTPUT_DIR = "/dev/shm"

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±º
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)  # benchmarks -> project_root
RUST_BINARY = os.environ.get('GPUPGPARSER_RUST_BINARY') or os.path.join(
    project_root, "rust_pg_binary_extractor/target/release/pg_chunk_extractor"
)

MAX_QUEUE_SIZE = 3  # ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚º

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
chunk_stats = []
gpu_row_counts = {}  # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯IDã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸ï¼‰
shutdown_flag = threading.Event()


def signal_handler(sig, frame):
    """Ctrl+Cãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    print("\n\nâš ï¸  å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ã„ã¾ã™...")
    shutdown_flag.set()
    sys.exit(1)


signal.signal(signal.SIGINT, signal_handler)


def setup_rmm_pool():
    """RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’é©åˆ‡ã«è¨­å®š"""
    try:
        if rmm.is_initialized():
            return
        
        # GPUãƒ¡ãƒ¢ãƒªã®90%ã‚’ä½¿ç”¨å¯èƒ½ã«è¨­å®š
        gpu_memory = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem']
        pool_size = int(gpu_memory * 0.9)
        
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=pool_size,
            maximum_pool_size=pool_size
        )
    except Exception as e:
        print(f"âš ï¸ RMMåˆæœŸåŒ–è­¦å‘Š: {e}")


def convert_rust_metadata_to_column_meta(rust_columns):
    """Rustå‡ºåŠ›ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ColumnMetaã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    # src/types.pyã‹ã‚‰å¿…è¦ãªå®šæ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from src.types import (
        INT16, INT32, INT64, FLOAT32, FLOAT64, DECIMAL128,
        UTF8, BINARY, DATE32, TS64_US, BOOL
    )
    
    columns = []
    for col in rust_columns:
        # Rustå´ã®arrow_typeã‚’æ­£ã—ã„arrow_idã«å¤‰æ›
        arrow_type_map = {
            'int16': (INT16, 2),          # INT16 = 0, elem_size = 2
            'int32': (INT32, 4),          # INT32 = 1, elem_size = 4
            'int64': (INT64, 8),          # INT64 = 2, elem_size = 8
            'float32': (FLOAT32, 4),      # FLOAT32 = 3, elem_size = 4
            'float64': (FLOAT64, 8),      # FLOAT64 = 4, elem_size = 8
            'decimal128': (DECIMAL128, 16), # DECIMAL128 = 5, elem_size = 16
            'string': (UTF8, None),       # UTF8 = 6, elem_size = None (å¯å¤‰é•·)
            'binary': (BINARY, None),     # BINARY = 7, elem_size = None (å¯å¤‰é•·)
            'date32': (DATE32, 4),        # DATE32 = 8, elem_size = 4
            'timestamp[ns]': (TS64_US, 8),       # TS64_US = 9, elem_size = 8
            'timestamp[ns, tz=UTC]': (TS64_US, 8), # TS64_US = 9, elem_size = 8
            'bool': (BOOL, 1),            # BOOL = 10, elem_size = 1
        }
        
        arrow_type = col['arrow_type']
        arrow_id, elem_size = arrow_type_map.get(arrow_type, (UNKNOWN, None))
        
        # elem_sizeãŒNoneã®å ´åˆã¯0ã«å¤‰æ›ï¼ˆå¯å¤‰é•·ã‚’ç¤ºã™ï¼‰
        if elem_size is None:
            elem_size = 0
        
        # arrow_paramã®è¨­å®šï¼ˆDECIMAL128ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼‰
        arrow_param = None
        if arrow_id == DECIMAL128:
            # Rustã‹ã‚‰precision/scaleãŒæ¸¡ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
            # PostgreSQL numericã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯(38, 0)
            arrow_param = (38, 0)
        
        # ColumnMetaã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        meta = ColumnMeta(
            name=col['name'],
            pg_oid=col['pg_oid'],
            pg_typmod=-1,  # Rustã‹ã‚‰å–å¾—ã§ããªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            arrow_id=arrow_id,
            elem_size=elem_size,
            arrow_param=arrow_param
        )
        columns.append(meta)
    
    return columns


def cleanup_files(total_chunks=8, table_name=None):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ä½¿ç”¨
    if table_name is None:
        table_name = TABLE_NAME
    
    # é€šå¸¸ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
    files = [
        f"{OUTPUT_DIR}/{table_name}_meta_0.json",
        f"{OUTPUT_DIR}/{table_name}_data_0.ready"
    ] + [f"{OUTPUT_DIR}/{table_name}_chunk_{i}.bin" for i in range(total_chunks)]
    
    for f in files:
        if os.path.exists(f):
            os.remove(f)
    
    # è¿½åŠ ã®å®‰å…¨å¯¾ç­–: OUTPUT_DIRå†…ã®å…¨ã¦ã®.binãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åãŒä¸€è‡´ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰
    try:
        output_path = Path(OUTPUT_DIR)
        if output_path.exists():
            # ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«é–¢é€£ã™ã‚‹å…¨ã¦ã®.binãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            for bin_file in output_path.glob(f"{table_name}_*.bin"):
                if bin_file.is_file():
                    bin_file.unlink()
    except Exception as e:
        print(f"âš ï¸ è¿½åŠ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã®è­¦å‘Š: {e}")


def rust_producer(chunk_queue: queue.Queue, total_chunks: int, stats_queue: queue.Queue, table_name: str, metadata_queue: queue.Queue):
    """Rustè»¢é€ã‚’å®Ÿè¡Œã™ã‚‹Producerã‚¹ãƒ¬ãƒƒãƒ‰"""
    for chunk_id in range(total_chunks):
        if shutdown_flag.is_set():
            break
            
        try:
            print(f"\n[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}/{total_chunks} Rustè»¢é€é–‹å§‹...")
            
            env = os.environ.copy()
            env['CHUNK_ID'] = str(chunk_id)
            env['TOTAL_CHUNKS'] = str(total_chunks)
            env['TABLE_NAME'] = table_name
            
            rust_start = time.time()
            process = subprocess.run(
                [RUST_BINARY],
                capture_output=True,
                text=True,
                env=env
            )
            
            if process.returncode != 0:
                print(f"âŒ Rustã‚¨ãƒ©ãƒ¼: {process.stderr}")
                continue
            
            # ãƒ‡ãƒãƒƒã‚°: Rustã®å…¨å‡ºåŠ›ã‚’ç¢ºèª
            if os.environ.get("GPUPGPARSER_TEST_MODE") == "1" and chunk_id == 0:
                print(f"[Producer] Rust stdouté•·ã•: {len(process.stdout)}æ–‡å­—")
                print(f"[Producer] Rust stderr: {process.stderr}" if process.stderr else "[Producer] stderrã¯ç©º")
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€Rustã®å‡ºåŠ›ã‚’è¡¨ç¤º
            if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
                for line in process.stdout.split('\n'):
                    if 'ãƒãƒ£ãƒ³ã‚¯' in line or 'ãƒšãƒ¼ã‚¸' in line or 'COPYç¯„å›²' in line:
                        print(f"[Rust Debug] {line}")
                # JSONå‡ºåŠ›ã‚’ç¢ºèª
                if "===CHUNK_RESULT_JSON===" not in process.stdout:
                    print("[Producer] è­¦å‘Š: Rustã‹ã‚‰JSONå‡ºåŠ›ãŒã‚ã‚Šã¾ã›ã‚“")
                    print(f"[Producer] Rustå‡ºåŠ›ã®æœ«å°¾100æ–‡å­—: ...{process.stdout[-100:]}")
            
            # JSONçµæœã‚’æŠ½å‡º
            output = process.stdout
            json_start = output.find("===CHUNK_RESULT_JSON===")
            json_end = output.find("===END_CHUNK_RESULT_JSON===")
            
            # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã§JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            if chunk_id == 0 and json_start == -1 and os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
                print(f"[Producer] Rust stdoutå…¨ä½“:\n{output}")
                print(f"[Producer] JSONæ¤œç´¢çµæœ: start={json_start}, end={json_end}")
            
            if json_start != -1 and json_end != -1:
                json_str = output[json_start + len("===CHUNK_RESULT_JSON==="):json_end].strip()
                result = json.loads(json_str)
                rust_time = result['elapsed_seconds']
                file_size = result['total_bytes']
                chunk_file = result['chunk_file']
                
                # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚æŠ½å‡ºã—ã¦ä¿å­˜
                if chunk_id == 0 and 'columns' in result:
                    metadata_queue.put(result['columns'])
            else:
                rust_time = time.time() - rust_start
                chunk_file = f"{OUTPUT_DIR}/{table_name}_chunk_{chunk_id}.bin"
                file_size = os.path.getsize(chunk_file)
            
            chunk_info = {
                'chunk_id': chunk_id,
                'chunk_file': chunk_file,
                'file_size': file_size,
                'rust_time': rust_time
            }
            
            print(f"[Producer] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} è»¢é€å®Œäº† ({rust_time:.1f}ç§’, {file_size / 1024**3:.1f}GB)")
            
            # ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_queue.put(chunk_info)
            stats_queue.put(('rust_time', rust_time))
            
        except Exception as e:
            print(f"[Producer] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
    chunk_queue.put(None)
    print("[Producer] å…¨ãƒãƒ£ãƒ³ã‚¯è»¢é€å®Œäº†")


def gpu_consumer(chunk_queue: queue.Queue, columns: List[ColumnMeta], consumer_id: int, stats_queue: queue.Queue, total_chunks: int, table_name: str, test_mode: bool = False):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹Consumerã‚¹ãƒ¬ãƒƒãƒ‰"""
    while not shutdown_flag.is_set():
        try:
            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            chunk_info = chunk_queue.get(timeout=1)
            
            if chunk_info is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                break
                
            chunk_id = chunk_info['chunk_id']
            chunk_file = chunk_info['chunk_file']
            file_size = chunk_info['file_size']
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†é–‹å§‹...")
            
            # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            gpu_start = time.time()
            
            # kvikio+RMMã§ç›´æ¥GPUè»¢é€
            transfer_start = time.time()
            
            
            # RMM DeviceBufferã‚’ä½¿ç”¨
            gpu_buffer = rmm.DeviceBuffer(size=file_size)
            
            # kvikioã§ç›´æ¥èª­ã¿è¾¼ã¿
            with kvikio.CuFile(chunk_file, "rb") as f:
                gpu_array = cp.asarray(gpu_buffer).view(dtype=cp.uint8)
                bytes_read = f.read(gpu_array)
            
            if bytes_read != file_size:
                raise RuntimeError(f"èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºä¸ä¸€è‡´: {bytes_read} != {file_size}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆkvikioèª­ã¿è¾¼ã¿å¾Œï¼‰ - ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®ã¿
            if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
                if os.path.exists(chunk_file):
                    print(f"[Consumer-{consumer_id}] kvikioèª­ã¿è¾¼ã¿å¾Œ: {chunk_file} ã¯ã¾ã å­˜åœ¨ã—ã¾ã™")
                else:
                    print(f"[Consumer-{consumer_id}] kvikioèª­ã¿è¾¼ã¿å¾Œ: {chunk_file} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
            
            # numba cudaé…åˆ—ã«å¤‰æ›ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
            raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
            
            transfer_time = time.time() - transfer_start
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
            header_sample = raw_dev[:min(128, raw_dev.shape[0])].copy_to_host()
            header_size = detect_pg_header_size(header_sample)
            
            # ç›´æ¥æŠ½å‡ºå‡¦ç†
            chunk_output = f"output/{table_name}_chunk_{chunk_id}_queue.parquet"
            
            # ãƒãƒ£ãƒ³ã‚¯IDã¨æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
            os.environ['GPUPGPARSER_CURRENT_CHUNK'] = str(chunk_id)
            if chunk_id == total_chunks - 1:
                os.environ['GPUPGPARSER_LAST_CHUNK'] = '1'
            else:
                os.environ['GPUPGPARSER_LAST_CHUNK'] = '0'
            
            cudf_df, detailed_timing = postgresql_to_cudf_parquet_direct(
                raw_dev=raw_dev,
                columns=columns,
                ncols=len(columns),
                header_size=header_size,
                output_path=chunk_output,
                compression='snappy',
                use_rmm=True,
                optimize_gpu=True,
                verbose=False,
                test_mode=(os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1')
            )
            
            gpu_time = time.time() - gpu_start
            
            # å‡¦ç†çµ±è¨ˆ
            rows = len(cudf_df) if cudf_df is not None else 0
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆGPUå‡¦ç†å¾Œï¼‰
            if os.path.exists(chunk_file):
                print(f"[Consumer-{consumer_id}] GPUå‡¦ç†å¾Œ: {chunk_file} ã¯ã¾ã å­˜åœ¨ã—ã¾ã™")
            else:
                print(f"[Consumer-{consumer_id}] GPUå‡¦ç†å¾Œ: {chunk_file} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
            
            # GPUå‡¦ç†è¡Œæ•°ã‚’ä¿å­˜
            gpu_row_counts[chunk_id] = rows
            
            print(f"[Consumer-{consumer_id}] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1} GPUå‡¦ç†å®Œäº† ({gpu_time:.1f}ç§’, {rows:,}è¡Œ)")
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
            if os.environ.get('GPUPGPARSER_TEST_MODE', '0') == '1':
                print(f"[CHUNK DEBUG] ãƒãƒ£ãƒ³ã‚¯ {chunk_id + 1}: ")
                print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size / 1024**2:.1f} MB")
                print(f"  - æ¤œå‡ºè¡Œæ•°: {rows:,}è¡Œ")
                print(f"  - GPUãƒ‘ãƒ¼ã‚¹æ™‚é–“: {detailed_timing.get('gpu_parsing', 0):.2f}ç§’")
                print(f"  - è¡Œã‚ãŸã‚Š: {file_size/rows if rows > 0 else 0:.1f} bytes/row")
            
            # çµ±è¨ˆæƒ…å ±ã‚’é€ä¿¡
            stats_queue.put(('gpu_time', gpu_time))
            stats_queue.put(('transfer_time', transfer_time))
            stats_queue.put(('rows', rows))
            stats_queue.put(('size', file_size))
            
            # è©³ç´°çµ±è¨ˆã‚’ä¿å­˜
            chunk_stats.append({
                'chunk_id': chunk_id,
                'consumer_id': consumer_id,
                'rust_time': chunk_info['rust_time'],
                'gpu_time': gpu_time,
                'transfer_time': transfer_time,
                'parse_time': detailed_timing.get('gpu_parsing', 0),
                'string_time': detailed_timing.get('string_buffer_creation', 0),
                'write_time': detailed_timing.get('parquet_export', 0),
                'rows': rows,
                'size_gb': file_size / 1024**3
            })
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del raw_dev
            del gpu_buffer
            del gpu_array
            if cudf_df is not None:
                del cudf_df
            
            mempool.free_all_blocks()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()
            
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å‰Šé™¤å‰ã«ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            if test_mode:
                # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                if not hasattr(gpu_consumer, 'test_save_dir'):
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    gpu_consumer.test_save_dir = f"test_binaries/{timestamp}"
                    os.makedirs(gpu_consumer.test_save_dir, exist_ok=True)
                    print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å…ˆ - {gpu_consumer.test_save_dir}")
                
                # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                import shutil
                dst = f"{gpu_consumer.test_save_dir}/{table_name}_chunk_{chunk_id}.bin"
                shutil.copy2(chunk_file, dst)
                size = os.path.getsize(chunk_file) / (1024**3)
                print(f"  âœ“ {table_name}_chunk_{chunk_id}.bin ã‚’ä¿å­˜ ({size:.2f} GB)")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            if os.path.exists(chunk_file):
                os.remove(chunk_file)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
                
        except queue.Empty:
            continue
        except Exception as e:
            print(f"[Consumer-{consumer_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[Consumer-{consumer_id}] çµ‚äº†")


def validate_parquet_output(file_path: str, num_rows: int = 5, gpu_rows: int = None) -> bool:
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ã¨ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    
    Args:
        file_path: æ¤œè¨¼ã™ã‚‹Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        num_rows: è¡¨ç¤ºã™ã‚‹è¡Œæ•°
        gpu_rows: GPUå‡¦ç†ã§æ¤œå‡ºã—ãŸè¡Œæ•°ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    
    Returns:
        æ¤œè¨¼æˆåŠŸã®å ´åˆTrue
    """
    try:
        # PyArrowã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        table = pq.read_table(file_path)
        
        print(f"\nğŸ“Š Parquetãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: {os.path.basename(file_path)}")
        print(f"â”œâ”€ è¡Œæ•°: {table.num_rows:,}", end="")
        if gpu_rows is not None:
            if table.num_rows == gpu_rows:
                print(f" âœ… OK (GPUå‡¦ç†è¡Œæ•°ã¨ä¸€è‡´)")
            else:
                print(f" âŒ NG (GPUå‡¦ç†è¡Œæ•°: {gpu_rows:,})")
        else:
            print()
        print(f"â”œâ”€ åˆ—æ•°: {table.num_columns}")
        print(f"â””â”€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(file_path) / 1024**2:.2f} MB")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­{num_rows}è¡Œï¼‰:")
        print("â”€" * 80)
        df_sample = table.slice(0, num_rows).to_pandas()
        print(df_sample.to_string(index=False, max_colwidth=20))
        print("â”€" * 80)
        
        return True
        
    except Exception as e:
        print(f"âŒ Parquetæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_parallel_pipeline(total_chunks: int, table_name: str, test_mode: bool = False):
    """çœŸã®ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ã‚­ãƒ¥ãƒ¼ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†
    chunk_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)
    stats_queue = queue.Queue()
    metadata_queue = queue.Queue()
    
    start_time = time.time()
    
    # Producerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
    producer_thread = threading.Thread(
        target=rust_producer,
        args=(chunk_queue, total_chunks, stats_queue, table_name, metadata_queue)
    )
    producer_thread.start()
    
    # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    try:
        rust_metadata = metadata_queue.get(timeout=30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        columns = convert_rust_metadata_to_column_meta(rust_metadata)
        print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(columns)} åˆ—")
    except queue.Empty:
        print("âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        print("ãƒ’ãƒ³ãƒˆ: Rustãƒã‚¤ãƒŠãƒªãŒJSONã‚’å‡ºåŠ›ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        # Producerã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤
        producer_thread.join()
        raise RuntimeError("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å¾Œï¼‰
    warmup_thread = threading.Thread(
        target=gpu_warmup,
        args=(columns,)
    )
    warmup_thread.start()
    
    # Consumerã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆ1ã¤ã®ã¿ - GPUãƒ¡ãƒ¢ãƒªåˆ¶ç´„ï¼‰
    consumer_thread = threading.Thread(
        target=gpu_consumer,
        args=(chunk_queue, columns, 1, stats_queue, total_chunks, table_name, test_mode)
    )
    consumer_thread.start()
    
    # çµ±è¨ˆåé›†
    total_rust_time = 0
    total_gpu_time = 0
    total_transfer_time = 0
    total_rows = 0
    total_size = 0
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…æ©Ÿã—ãªãŒã‚‰çµ±è¨ˆã‚’åé›†
    warmup_thread.join()  # ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—å®Œäº†ã‚’å¾…ã¤
    producer_thread.join()
    consumer_thread.join()
    
    # çµ±è¨ˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœã‚’åé›†
    while not stats_queue.empty():
        stat_type, value = stats_queue.get()
        if stat_type == 'rust_time':
            total_rust_time += value
        elif stat_type == 'gpu_time':
            total_gpu_time += value
        elif stat_type == 'transfer_time':
            total_transfer_time += value
        elif stat_type == 'rows':
            total_rows += value
        elif stat_type == 'size':
            total_size += value
    
    total_time = time.time() - start_time
    
    return {
        'total_time': total_time,
        'total_rust_time': total_rust_time,
        'total_gpu_time': total_gpu_time,
        'total_transfer_time': total_transfer_time,
        'total_rows': total_rows,
        'total_size': total_size,
        'processed_chunks': len(chunk_stats)
    }


def gpu_warmup(columns):
    """GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ— - JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨CUDAåˆæœŸåŒ–"""
    try:
        print("\nğŸ”¥ GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ä¸­...")
        
        # PostgreSQL COPY BINARYãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ19ãƒã‚¤ãƒˆï¼‰
        header = [
            0x50, 0x47, 0x43, 0x4F, 0x50, 0x59, 0x0A, 0xFF, 0x0D, 0x0A, 0x00,  # PGCOPY
            0x00, 0x00, 0x00, 0x00,  # flags
            0x00, 0x00, 0x00, 0x00   # header extension
        ]
        
        # 1è¡Œåˆ†ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ17åˆ—ï¼‰ã‚’ä½œæˆ
        row_data = []
        row_data.extend([0x00, 0x11])  # 17ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        
        for i in range(17):
            if i < 8:  # æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆint64ï¼‰
                row_data.extend([0x00, 0x00, 0x00, 0x08])  # é•·ã•8
                row_data.extend([0x00] * 8)  # 8ãƒã‚¤ãƒˆã®ã‚¼ãƒ­
            else:  # æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                row_data.extend([0x00, 0x00, 0x00, 0x04])  # é•·ã•4
                row_data.extend([0x54, 0x45, 0x53, 0x54])  # "TEST"
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆ100KBç¨‹åº¦ - ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚µã‚¤ã‚ºï¼‰
        dummy_list = header + row_data * 1000  # 1000è¡Œåˆ†
        # çµ‚ç«¯ãƒãƒ¼ã‚«ãƒ¼è¿½åŠ ï¼ˆ0xFFFFï¼‰
        dummy_list.extend([0xFF, 0xFF])
        dummy_data = np.array(dummy_list, dtype=np.uint8)
        
        # GPUå‡¦ç†å®Ÿè¡Œï¼ˆJITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨CUDAåˆæœŸåŒ–ï¼‰
        # å®Ÿéš›ã®å‡¦ç†ã¨åŒã˜ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        os.environ['GPUPGPARSER_ROWS_PER_THREAD'] = os.environ.get('GPUPGPARSER_ROWS_PER_THREAD', '32')
        os.environ['GPUPGPARSER_STRING_ROWS_PER_THREAD'] = os.environ.get('GPUPGPARSER_STRING_ROWS_PER_THREAD', '1')
        
        # GPUè»¢é€
        import cupy as cp
        gpu_buffer = cp.asarray(dummy_data).view(dtype=cp.uint8)
        raw_dev = cuda.as_cuda_array(gpu_buffer).view(dtype=np.uint8)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºæ¤œå‡º
        header_size = detect_pg_header_size(dummy_data)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œã§JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’ãƒˆãƒªã‚¬ãƒ¼
        from src.cuda_kernels.postgres_binary_parser import parse_binary_chunk_gpu_ultra_fast_v2_lite
        
        # GPUãƒ‘ãƒ¼ã‚¹ã‚«ãƒ¼ãƒãƒ«ã‚’ç›´æ¥å®Ÿè¡Œï¼ˆJITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç¢ºå®Ÿã«å®Ÿè¡Œï¼‰
        row_positions, field_offsets, field_lengths = parse_binary_chunk_gpu_ultra_fast_v2_lite(
            raw_dev, columns, header_size, debug=False, test_mode=False
        )
        
        # çµæœã‚’ç ´æ£„
        del row_positions
        del field_offsets
        del field_lengths
        del gpu_buffer
        del raw_dev
        
        print("âœ… GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—å®Œäº†\n")
        
    except Exception as e:
        print(f"âš ï¸  GPUã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}\n")


def main(total_chunks=8, table_name=None, test_mode=False, test_duplicate_keys=None):
    global TABLE_NAME
    if table_name:
        TABLE_NAME = table_name
    else:
        table_name = TABLE_NAME  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€GPUç‰¹æ€§ã‚’è¡¨ç¤º
    if test_mode:
        from src.cuda_kernels.postgres_binary_parser import print_gpu_properties
        print_gpu_properties()
    
    # kvikioè¨­å®šç¢ºèª
    is_compat = os.environ.get("KVIKIO_COMPAT_MODE", "").lower() in ["on", "1", "true"]
    
    # RMMãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š
    setup_rmm_pool()
    
    # CUDA contextç¢ºèª
    try:
        cuda.current_context()
    except Exception as e:
        print(f"âŒ CUDA context ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_files(total_chunks, table_name)
    
    try:
        # ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        print("\nä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        print("=" * 80)
        
        results = run_parallel_pipeline(total_chunks, table_name, test_mode)
        
        # æœ€çµ‚çµ±è¨ˆã‚’æ§‹é€ åŒ–è¡¨ç¤º
        total_gb = results['total_size'] / 1024**3
        
        print(f"\n{'='*80}")
        print(f" âœ… å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")
        
        # å…¨ä½“çµ±è¨ˆ
        print(f"\nã€å…¨ä½“çµ±è¨ˆã€‘")
        print(f"â”œâ”€ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_gb:.2f} GB")
        print(f"â”œâ”€ ç·è¡Œæ•°: {results['total_rows']:,} è¡Œ")
        print(f"â”œâ”€ ç·å®Ÿè¡Œæ™‚é–“: {results['total_time']:.2f}ç§’")
        print(f"â””â”€ å…¨ä½“ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_gb / results['total_time']:.2f} GB/ç§’")
        
        # å‡¦ç†æ™‚é–“å†…è¨³
        print(f"\nã€å‡¦ç†æ™‚é–“å†…è¨³ã€‘")
        if results['total_rust_time'] > 0:
            print(f"â”œâ”€ Rustè»¢é€åˆè¨ˆ: {results['total_rust_time']:.2f}ç§’ ({total_gb / results['total_rust_time']:.2f} GB/ç§’)")
        else:
            print(f"â”œâ”€ Rustè»¢é€åˆè¨ˆ: {results['total_rust_time']:.2f}ç§’")
        if results['total_gpu_time'] > 0:
            print(f"â””â”€ GPUå‡¦ç†åˆè¨ˆ: {results['total_gpu_time']:.2f}ç§’ ({total_gb / results['total_gpu_time']:.2f} GB/ç§’)")
        else:
            print(f"â””â”€ GPUå‡¦ç†åˆè¨ˆ: {results['total_gpu_time']:.2f}ç§’")
        print(f"   â””â”€ kvikioè»¢é€: {results['total_transfer_time']:.2f}ç§’")
        
        
        # ãƒãƒ£ãƒ³ã‚¯æ¯ã®è©³ç´°çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        if chunk_stats:
            print(f"\nã€ãƒãƒ£ãƒ³ã‚¯æ¯ã®å‡¦ç†æ™‚é–“ã€‘")
            print(f"â”Œ{'â”€'*7}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*10}â”¬{'â”€'*12}â”")
            print(f"â”‚ãƒãƒ£ãƒ³ã‚¯â”‚ Rustè»¢é€ â”‚kvikioè»¢é€â”‚ GPUãƒ‘ãƒ¼ã‚¹â”‚ æ–‡å­—åˆ—å‡¦ç†â”‚ Parquet â”‚   å‡¦ç†è¡Œæ•°  â”‚")
            print(f"â”œ{'â”€'*7}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*10}â”¼{'â”€'*12}â”¤")
            
            # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
            sorted_stats = sorted(chunk_stats, key=lambda x: x['chunk_id'])
            for stat in sorted_stats:
                print(f"â”‚  {stat['chunk_id']:^3}  â”‚ {stat['rust_time']:>6.2f}ç§’ â”‚ {stat['transfer_time']:>6.2f}ç§’ â”‚"
                      f" {stat['parse_time']:>6.2f}ç§’ â”‚ {stat['string_time']:>6.2f}ç§’ â”‚"
                      f" {stat['write_time']:>6.2f}ç§’ â”‚{stat['rows']:>10,}è¡Œâ”‚")
            
            print(f"â””{'â”€'*7}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*10}â”´{'â”€'*12}â”˜")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ€§èƒ½æ¸¬å®šå®Œäº†å¾Œã€ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã‚’å®Ÿè¡Œ
        sample_parquet = f"output/{table_name}_chunk_0_queue.parquet"
        if os.path.exists(sample_parquet):
            # chunk_0ã®GPUå‡¦ç†è¡Œæ•°ã‚’å–å¾—
            gpu_rows_chunk0 = gpu_row_counts.get(0, None)
            validate_parquet_output(sample_parquet, num_rows=5, gpu_rows=gpu_rows_chunk0)
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¯å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¿æŒ
        
        # å…¨Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿéš›ã®è¡Œæ•°ã‚’ç¢ºèª
        print("\nã€Parquetãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã€‘")
        actual_total_rows = 0
        parquet_files = sorted(Path("output").glob(f"{table_name}_chunk_*_queue.parquet"))
        for pf in parquet_files:
            try:
                table = pq.read_table(pf)
                actual_total_rows += table.num_rows
                print(f"â”œâ”€ {pf.name}: {table.num_rows:,} è¡Œ")
            except Exception as e:
                print(f"â”œâ”€ {pf.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
        print(f"â””â”€ å®Ÿéš›ã®ç·è¡Œæ•°: {actual_total_rows:,} è¡Œ")
        
        # --testãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã¨æ¯”è¼ƒ
        if os.environ.get("GPUPGPARSER_TEST_MODE") == "1":
            print("\nã€PostgreSQLãƒ†ãƒ¼ãƒ–ãƒ«è¡Œæ•°ã¨ã®æ¯”è¼ƒã€‘")
            print("â€» psycopgä¾å­˜ã‚’å‰Šé™¤ã—ãŸãŸã‚ã€ã“ã®æ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹ã§ã™")
            # TODO: Rustãƒã‚¤ãƒŠãƒªã‹ã‚‰è¡Œæ•°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‹ã€åˆ¥ã®æ–¹æ³•ã‚’å®Ÿè£…
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼ˆpsycopgä¾å­˜ã®ãŸã‚ï¼‰
            if test_duplicate_keys:
                print("\nã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ã€‘")
                print("â€» psycopgä¾å­˜ã‚’å‰Šé™¤ã—ãŸãŸã‚ã€ã“ã®æ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹ã§ã™")
        
        if actual_total_rows != results['total_rows']:
            print(f"\nâš ï¸  è¡Œæ•°ä¸ä¸€è‡´: GPUå ±å‘Šå€¤ {results['total_rows']:,} vs Parquetå®Ÿéš›å€¤ {actual_total_rows:,}")
        
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if test_mode:
            import shutil
            
            # gpu_consumerã§ä½¿ç”¨ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼‰
            if hasattr(gpu_consumer, 'test_save_dir'):
                save_dir = gpu_consumer.test_save_dir
                print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ä¸­...")
            else:
                # gpu_consumerãŒå®Ÿè¡Œã•ã‚Œãªã‹ã£ãŸå ´åˆã®å‡¦ç†
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                save_dir = f"test_binaries/{timestamp}"
                os.makedirs(save_dir, exist_ok=True)
                print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªä¸­...")
                
                # ç¾æ™‚ç‚¹ã§å­˜åœ¨ã™ã‚‹ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆé€šå¸¸ã¯gpu_consumerã§ä¿å­˜æ¸ˆã¿ï¼‰
                saved_count = 0
                for i in range(total_chunks):
                    src = f"{OUTPUT_DIR}/{table_name}_chunk_{i}.bin"
                    if os.path.exists(src):
                        dst = f"{save_dir}/{table_name}_chunk_{i}.bin"
                        shutil.copy2(src, dst)
                        size = os.path.getsize(src) / (1024**3)
                        print(f"  âœ“ {table_name}_chunk_{i}.bin ã‚’ä¿å­˜ ({size:.2f} GB)")
                        saved_count += 1
            
            # ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            meta_src = f"{OUTPUT_DIR}/{table_name}_meta_0.json"
            if os.path.exists(meta_src):
                shutil.copy2(meta_src, f"{save_dir}/{table_name}_meta_0.json")
                print(f"  âœ“ {table_name}_meta_0.json ã‚’ä¿å­˜")
            
            # å®Ÿè¡Œæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            from datetime import datetime
            metadata = {
                "timestamp": save_dir.split('/')[-1],  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
                "table_name": table_name,
                "total_chunks": total_chunks,
                "saved_chunks": total_chunks,  # gpu_consumerã§å…¨ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã¯ãš
                "note": "Binary files saved before deletion in gpu_consumer",
                "parallel_connections": int(os.environ.get('RUST_PARALLEL_CONNECTIONS', 16)),
                "command": f"python cu_pg_parquet.py --test --table {table_name} --parallel {int(os.environ.get('RUST_PARALLEL_CONNECTIONS', 16))} --chunks {total_chunks}"
            }
            metadata_path = f"{save_dir}/execution_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"  âœ“ execution_metadata.json ã‚’ä¿å­˜")
            
            print(f"\nğŸ“ ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {save_dir} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleanup_files(total_chunks, table_name)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='PostgreSQL â†’ GPU ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯')
    parser.add_argument('--table', type=str, default='lineorder', help='å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«å')
    parser.add_argument('--parallel', type=int, default=16, help='ä¸¦åˆ—æ¥ç¶šæ•°')
    parser.add_argument('--chunks', type=int, default=8, help='ãƒãƒ£ãƒ³ã‚¯æ•°')
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆGPUç‰¹æ€§ãƒ»ã‚«ãƒ¼ãƒãƒ«æƒ…å ±è¡¨ç¤ºï¼‰')
    args = parser.parse_args()
    
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    os.environ['RUST_PARALLEL_CONNECTIONS'] = str(args.parallel)
    os.environ['TOTAL_CHUNKS'] = str(args.chunks)
    os.environ['TABLE_NAME'] = args.table  # Rustå´ã«ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ä¼ãˆã‚‹
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    if args.test:
        os.environ['GPUPGPARSER_TEST_MODE'] = '1'
    
    main(total_chunks=args.chunks, table_name=args.table, test_mode=args.test)