#!/usr/bin/env python3
"""
å¤§è¦æ¨¡Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¡¨ç¤ºã‚µãƒ³ãƒ—ãƒ«
cuDFã‚’ä½¿ç”¨ã—ãŸGPUé«˜é€Ÿå‡¦ç†ç‰ˆ
"""

import cudf
import dask_cudf
import time
import sys
from pathlib import Path
import gc
import pandas as pd
import argparse

# pandasè¡¨ç¤ºè¨­å®šã‚’å…¨åˆ—è¡¨ç¤ºã«å¤‰æ›´
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

def check_gpu_environment():
    """GPUç’°å¢ƒã®ç¢ºèª"""
    try:
        import pynvml
        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        print(f"âœ“ GPUæ•°: {device_count}")
        
        for i in range(device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            # nvmlDeviceGetNameã®æˆ»ã‚Šå€¤ã®å‹ã‚’ãƒã‚§ãƒƒã‚¯
            name = pynvml.nvmlDeviceGetName(handle)
            if isinstance(name, bytes):
                name = name.decode('utf-8')
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            print(f"  GPU {i}: {name}")
            print(f"    ãƒ¡ãƒ¢ãƒª: {mem_info.total / 1024**3:.1f} GB (ä½¿ç”¨ä¸­: {mem_info.used / 1024**3:.1f} GB)")
    except Exception as e:
        print(f"âš  GPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        print("  cuDFã¯å‹•ä½œã—ã¾ã™ãŒã€ãƒ¡ãƒ¢ãƒªæƒ…å ±ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“")

def read_parquet_files(file_paths, use_dask=False):
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    
    Args:
        file_paths: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        use_dask: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ã«Dask-cuDFã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆ
    """
    dfs = []
    
    for file_path in file_paths:
        if not Path(file_path).exists():
            print(f"âš  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            continue
            
        print(f"\nğŸ“ èª­ã¿è¾¼ã¿ä¸­: {file_path}")
        start_time = time.time()
        
        try:
            if use_dask:
                # Dask-cuDFã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
                df = dask_cudf.read_parquet(file_path)
                # å¿…è¦ã«å¿œã˜ã¦è¨ˆç®—ã‚’å®Ÿè¡Œ
                df = df.persist()
            else:
                # é€šå¸¸ã®cuDFèª­ã¿è¾¼ã¿
                df = cudf.read_parquet(file_path)
            
            elapsed = time.time() - start_time
            print(f"  âœ“ èª­ã¿è¾¼ã¿å®Œäº† ({elapsed:.2f}ç§’)")
            
            # åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
            if use_dask:
                print(f"  ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {df.npartitions}")
                # Daskã®å ´åˆã¯.compute()ã§å®Ÿéš›ã®å€¤ã‚’å–å¾—
                shape = (len(df), len(df.columns))
            else:
                shape = df.shape
            
            print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {shape[0]:,} è¡Œ Ã— {shape[1]} åˆ—")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š
            if not use_dask:
                memory_usage = df.memory_usage(deep=True).sum() / 1024**3
                print(f"  GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage:.2f} GB")
            
            dfs.append(df)
            
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    return dfs

def check_corrupted_string(s):
    """æ–‡å­—åˆ—ãŒç ´æã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    if not isinstance(s, str):
        return False
    
    # åˆ¶å¾¡æ–‡å­—ï¼ˆæ”¹è¡Œã€ã‚¿ãƒ–ä»¥å¤–ï¼‰ã‚„ç•°å¸¸ãªæ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯
    control_chars = 0
    for c in s:
        if ord(c) < 32 and c not in '\n\t\r':
            control_chars += 1
    
    # åˆ¶å¾¡æ–‡å­—ãŒå¤šã„ã€ã¾ãŸã¯ç•°å¸¸ã«é•·ã„å ´åˆã¯ç ´æã¨åˆ¤å®š
    return control_chars > 2 or len(s) > 100

def display_sample_data(df, n_rows=10, name="", filter_column=None, filter_value=None, file_path=None, thread_id=None):
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿{' - ' + name if name else ''}")
    print(f"{'='*60}")
    
    # æ—§å½¢å¼ã®thread_idã‚µãƒãƒ¼ãƒˆï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
    if thread_id is not None and filter_column is None:
        filter_column = '_thread_id'
        filter_value = str(thread_id)
    
    # ã‚«ãƒ©ãƒ ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if filter_column is not None and filter_value is not None:
        if filter_column in df.columns:
            print(f"\nã€{filter_column} = {filter_value} ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‘")
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã«å¿œã˜ã¦å€¤ã‚’å¤‰æ›
            dtype_str = str(df[filter_column].dtype)
            original_filter_value = filter_value
            
            try:
                # æ•°å€¤å‹ã¸ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹
                if dtype_str in ['int8', 'int16', 'int32', 'int64']:
                    filter_value = int(filter_value)
                elif dtype_str in ['float32', 'float64']:
                    filter_value = float(filter_value)
                elif 'decimal' in dtype_str.lower():
                    # Decimalå‹ã®å ´åˆã¯intã«å¤‰æ›ï¼ˆcuDFã®decimalæ¯”è¼ƒã®äº’æ›æ€§ã®ãŸã‚ï¼‰
                    filter_value = int(filter_value)
                elif dtype_str == 'object' or 'string' in dtype_str.lower():
                    # æ–‡å­—åˆ—å‹ã®å ´åˆã¯æ–‡å­—åˆ—ã®ã¾ã¾
                    filter_value = str(filter_value)
                else:
                    # ãã®ä»–ã®å‹ã¯å¯èƒ½ãªé™ã‚Šå…ƒã®å€¤ã‚’ä½¿ç”¨
                    pass
            except (ValueError, TypeError) as e:
                print(f"  è­¦å‘Š: å€¤ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ ({original_filter_value} -> {dtype_str}): {e}")
                # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®å€¤ã‚’ä½¿ç”¨
            
            # cuDFã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            mask = df[filter_column] == filter_value
            filtered_df = df[mask]
            
            if len(filtered_df) == 0:
                print(f"  âš ï¸ {filter_column} = {filter_value} ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                # å­˜åœ¨ã™ã‚‹å€¤ã®ç¯„å›²ã‚’è¡¨ç¤º
                if not isinstance(df, dask_cudf.DataFrame):
                    try:
                        unique_values = df[filter_column].unique().to_pandas()
                        if len(unique_values) <= 20:
                            print(f"  å­˜åœ¨ã™ã‚‹å€¤: {sorted(unique_values)}")
                        else:
                            # æ•°å€¤å‹ã®å ´åˆã¯æœ€å°å€¤ã¨æœ€å¤§å€¤ã‚’è¡¨ç¤º
                            if str(df[filter_column].dtype) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:
                                min_val = df[filter_column].min()
                                max_val = df[filter_column].max()
                                # cuDFã®Scalarã‚’å‡¦ç†
                                if hasattr(min_val, 'compute'):
                                    min_val = min_val.compute()
                                if hasattr(max_val, 'compute'):
                                    max_val = max_val.compute()
                                print(f"  å€¤ã®ç¯„å›²: {min_val} ã€œ {max_val}")
                            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã®æ•°: {len(unique_values):,}")
                    except:
                        pass
                return
            else:
                print(f"  âœ“ {len(filtered_df):,} ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                df = filtered_df
        else:
            print(f"  âš ï¸ '{filter_column}' ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            print(f"  åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {', '.join(df.columns)}")
            return
    
    # ãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±
    print("\nã€ã‚«ãƒ©ãƒ æƒ…å ±ã€‘")
    for col, dtype in df.dtypes.items():
        print(f"  {col}: {dtype}")
    
    # è¡¨ç¤ºã™ã‚‹è¡Œæ•°ã®èª¿æ•´ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚ã¯å…¨ä»¶è¡¨ç¤ºï¼‰
    if filter_column is not None and filter_value is not None:
        display_rows = min(len(df), 100)  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚ã¯æœ€å¤§100è¡Œã¾ã§
        if display_rows < len(df):
            print(f"\nã€{filter_column} = {filter_value} ã®æœ€åˆã® {display_rows} è¡Œï¼ˆå…¨ {len(df)} è¡Œä¸­ï¼‰ã€‘")
        else:
            print(f"\nã€{filter_column} = {filter_value} ã®å…¨ {display_rows} è¡Œã€‘")
    else:
        display_rows = min(n_rows, len(df))
        print(f"\nã€å…ˆé ­{display_rows}è¡Œã€‘")
    
    # ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    try:
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚ã®è©³ç´°è¡¨ç¤º
        if filter_column is not None and filter_value is not None and len(df) > 0:
            # cuDFã‹ã‚‰Pandasã«å¤‰æ›ã—ã¦è¡¨ç¤º
            if isinstance(df, dask_cudf.DataFrame):
                display_df = df.head(display_rows).to_pandas()
            else:
                display_df = df.head(display_rows).to_pandas()
            
            # decimal128å‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            for col in display_df.columns:
                if 'decimal' in str(df[col].dtype).lower():
                    display_df[col] = display_df[col].astype(str)
            
            # è©³ç´°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
            for idx, row in display_df.iterrows():
                print(f"\n  --- ãƒ¬ã‚³ãƒ¼ãƒ‰ {idx + 1} ---")
                
                field_shift_detected = False
                
                for col_name in display_df.columns:
                    value = row[col_name]
                    
                    # å€¤ã‚’å®‰å…¨ã«è¡¨ç¤º
                    if pd.isna(value):
                        display_value = "NULL"
                    else:
                        display_value = str(value)
                    
                    # customerãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚·ãƒ•ãƒˆæ¤œå‡º
                    if 'customer' in str(file_path).lower() if file_path else False:
                        if col_name == 'c_custkey' and str(value) == '0':
                            field_shift_detected = True
                            print(f"  âš ï¸ {col_name}: {display_value} [ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚·ãƒ•ãƒˆã®å¯èƒ½æ€§]")
                        elif col_name == 'c_name' and not str(value).startswith('Customer#'):
                            print(f"  âš ï¸ {col_name}: {display_value} [c_addressã®å€¤ã®å¯èƒ½æ€§]")
                        else:
                            print(f"  {col_name}: {display_value}")
                    else:
                        print(f"  {col_name}: {display_value}")
                
                if field_shift_detected:
                    print(f"\n  ğŸ”´ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚·ãƒ•ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆc_custkeyã‹ã‚‰é †ã«1ã¤ãšã¤å‰ã«ã‚·ãƒ•ãƒˆï¼‰")
        else:
            # é€šå¸¸ã®è¡¨ç¤ºï¼ˆãƒ•ã‚£ãƒ«ã‚¿æŒ‡å®šãªã—ã®å ´åˆï¼‰
            if isinstance(df, dask_cudf.DataFrame):
                sample_df = df.head(display_rows).to_pandas()
            else:
                sample_df = df.head(display_rows).to_pandas()
            
            # decimal128å‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            for col in sample_df.columns:
                if 'decimal' in str(df[col].dtype).lower():
                    sample_df[col] = sample_df[col].astype(str)
            
            print(sample_df.to_string())
                
    except Exception as e:
        print(f"\n  âŒ ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {type(e).__name__}")
        print(f"     {str(e)[:200]}...")
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€å°‘ãªãã¨ã‚‚ãƒ¡ã‚¿æƒ…å ±ã‚’è¡¨ç¤º
        try:
            print("\n  ã€è¡Œæ•°ã¨åŸºæœ¬æƒ…å ±ã®ã¿è¡¨ç¤ºã€‘")
            print(f"  ç·è¡Œæ•°: {len(df)}")
            if '_thread_id' in df.columns:
                thread_counts = df['_thread_id'].value_counts().to_pandas()
                print(f"  Threadåˆ¥è¡Œæ•°:")
                for tid, count in thread_counts.items():
                    print(f"    Thread {tid}: {count}è¡Œ")
        except:
            print("  åŸºæœ¬æƒ…å ±ã‚‚å–å¾—ã§ãã¾ã›ã‚“")
    
    # åŸºæœ¬çµ±è¨ˆé‡ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
    print("\nã€åŸºæœ¬çµ±è¨ˆé‡ã€‘")
    try:
        # cuDFã®select_dtypesã¯'number'ã§ã¯ãªãå…·ä½“çš„ãªå‹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        numeric_types = ['int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64', 'float32', 'float64']
        numeric_cols = []
        
        for col in df.columns:
            if str(df[col].dtype) in numeric_types:
                numeric_cols.append(col)
        
        if len(numeric_cols) > 0:
            if isinstance(df, dask_cudf.DataFrame):
                stats = df[numeric_cols].describe().compute()
                print(stats.to_pandas().to_string())
            else:
                stats = df[numeric_cols].describe()
                print(stats.to_pandas().to_string())
        else:
            print("  æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print(f"  çµ±è¨ˆé‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

def process_large_parquet_files(filter_column=None, filter_value=None, target_dir=None):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ•ãƒ­ãƒ¼"""
    print("ğŸš€ cuDF Parquetãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹")
    print("="*60)
    
    if filter_column is not None and filter_value is not None:
        print(f"\nğŸ” {filter_column} = {filter_value} ã‚’æ¤œç´¢ã—ã¾ã™")
    
    # 1. GPUç’°å¢ƒç¢ºèª
    print("\nã€GPUç’°å¢ƒç¢ºèªã€‘")
    check_gpu_environment()
    
    # 2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    if target_dir:
        output_dir = Path(target_dir)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ãŸã¯outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if Path("output").exists():
            output_dir = Path("output")
        else:
            output_dir = Path(".")
    
    if not output_dir.exists():
        print(f"\nâœ— ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {output_dir}")
        return
        
    file_paths = sorted(output_dir.glob("*.parquet"))
    
    if not file_paths:
        print(f"\nâœ— {output_dir}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
        
    print(f"\nã€æ¤œå‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã€‘")
    print(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_paths)}")
    for file_path in file_paths:
        print(f"  - {file_path.name}")
    
    # 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèªï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼‰
    total_size = 0
    for file_path in file_paths:
        if Path(file_path).exists():
            size = Path(file_path).stat().st_size / 1024**3
            print(f"\nğŸ“ {file_path}: {size:.2f} GB")
            total_size += size
    
    # 4. èª­ã¿è¾¼ã¿æ–¹æ³•ã®é¸æŠï¼ˆ10GBä»¥ä¸Šãªã‚‰Daskä½¿ç”¨ã‚’æ¨å¥¨ï¼‰
    use_dask = total_size > 10
    if use_dask:
        print(f"\nâš¡ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º (åˆè¨ˆ {total_size:.2f} GB)")
        print("  â†’ Dask-cuDFã‚’ä½¿ç”¨ã—ã¦åˆ†æ•£å‡¦ç†ã—ã¾ã™")
    
    # 5. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print("\nã€ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã€‘")
    dfs = read_parquet_files(file_paths, use_dask=use_dask)
    
    if not dfs:
        print("\nâœ— èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # 6. å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    for i, (df, file_path) in enumerate(zip(dfs, file_paths)):
        display_sample_data(df, n_rows=5, name=f"ãƒ•ã‚¡ã‚¤ãƒ«{i} ({Path(file_path).name})", 
                          filter_column=filter_column, filter_value=filter_value, file_path=file_path)


def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(
        description='Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¡¨ç¤ºï¼ˆcuDFä½¿ç”¨ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ä¾‹:
  # é€šå¸¸ã®è¡¨ç¤º
  python show_parquet_sample.py
  
  # ç‰¹å®šã®thread_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ—§å½¢å¼ã€äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
  python show_parquet_sample.py --thread_id 1852295
  
  # ä»»æ„ã®ã‚«ãƒ©ãƒ ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
  python show_parquet_sample.py --filter _thread_id=1852295
  python show_parquet_sample.py --filter c_custkey=3045312
  python show_parquet_sample.py --filter c_name="Customer#003045312"
  python show_parquet_sample.py --filter c_nation="JAPAN"
  
  # ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
  python show_parquet_sample.py --dir /path/to/parquet/files
  
  # çµ„ã¿åˆã‚ã›
  python show_parquet_sample.py --filter c_region="ASIA" --dir .
        '''
    )
    
    parser.add_argument(
        '--thread_id',
        type=int,
        help='ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹thread_idï¼ˆéæ¨å¥¨: --filter _thread_id=å€¤ ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰'
    )
    
    parser.add_argument(
        '--filter',
        type=str,
        help='ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ï¼ˆå½¢å¼: ã‚«ãƒ©ãƒ å=å€¤ï¼‰'
    )
    
    parser.add_argument(
        '--dir',
        type=str,
        help='Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputã¾ãŸã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰'
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    try:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
        args = parse_args()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã®è§£æ
        filter_column = None
        filter_value = None
        
        if args.filter:
            # --filter ã‚«ãƒ©ãƒ å=å€¤ ã®å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            if '=' in args.filter:
                filter_column, filter_value = args.filter.split('=', 1)
                # å¼•ç”¨ç¬¦ã‚’é™¤å»
                filter_value = filter_value.strip('"\'')
            else:
                print(f"âœ— ãƒ•ã‚£ãƒ«ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {args.filter}")
                print("  æ­£ã—ã„å½¢å¼: --filter ã‚«ãƒ©ãƒ å=å€¤")
                sys.exit(1)
        elif args.thread_id is not None:
            # æ—§å½¢å¼ã® --thread_id ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            filter_column = '_thread_id'
            filter_value = str(args.thread_id)
        
        # ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ
        process_large_parquet_files(
            filter_column=filter_column,
            filter_value=filter_value,
            target_dir=args.dir
        )
        
    except ImportError as e:
        print(f"\nâœ— ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print("\nã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã€‘")
        print("conda install -c rapidsai -c conda-forge -c nvidia cudf")
        print("ã¾ãŸã¯")
        print("pip install cudf-cu11  # CUDA 11.xå‘ã‘")
        print("pip install cudf-cu12  # CUDA 12.xå‘ã‘")
        
    except Exception as e:
        print(f"\nâœ— ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
